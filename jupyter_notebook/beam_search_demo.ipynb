{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append(\"/home/lcz/lenlp/text-generation/py_project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置GPU使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root path: /home/lcz/lenlp/text-generation\n"
     ]
    }
   ],
   "source": [
    "from utils.data_loader import load_dataset, load_test_dataset\n",
    "from utils.linux_config import embedding_matrix_path\n",
    "from utils.wv_loader import load_embedding_matrix, load_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = load_embedding_matrix(\"./../data/embedding_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = \"./../data/vocab.txt\"\n",
    "vocab, reverse_vocab = load_vocab(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 1024\n",
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"embed_size\"] = 300\n",
    "params[\"enc_units\"] = units\n",
    "params[\"attn_units\"] = units\n",
    "params[\"dec_units\"] = units\n",
    "params[\"batch_size\"] = 64\n",
    "params[\"epochs\"] = 2\n",
    "params[\"max_enc_len\"] = 200\n",
    "params[\"max_dec_len\"] = 41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "steps_per_epoch = len(train_X) // params[\"batch_size\"]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_Y)).shuffle(params[\"batch_size\"])\n",
    "dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root path: /home/lcz/lenlp/text-generation\n"
     ]
    }
   ],
   "source": [
    "from seq2seq.layers import Encoder, BahdanauAttention, Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 200, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(params[\"vocab_size\"], params[\"embed_size\"], embedding_matrix, params[\"enc_units\"], params[\"batch_size\"])\n",
    "enc_hidden = encoder.initialize_hidden_state()\n",
    "example_input_batch = tf.ones(shape=(params[\"batch_size\"], params[\"max_enc_len\"]), dtype=tf.int32)\n",
    "sample_output, sample_hidden = encoder(example_input_batch, enc_hidden)\n",
    "sample_output.shape\n",
    "sample_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 32909])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(params[\"vocab_size\"], params[\"embed_size\"], embedding_matrix, params[\"enc_units\"], params[\"batch_size\"])\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)), sample_hidden, sample_output)\n",
    "sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存点设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "nuk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    nuk_mask = tf.math.equal(real, nuk_index)\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,nuk_mask))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_dir = \"./../data/checkpoints/beam_search_training_checkpoints_mask_loss_dim300_seq\"\n",
    "import os\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 构建encoder\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        # 2. 复制\n",
    "        dec_hidden = enc_hidden\n",
    "        # 3. <START> * BATCH_SIZE \n",
    "        dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "        \n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # decoder(x, hidden, enc_output)\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n",
      "(64, 260, 300)\n",
      "(64, 260, 300)\n",
      "Epoch 1 Step 0 Loss 2.4456\n",
      "Epoch 1 Step 1 Loss 2.1865\n",
      "Epoch 1 Step 2 Loss 2.2299\n",
      "Epoch 1 Step 3 Loss 1.8850\n",
      "Epoch 1 Step 4 Loss 2.0126\n",
      "Epoch 1 Step 5 Loss 1.7054\n",
      "Epoch 1 Step 6 Loss 1.3285\n",
      "Epoch 1 Step 7 Loss 1.3981\n",
      "Epoch 1 Step 8 Loss 1.1917\n",
      "Epoch 1 Step 9 Loss 1.9490\n",
      "Epoch 1 Step 10 Loss 1.8808\n",
      "Epoch 1 Step 11 Loss 1.7249\n",
      "Epoch 1 Step 12 Loss 1.4042\n",
      "Epoch 1 Step 13 Loss 1.5886\n",
      "Epoch 1 Step 14 Loss 1.9183\n",
      "Epoch 1 Step 15 Loss 2.1339\n",
      "Epoch 1 Step 16 Loss 1.2378\n",
      "Epoch 1 Step 17 Loss 1.1591\n",
      "Epoch 1 Step 18 Loss 1.0553\n",
      "Epoch 1 Step 19 Loss 0.9926\n",
      "Epoch 1 Step 20 Loss 1.2389\n",
      "Epoch 1 Step 21 Loss 1.3238\n",
      "Epoch 1 Step 22 Loss 1.2575\n",
      "Epoch 1 Step 23 Loss 1.8039\n",
      "Epoch 1 Step 24 Loss 1.6078\n",
      "Epoch 1 Step 25 Loss 1.2910\n",
      "Epoch 1 Step 26 Loss 1.7685\n",
      "Epoch 1 Step 27 Loss 1.9162\n",
      "Epoch 1 Step 28 Loss 2.2273\n",
      "Epoch 1 Step 29 Loss 2.0795\n",
      "Epoch 1 Step 30 Loss 1.6395\n",
      "Epoch 1 Step 31 Loss 1.6040\n",
      "Epoch 1 Step 32 Loss 1.6771\n",
      "Epoch 1 Step 33 Loss 1.4080\n",
      "Epoch 1 Step 34 Loss 1.5168\n",
      "Epoch 1 Step 35 Loss 1.8394\n",
      "Epoch 1 Step 36 Loss 1.7932\n",
      "Epoch 1 Step 37 Loss 2.0487\n",
      "Epoch 1 Step 38 Loss 1.7403\n",
      "Epoch 1 Step 39 Loss 1.8139\n",
      "Epoch 1 Step 40 Loss 1.9694\n",
      "Epoch 1 Step 41 Loss 1.4016\n",
      "Epoch 1 Step 42 Loss 1.7595\n",
      "Epoch 1 Step 43 Loss 1.9898\n",
      "Epoch 1 Step 44 Loss 1.5754\n",
      "Epoch 1 Step 45 Loss 1.6469\n",
      "Epoch 1 Step 46 Loss 1.5395\n",
      "Epoch 1 Step 47 Loss 1.2715\n",
      "Epoch 1 Step 48 Loss 1.5456\n",
      "Epoch 1 Step 49 Loss 2.0446\n",
      "Epoch 1 Step 50 Loss 2.0938\n",
      "Epoch 1 Step 51 Loss 1.9313\n",
      "Epoch 1 Step 52 Loss 1.7648\n",
      "Epoch 1 Step 53 Loss 1.7523\n",
      "Epoch 1 Step 54 Loss 2.0071\n",
      "Epoch 1 Step 55 Loss 1.2543\n",
      "Epoch 1 Step 56 Loss 1.5831\n",
      "Epoch 1 Step 57 Loss 1.6225\n",
      "Epoch 1 Step 58 Loss 1.6141\n",
      "Epoch 1 Step 59 Loss 1.6944\n",
      "Epoch 1 Step 60 Loss 1.4912\n",
      "Epoch 1 Step 61 Loss 1.6052\n",
      "Epoch 1 Step 62 Loss 1.2971\n",
      "Epoch 1 Step 63 Loss 1.4767\n",
      "Epoch 1 Step 64 Loss 1.3411\n",
      "Epoch 1 Step 65 Loss 1.7869\n",
      "Epoch 1 Step 66 Loss 1.4780\n",
      "Epoch 1 Step 67 Loss 1.9669\n",
      "Epoch 1 Step 68 Loss 1.6244\n",
      "Epoch 1 Step 69 Loss 1.6606\n",
      "Epoch 1 Step 70 Loss 1.5022\n",
      "Epoch 1 Step 71 Loss 1.4567\n",
      "Epoch 1 Step 72 Loss 1.8286\n",
      "Epoch 1 Step 73 Loss 1.6092\n",
      "Epoch 1 Step 74 Loss 2.0829\n",
      "Epoch 1 Step 75 Loss 1.6816\n",
      "Epoch 1 Step 76 Loss 1.6889\n",
      "Epoch 1 Step 77 Loss 1.5936\n",
      "Epoch 1 Step 78 Loss 1.8274\n",
      "Epoch 1 Step 79 Loss 2.2079\n",
      "Epoch 1 Step 80 Loss 1.7042\n",
      "Epoch 1 Step 81 Loss 1.1787\n",
      "Epoch 1 Step 82 Loss 1.5897\n",
      "Epoch 1 Step 83 Loss 1.5304\n",
      "Epoch 1 Step 84 Loss 1.7178\n",
      "Epoch 1 Step 85 Loss 1.5491\n",
      "Epoch 1 Step 86 Loss 1.5057\n",
      "Epoch 1 Step 87 Loss 1.8274\n",
      "Epoch 1 Step 88 Loss 1.4441\n",
      "Epoch 1 Step 89 Loss 1.4575\n",
      "Epoch 1 Step 90 Loss 1.2957\n",
      "Epoch 1 Step 91 Loss 1.2201\n",
      "Epoch 1 Step 92 Loss 1.2201\n",
      "Epoch 1 Step 93 Loss 0.9762\n",
      "Epoch 1 Step 94 Loss 1.3502\n",
      "Epoch 1 Step 95 Loss 1.8517\n",
      "Epoch 1 Step 96 Loss 1.7689\n",
      "Epoch 1 Step 97 Loss 1.7275\n",
      "Epoch 1 Step 98 Loss 1.7432\n",
      "Epoch 1 Step 99 Loss 1.5597\n",
      "Epoch 1 Step 100 Loss 1.4611\n",
      "Epoch 1 Step 101 Loss 1.3757\n",
      "Epoch 1 Step 102 Loss 1.4180\n",
      "Epoch 1 Step 103 Loss 1.5956\n",
      "Epoch 1 Step 104 Loss 1.4458\n",
      "Epoch 1 Step 105 Loss 1.6593\n",
      "Epoch 1 Step 106 Loss 1.5183\n",
      "Epoch 1 Step 107 Loss 1.5948\n",
      "Epoch 1 Step 108 Loss 1.9278\n",
      "Epoch 1 Step 109 Loss 1.4164\n",
      "Epoch 1 Step 110 Loss 1.4608\n",
      "Epoch 1 Step 111 Loss 1.5068\n",
      "Epoch 1 Step 112 Loss 1.3945\n",
      "Epoch 1 Step 113 Loss 1.5191\n",
      "Epoch 1 Step 114 Loss 1.5965\n",
      "Epoch 1 Step 115 Loss 1.3622\n",
      "Epoch 1 Step 116 Loss 1.2902\n",
      "Epoch 1 Step 117 Loss 1.1655\n",
      "Epoch 1 Step 118 Loss 1.4026\n",
      "Epoch 1 Step 119 Loss 1.6819\n",
      "Epoch 1 Step 120 Loss 1.5565\n",
      "Epoch 1 Step 121 Loss 1.7680\n",
      "Epoch 1 Step 122 Loss 1.5383\n",
      "Epoch 1 Step 123 Loss 1.4429\n",
      "Epoch 1 Step 124 Loss 1.6785\n",
      "Epoch 1 Step 125 Loss 1.4788\n",
      "Epoch 1 Step 126 Loss 2.0442\n",
      "Epoch 1 Step 127 Loss 1.6760\n",
      "Epoch 1 Step 128 Loss 1.7128\n",
      "Epoch 1 Step 129 Loss 1.4166\n",
      "Epoch 1 Step 130 Loss 1.2224\n",
      "Epoch 1 Step 131 Loss 1.3511\n",
      "Epoch 1 Step 132 Loss 1.2980\n",
      "Epoch 1 Step 133 Loss 1.5021\n",
      "Epoch 1 Step 134 Loss 1.5698\n",
      "Epoch 1 Step 135 Loss 2.1878\n",
      "Epoch 1 Step 136 Loss 2.1983\n",
      "Epoch 1 Step 137 Loss 1.9468\n",
      "Epoch 1 Step 138 Loss 1.3963\n",
      "Epoch 1 Step 139 Loss 1.5433\n",
      "Epoch 1 Step 140 Loss 1.4864\n",
      "Epoch 1 Step 141 Loss 1.8773\n",
      "Epoch 1 Step 142 Loss 1.4025\n",
      "Epoch 1 Step 143 Loss 1.4910\n",
      "Epoch 1 Step 144 Loss 1.7047\n",
      "Epoch 1 Step 145 Loss 1.4330\n",
      "Epoch 1 Step 146 Loss 1.3587\n",
      "Epoch 1 Step 147 Loss 1.4399\n",
      "Epoch 1 Step 148 Loss 1.2862\n",
      "Epoch 1 Step 149 Loss 1.3580\n",
      "Epoch 1 Step 150 Loss 1.5640\n",
      "Epoch 1 Step 151 Loss 1.2538\n",
      "Epoch 1 Step 152 Loss 1.4754\n",
      "Epoch 1 Step 153 Loss 1.6771\n",
      "Epoch 1 Step 154 Loss 1.3820\n",
      "Epoch 1 Step 155 Loss 1.4579\n",
      "Epoch 1 Step 156 Loss 1.9298\n",
      "Epoch 1 Step 157 Loss 1.4237\n",
      "Epoch 1 Step 158 Loss 1.5014\n",
      "Epoch 1 Step 159 Loss 1.2786\n",
      "Epoch 1 Step 160 Loss 1.7180\n",
      "Epoch 1 Step 161 Loss 1.7585\n",
      "Epoch 1 Step 162 Loss 1.2874\n",
      "Epoch 1 Step 163 Loss 1.6633\n",
      "Epoch 1 Step 164 Loss 1.5306\n",
      "Epoch 1 Step 165 Loss 1.4534\n",
      "Epoch 1 Step 166 Loss 1.6219\n",
      "Epoch 1 Step 167 Loss 1.4183\n",
      "Epoch 1 Step 168 Loss 1.3404\n",
      "Epoch 1 Step 169 Loss 1.7378\n",
      "Epoch 1 Step 170 Loss 1.6072\n",
      "Epoch 1 Step 171 Loss 1.4259\n",
      "Epoch 1 Step 172 Loss 1.4355\n",
      "Epoch 1 Step 173 Loss 1.7012\n",
      "Epoch 1 Step 174 Loss 1.7902\n",
      "Epoch 1 Step 175 Loss 1.5414\n",
      "Epoch 1 Step 176 Loss 1.2773\n",
      "Epoch 1 Step 177 Loss 1.4009\n",
      "Epoch 1 Step 178 Loss 1.5706\n",
      "Epoch 1 Step 179 Loss 1.6653\n",
      "Epoch 1 Step 180 Loss 1.7294\n",
      "Epoch 1 Step 181 Loss 1.6984\n",
      "Epoch 1 Step 182 Loss 1.8520\n",
      "Epoch 1 Step 183 Loss 1.6105\n",
      "Epoch 1 Step 184 Loss 1.6936\n",
      "Epoch 1 Step 185 Loss 1.6059\n",
      "Epoch 1 Step 186 Loss 1.3519\n",
      "Epoch 1 Step 187 Loss 1.2847\n",
      "Epoch 1 Step 188 Loss 1.4164\n",
      "Epoch 1 Step 189 Loss 1.3760\n",
      "Epoch 1 Step 190 Loss 1.3287\n",
      "Epoch 1 Step 191 Loss 1.4804\n",
      "Epoch 1 Step 192 Loss 1.3551\n",
      "Epoch 1 Step 193 Loss 1.5757\n",
      "Epoch 1 Step 194 Loss 1.7319\n",
      "Epoch 1 Step 195 Loss 1.5058\n",
      "Epoch 1 Step 196 Loss 1.6826\n",
      "Epoch 1 Step 197 Loss 1.2485\n",
      "Epoch 1 Step 198 Loss 1.3060\n",
      "Epoch 1 Step 199 Loss 1.4431\n",
      "Epoch 1 Step 200 Loss 1.5780\n",
      "Epoch 1 Step 201 Loss 1.3230\n",
      "Epoch 1 Step 202 Loss 1.4929\n",
      "Epoch 1 Step 203 Loss 1.6509\n",
      "Epoch 1 Step 204 Loss 1.5712\n",
      "Epoch 1 Step 205 Loss 1.6020\n",
      "Epoch 1 Step 206 Loss 1.9023\n",
      "Epoch 1 Step 207 Loss 1.6693\n",
      "Epoch 1 Step 208 Loss 1.5901\n",
      "Epoch 1 Step 209 Loss 1.7144\n",
      "Epoch 1 Step 210 Loss 1.5910\n",
      "Epoch 1 Step 211 Loss 1.5683\n",
      "Epoch 1 Step 212 Loss 1.8546\n",
      "Epoch 1 Step 213 Loss 1.6439\n",
      "Epoch 1 Step 214 Loss 1.7950\n",
      "Epoch 1 Step 215 Loss 1.6628\n",
      "Epoch 1 Step 216 Loss 1.5235\n",
      "Epoch 1 Step 217 Loss 1.5003\n",
      "Epoch 1 Step 218 Loss 1.7207\n",
      "Epoch 1 Step 219 Loss 1.6717\n",
      "Epoch 1 Step 220 Loss 1.8223\n",
      "Epoch 1 Step 221 Loss 1.5386\n",
      "Epoch 1 Step 222 Loss 1.5634\n",
      "Epoch 1 Step 223 Loss 1.7051\n",
      "Epoch 1 Step 224 Loss 1.9103\n",
      "Epoch 1 Step 225 Loss 1.8461\n",
      "Epoch 1 Step 226 Loss 1.7753\n",
      "Epoch 1 Step 227 Loss 1.5498\n",
      "Epoch 1 Step 228 Loss 1.4152\n",
      "Epoch 1 Step 229 Loss 1.3247\n",
      "Epoch 1 Step 230 Loss 1.3989\n",
      "Epoch 1 Step 231 Loss 1.6976\n",
      "Epoch 1 Step 232 Loss 1.6819\n",
      "Epoch 1 Step 233 Loss 1.6957\n",
      "Epoch 1 Step 234 Loss 1.7831\n",
      "Epoch 1 Step 235 Loss 1.6183\n",
      "Epoch 1 Step 236 Loss 1.3791\n",
      "Epoch 1 Step 237 Loss 1.5278\n",
      "Epoch 1 Step 238 Loss 1.4903\n",
      "Epoch 1 Step 239 Loss 1.4629\n",
      "Epoch 1 Step 240 Loss 1.4479\n",
      "Epoch 1 Step 241 Loss 1.6881\n",
      "Epoch 1 Step 242 Loss 1.6291\n",
      "Epoch 1 Step 243 Loss 1.5539\n",
      "Epoch 1 Step 244 Loss 1.1298\n",
      "Epoch 1 Step 245 Loss 1.3081\n",
      "Epoch 1 Step 246 Loss 1.6067\n",
      "Epoch 1 Step 247 Loss 1.7755\n",
      "Epoch 1 Step 248 Loss 1.3267\n",
      "Epoch 1 Step 249 Loss 1.3447\n",
      "Epoch 1 Step 250 Loss 1.1476\n",
      "Epoch 1 Step 251 Loss 1.0427\n",
      "Epoch 1 Step 252 Loss 1.2923\n",
      "Epoch 1 Step 253 Loss 1.8810\n",
      "Epoch 1 Step 254 Loss 1.4703\n",
      "Epoch 1 Step 255 Loss 1.6804\n",
      "Epoch 1 Step 256 Loss 1.4873\n",
      "Epoch 1 Step 257 Loss 1.4973\n",
      "Epoch 1 Step 258 Loss 1.6791\n",
      "Epoch 1 Step 259 Loss 1.4384\n",
      "Epoch 1 Step 260 Loss 1.1967\n",
      "Epoch 1 Step 261 Loss 1.3742\n",
      "Epoch 1 Step 262 Loss 1.2951\n",
      "Epoch 1 Step 263 Loss 1.2374\n",
      "Epoch 1 Step 264 Loss 1.5355\n",
      "Epoch 1 Step 265 Loss 1.7895\n",
      "Epoch 1 Step 266 Loss 1.6219\n",
      "Epoch 1 Step 267 Loss 2.0280\n",
      "Epoch 1 Step 268 Loss 1.4453\n",
      "Epoch 1 Step 269 Loss 1.3738\n",
      "Epoch 1 Step 270 Loss 1.2788\n",
      "Epoch 1 Step 271 Loss 1.3474\n",
      "Epoch 1 Step 272 Loss 1.6109\n",
      "Epoch 1 Step 273 Loss 1.7667\n",
      "Epoch 1 Step 274 Loss 1.7030\n",
      "Epoch 1 Step 275 Loss 1.8536\n",
      "Epoch 1 Step 276 Loss 1.3739\n",
      "Epoch 1 Step 277 Loss 1.7326\n",
      "Epoch 1 Step 278 Loss 1.4328\n",
      "Epoch 1 Step 279 Loss 1.4334\n",
      "Epoch 1 Step 280 Loss 1.5192\n",
      "Epoch 1 Step 281 Loss 1.7357\n",
      "Epoch 1 Step 282 Loss 1.4536\n",
      "Epoch 1 Step 283 Loss 1.3751\n",
      "Epoch 1 Step 284 Loss 1.4447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 285 Loss 1.2565\n",
      "Epoch 1 Step 286 Loss 1.2878\n",
      "Epoch 1 Step 287 Loss 1.4938\n",
      "Epoch 1 Step 288 Loss 1.8572\n",
      "Epoch 1 Step 289 Loss 1.6122\n",
      "Epoch 1 Step 290 Loss 1.6246\n",
      "Epoch 1 Step 291 Loss 1.5670\n",
      "Epoch 1 Step 292 Loss 1.5945\n",
      "Epoch 1 Step 293 Loss 1.4165\n",
      "Epoch 1 Step 294 Loss 1.3098\n",
      "Epoch 1 Step 295 Loss 1.7176\n",
      "Epoch 1 Step 296 Loss 1.7223\n",
      "Epoch 1 Step 297 Loss 1.6943\n",
      "Epoch 1 Step 298 Loss 1.6069\n",
      "Epoch 1 Step 299 Loss 1.3955\n",
      "Epoch 1 Step 300 Loss 1.6190\n",
      "Epoch 1 Step 301 Loss 1.8479\n",
      "Epoch 1 Step 302 Loss 1.5356\n",
      "Epoch 1 Step 303 Loss 1.8408\n",
      "Epoch 1 Step 304 Loss 1.6648\n",
      "Epoch 1 Step 305 Loss 1.4143\n",
      "Epoch 1 Step 306 Loss 1.7094\n",
      "Epoch 1 Step 307 Loss 1.5851\n",
      "Epoch 1 Step 308 Loss 1.5710\n",
      "Epoch 1 Step 309 Loss 1.6333\n",
      "Epoch 1 Step 310 Loss 1.4250\n",
      "Epoch 1 Step 311 Loss 1.3075\n",
      "Epoch 1 Step 312 Loss 1.3422\n",
      "Epoch 1 Step 313 Loss 1.3995\n",
      "Epoch 1 Step 314 Loss 1.7133\n",
      "Epoch 1 Step 315 Loss 1.8418\n",
      "Epoch 1 Step 316 Loss 1.5623\n",
      "Epoch 1 Step 317 Loss 1.7417\n",
      "Epoch 1 Step 318 Loss 1.5493\n",
      "Epoch 1 Step 319 Loss 1.6308\n",
      "Epoch 1 Step 320 Loss 1.4862\n",
      "Epoch 1 Step 321 Loss 1.3043\n",
      "Epoch 1 Step 322 Loss 1.3569\n",
      "Epoch 1 Step 323 Loss 1.6783\n",
      "Epoch 1 Step 324 Loss 1.6246\n",
      "Epoch 1 Step 325 Loss 1.7266\n",
      "Epoch 1 Step 326 Loss 1.6922\n",
      "Epoch 1 Step 327 Loss 1.8097\n",
      "Epoch 1 Step 328 Loss 1.7534\n",
      "Epoch 1 Step 329 Loss 1.6427\n",
      "Epoch 1 Step 330 Loss 1.5996\n",
      "Epoch 1 Step 331 Loss 1.8010\n",
      "Epoch 1 Step 332 Loss 1.5720\n",
      "Epoch 1 Step 333 Loss 1.9286\n",
      "Epoch 1 Step 334 Loss 1.5375\n",
      "Epoch 1 Step 335 Loss 1.5528\n",
      "Epoch 1 Step 336 Loss 1.5950\n",
      "Epoch 1 Step 337 Loss 1.3501\n",
      "Epoch 1 Step 338 Loss 1.5112\n",
      "Epoch 1 Step 339 Loss 1.5567\n",
      "Epoch 1 Step 340 Loss 1.5597\n",
      "Epoch 1 Step 341 Loss 1.9048\n",
      "Epoch 1 Step 342 Loss 1.6990\n",
      "Epoch 1 Step 343 Loss 1.2345\n",
      "Epoch 1 Step 344 Loss 1.8685\n",
      "Epoch 1 Step 345 Loss 1.9300\n",
      "Epoch 1 Step 346 Loss 1.5897\n",
      "Epoch 1 Step 347 Loss 1.7053\n",
      "Epoch 1 Step 348 Loss 1.4645\n",
      "Epoch 1 Step 349 Loss 1.3597\n",
      "Epoch 1 Step 350 Loss 1.4031\n",
      "Epoch 1 Step 351 Loss 1.4215\n",
      "Epoch 1 Step 352 Loss 1.4229\n",
      "Epoch 1 Step 353 Loss 1.6772\n",
      "Epoch 1 Step 354 Loss 1.9112\n",
      "Epoch 1 Step 355 Loss 1.7789\n",
      "Epoch 1 Step 356 Loss 1.3952\n",
      "Epoch 1 Step 357 Loss 1.4096\n",
      "Epoch 1 Step 358 Loss 1.7151\n",
      "Epoch 1 Step 359 Loss 1.6313\n",
      "Epoch 1 Step 360 Loss 1.7485\n",
      "Epoch 1 Step 361 Loss 1.7691\n",
      "Epoch 1 Step 362 Loss 1.6531\n",
      "Epoch 1 Step 363 Loss 1.5456\n",
      "Epoch 1 Step 364 Loss 1.2807\n",
      "Epoch 1 Step 365 Loss 1.3241\n",
      "Epoch 1 Step 366 Loss 1.1912\n",
      "Epoch 1 Step 367 Loss 1.1979\n",
      "Epoch 1 Step 368 Loss 1.2132\n",
      "Epoch 1 Step 369 Loss 1.1180\n",
      "Epoch 1 Step 370 Loss 1.3652\n",
      "Epoch 1 Step 371 Loss 1.3559\n",
      "Epoch 1 Step 372 Loss 1.8950\n",
      "Epoch 1 Step 373 Loss 1.4601\n",
      "Epoch 1 Step 374 Loss 1.4269\n",
      "Epoch 1 Step 375 Loss 1.4519\n",
      "Epoch 1 Step 376 Loss 1.5426\n",
      "Epoch 1 Step 377 Loss 1.2125\n",
      "Epoch 1 Step 378 Loss 1.5746\n",
      "Epoch 1 Step 379 Loss 1.6970\n",
      "Epoch 1 Step 380 Loss 1.7049\n",
      "Epoch 1 Step 381 Loss 1.9275\n",
      "Epoch 1 Step 382 Loss 1.5758\n",
      "Epoch 1 Step 383 Loss 1.7112\n",
      "Epoch 1 Step 384 Loss 2.0374\n",
      "Epoch 1 Step 385 Loss 1.9262\n",
      "Epoch 1 Step 386 Loss 1.9475\n",
      "Epoch 1 Step 387 Loss 1.6471\n",
      "Epoch 1 Step 388 Loss 1.6089\n",
      "Epoch 1 Step 389 Loss 1.5901\n",
      "Epoch 1 Step 390 Loss 1.5423\n",
      "Epoch 1 Step 391 Loss 1.4488\n",
      "Epoch 1 Step 392 Loss 1.7619\n",
      "Epoch 1 Step 393 Loss 1.4910\n",
      "Epoch 1 Step 394 Loss 1.4994\n",
      "Epoch 1 Step 395 Loss 1.4948\n",
      "Epoch 1 Step 396 Loss 1.2822\n",
      "Epoch 1 Step 397 Loss 1.0584\n",
      "Epoch 1 Step 398 Loss 1.2616\n",
      "Epoch 1 Step 399 Loss 1.3220\n",
      "Epoch 1 Step 400 Loss 1.6053\n",
      "Epoch 1 Step 401 Loss 1.2975\n",
      "Epoch 1 Step 402 Loss 1.3288\n",
      "Epoch 1 Step 403 Loss 1.5525\n",
      "Epoch 1 Step 404 Loss 1.7699\n",
      "Epoch 1 Step 405 Loss 1.8738\n",
      "Epoch 1 Step 406 Loss 1.6364\n",
      "Epoch 1 Step 407 Loss 1.6739\n",
      "Epoch 1 Step 408 Loss 1.6212\n",
      "Epoch 1 Step 409 Loss 1.8991\n",
      "Epoch 1 Step 410 Loss 2.0433\n",
      "Epoch 1 Step 411 Loss 1.4296\n",
      "Epoch 1 Step 412 Loss 1.4545\n",
      "Epoch 1 Step 413 Loss 1.5630\n",
      "Epoch 1 Step 414 Loss 1.7740\n",
      "Epoch 1 Step 415 Loss 1.9502\n",
      "Epoch 1 Step 416 Loss 1.5453\n",
      "Epoch 1 Step 417 Loss 1.2210\n",
      "Epoch 1 Step 418 Loss 1.3948\n",
      "Epoch 1 Step 419 Loss 1.6395\n",
      "Epoch 1 Step 420 Loss 1.4327\n",
      "Epoch 1 Step 421 Loss 1.3935\n",
      "Epoch 1 Step 422 Loss 1.5663\n",
      "Epoch 1 Step 423 Loss 1.5884\n",
      "Epoch 1 Step 424 Loss 1.4773\n",
      "Epoch 1 Step 425 Loss 1.6071\n",
      "Epoch 1 Step 426 Loss 1.8297\n",
      "Epoch 1 Step 427 Loss 1.3799\n",
      "Epoch 1 Step 428 Loss 1.3432\n",
      "Epoch 1 Step 429 Loss 1.3518\n",
      "Epoch 1 Step 430 Loss 1.6145\n",
      "Epoch 1 Step 431 Loss 1.6448\n",
      "Epoch 1 Step 432 Loss 1.7620\n",
      "Epoch 1 Step 433 Loss 1.6728\n",
      "Epoch 1 Step 434 Loss 1.5216\n",
      "Epoch 1 Step 435 Loss 1.2869\n",
      "Epoch 1 Step 436 Loss 1.2505\n",
      "Epoch 1 Step 437 Loss 1.2688\n",
      "Epoch 1 Step 438 Loss 1.4550\n",
      "Epoch 1 Step 439 Loss 1.3469\n",
      "Epoch 1 Step 440 Loss 1.3233\n",
      "Epoch 1 Step 441 Loss 1.5562\n",
      "Epoch 1 Step 442 Loss 1.6002\n",
      "Epoch 1 Step 443 Loss 1.4487\n",
      "Epoch 1 Step 444 Loss 1.3049\n",
      "Epoch 1 Step 445 Loss 1.4542\n",
      "Epoch 1 Step 446 Loss 1.3764\n",
      "Epoch 1 Step 447 Loss 1.4767\n",
      "Epoch 1 Step 448 Loss 1.4164\n",
      "Epoch 1 Step 449 Loss 1.3182\n",
      "Epoch 1 Step 450 Loss 1.5191\n",
      "Epoch 1 Step 451 Loss 1.6564\n",
      "Epoch 1 Step 452 Loss 1.6273\n",
      "Epoch 1 Step 453 Loss 1.9507\n",
      "Epoch 1 Step 454 Loss 1.6910\n",
      "Epoch 1 Step 455 Loss 1.6467\n",
      "Epoch 1 Step 456 Loss 1.4407\n",
      "Epoch 1 Step 457 Loss 1.8286\n",
      "Epoch 1 Step 458 Loss 1.8017\n",
      "Epoch 1 Step 459 Loss 1.5766\n",
      "Epoch 1 Step 460 Loss 1.7251\n",
      "Epoch 1 Step 461 Loss 1.3408\n",
      "Epoch 1 Step 462 Loss 1.6137\n",
      "Epoch 1 Step 463 Loss 1.8877\n",
      "Epoch 1 Step 464 Loss 1.8911\n",
      "Epoch 1 Step 465 Loss 2.0622\n",
      "Epoch 1 Step 466 Loss 2.0036\n",
      "Epoch 1 Step 467 Loss 1.9487\n",
      "Epoch 1 Step 468 Loss 1.9470\n",
      "Epoch 1 Step 469 Loss 1.6812\n",
      "Epoch 1 Step 470 Loss 1.4048\n",
      "Epoch 1 Step 471 Loss 1.3802\n",
      "Epoch 1 Step 472 Loss 1.4921\n",
      "Epoch 1 Step 473 Loss 1.4757\n",
      "Epoch 1 Step 474 Loss 1.8506\n",
      "Epoch 1 Step 475 Loss 1.7386\n",
      "Epoch 1 Step 476 Loss 1.4042\n",
      "Epoch 1 Step 477 Loss 1.4149\n",
      "Epoch 1 Step 478 Loss 1.7579\n",
      "Epoch 1 Step 479 Loss 1.7900\n",
      "Epoch 1 Step 480 Loss 1.7523\n",
      "Epoch 1 Step 481 Loss 1.6289\n",
      "Epoch 1 Step 482 Loss 1.5548\n",
      "Epoch 1 Step 483 Loss 1.5363\n",
      "Epoch 1 Step 484 Loss 1.3482\n",
      "Epoch 1 Step 485 Loss 1.5119\n",
      "Epoch 1 Step 486 Loss 1.5188\n",
      "Epoch 1 Step 487 Loss 1.5568\n",
      "Epoch 1 Step 488 Loss 1.3687\n",
      "Epoch 1 Step 489 Loss 1.4848\n",
      "Epoch 1 Step 490 Loss 1.2358\n",
      "Epoch 1 Step 491 Loss 1.7458\n",
      "Epoch 1 Step 492 Loss 1.5099\n",
      "Epoch 1 Step 493 Loss 1.6670\n",
      "Epoch 1 Step 494 Loss 1.4895\n",
      "Epoch 1 Step 495 Loss 1.2680\n",
      "Epoch 1 Step 496 Loss 1.4489\n",
      "Epoch 1 Step 497 Loss 1.3339\n",
      "Epoch 1 Step 498 Loss 1.5899\n",
      "Epoch 1 Step 499 Loss 1.4066\n",
      "Epoch 1 Step 500 Loss 1.7252\n",
      "Epoch 1 Step 501 Loss 1.5499\n",
      "Epoch 1 Step 502 Loss 1.5595\n",
      "Epoch 1 Step 503 Loss 1.6495\n",
      "Epoch 1 Step 504 Loss 1.5252\n",
      "Epoch 1 Step 505 Loss 1.2620\n",
      "Epoch 1 Step 506 Loss 1.7781\n",
      "Epoch 1 Step 507 Loss 1.6114\n",
      "Epoch 1 Step 508 Loss 1.4695\n",
      "Epoch 1 Step 509 Loss 1.4037\n",
      "Epoch 1 Step 510 Loss 1.4112\n",
      "Epoch 1 Step 511 Loss 1.7974\n",
      "Epoch 1 Step 512 Loss 1.6596\n",
      "Epoch 1 Step 513 Loss 1.5069\n",
      "Epoch 1 Step 514 Loss 1.3738\n",
      "Epoch 1 Step 515 Loss 1.5107\n",
      "Epoch 1 Step 516 Loss 1.4460\n",
      "Epoch 1 Step 517 Loss 1.3553\n",
      "Epoch 1 Step 518 Loss 1.4103\n",
      "Epoch 1 Step 519 Loss 1.4820\n",
      "Epoch 1 Step 520 Loss 1.2493\n",
      "Epoch 1 Step 521 Loss 1.3150\n",
      "Epoch 1 Step 522 Loss 1.3285\n",
      "Epoch 1 Step 523 Loss 1.3028\n",
      "Epoch 1 Step 524 Loss 1.8787\n",
      "Epoch 1 Step 525 Loss 1.7681\n",
      "Epoch 1 Step 526 Loss 1.7202\n",
      "Epoch 1 Step 527 Loss 1.5869\n",
      "Epoch 1 Step 528 Loss 1.4476\n",
      "Epoch 1 Step 529 Loss 1.4949\n",
      "Epoch 1 Step 530 Loss 1.6172\n",
      "Epoch 1 Step 531 Loss 1.2329\n",
      "Epoch 1 Step 532 Loss 1.2476\n",
      "Epoch 1 Step 533 Loss 1.4201\n",
      "Epoch 1 Step 534 Loss 1.3352\n",
      "Epoch 1 Step 535 Loss 1.1576\n",
      "Epoch 1 Step 536 Loss 1.5568\n",
      "Epoch 1 Step 537 Loss 1.3257\n",
      "Epoch 1 Step 538 Loss 1.6050\n",
      "Epoch 1 Step 539 Loss 1.4747\n",
      "Epoch 1 Step 540 Loss 1.6165\n",
      "Epoch 1 Step 541 Loss 1.6203\n",
      "Epoch 1 Step 542 Loss 1.3732\n",
      "Epoch 1 Step 543 Loss 1.7121\n",
      "Epoch 1 Step 544 Loss 1.7656\n",
      "Epoch 1 Step 545 Loss 1.8118\n",
      "Epoch 1 Step 546 Loss 1.7314\n",
      "Epoch 1 Step 547 Loss 1.6702\n",
      "Epoch 1 Step 548 Loss 1.6904\n",
      "Epoch 1 Step 549 Loss 1.6521\n",
      "Epoch 1 Step 550 Loss 1.5883\n",
      "Epoch 1 Step 551 Loss 1.2819\n",
      "Epoch 1 Step 552 Loss 1.6807\n",
      "Epoch 1 Step 553 Loss 1.5457\n",
      "Epoch 1 Step 554 Loss 1.9348\n",
      "Epoch 1 Step 555 Loss 1.6489\n",
      "Epoch 1 Step 556 Loss 1.3900\n",
      "Epoch 1 Step 557 Loss 1.6447\n",
      "Epoch 1 Step 558 Loss 1.3875\n",
      "Epoch 1 Step 559 Loss 1.5976\n",
      "Epoch 1 Step 560 Loss 1.7406\n",
      "Epoch 1 Step 561 Loss 1.6491\n",
      "Epoch 1 Step 562 Loss 1.7769\n",
      "Epoch 1 Step 563 Loss 1.7039\n",
      "Epoch 1 Step 564 Loss 1.7845\n",
      "Epoch 1 Step 565 Loss 1.8892\n",
      "Epoch 1 Step 566 Loss 1.6334\n",
      "Epoch 1 Step 567 Loss 1.8229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 568 Loss 1.5719\n",
      "Epoch 1 Step 569 Loss 1.8428\n",
      "Epoch 1 Step 570 Loss 1.8017\n",
      "Epoch 1 Step 571 Loss 1.3925\n",
      "Epoch 1 Step 572 Loss 1.2540\n",
      "Epoch 1 Step 573 Loss 1.3535\n",
      "Epoch 1 Step 574 Loss 1.4959\n",
      "Epoch 1 Step 575 Loss 1.6224\n",
      "Epoch 1 Step 576 Loss 1.6155\n",
      "Epoch 1 Step 577 Loss 1.7281\n",
      "Epoch 1 Step 578 Loss 1.7099\n",
      "Epoch 1 Step 579 Loss 1.5102\n",
      "Epoch 1 Step 580 Loss 1.6561\n",
      "Epoch 1 Step 581 Loss 1.3339\n",
      "Epoch 1 Step 582 Loss 1.6021\n",
      "Epoch 1 Step 583 Loss 1.6234\n",
      "Epoch 1 Step 584 Loss 1.5980\n",
      "Epoch 1 Step 585 Loss 1.1881\n",
      "Epoch 1 Step 586 Loss 1.3655\n",
      "Epoch 1 Step 587 Loss 1.0363\n",
      "Epoch 1 Step 588 Loss 1.3266\n",
      "Epoch 1 Step 589 Loss 1.3545\n",
      "Epoch 1 Step 590 Loss 1.3855\n",
      "Epoch 1 Step 591 Loss 1.4297\n",
      "Epoch 1 Step 592 Loss 1.5634\n",
      "Epoch 1 Step 593 Loss 1.9073\n",
      "Epoch 1 Step 594 Loss 1.6187\n",
      "Epoch 1 Step 595 Loss 1.7126\n",
      "Epoch 1 Step 596 Loss 1.7541\n",
      "Epoch 1 Step 597 Loss 1.7585\n",
      "Epoch 1 Step 598 Loss 1.4500\n",
      "Epoch 1 Step 599 Loss 1.1699\n",
      "Epoch 1 Step 600 Loss 1.6550\n",
      "Epoch 1 Step 601 Loss 1.4049\n",
      "Epoch 1 Step 602 Loss 1.2557\n",
      "Epoch 1 Step 603 Loss 1.4826\n",
      "Epoch 1 Step 604 Loss 1.5817\n",
      "Epoch 1 Step 605 Loss 1.6474\n",
      "Epoch 1 Step 606 Loss 1.4924\n",
      "Epoch 1 Step 607 Loss 1.5548\n",
      "Epoch 1 Step 608 Loss 1.7410\n",
      "Epoch 1 Step 609 Loss 1.5037\n",
      "Epoch 1 Step 610 Loss 1.5655\n",
      "Epoch 1 Step 611 Loss 1.6076\n",
      "Epoch 1 Step 612 Loss 1.5760\n",
      "Epoch 1 Step 613 Loss 1.5526\n",
      "Epoch 1 Step 614 Loss 1.4421\n",
      "Epoch 1 Step 615 Loss 1.8222\n",
      "Epoch 1 Step 616 Loss 1.9627\n",
      "Epoch 1 Step 617 Loss 1.6057\n",
      "Epoch 1 Step 618 Loss 1.3832\n",
      "Epoch 1 Step 619 Loss 1.3293\n",
      "Epoch 1 Step 620 Loss 1.3678\n",
      "Epoch 1 Step 621 Loss 1.5985\n",
      "Epoch 1 Step 622 Loss 1.6974\n",
      "Epoch 1 Step 623 Loss 1.6892\n",
      "Epoch 1 Step 624 Loss 1.6160\n",
      "Epoch 1 Step 625 Loss 1.7519\n",
      "Epoch 1 Step 626 Loss 1.6104\n",
      "Epoch 1 Step 627 Loss 1.6591\n",
      "Epoch 1 Step 628 Loss 1.4060\n",
      "Epoch 1 Step 629 Loss 1.4571\n",
      "Epoch 1 Step 630 Loss 1.6154\n",
      "Epoch 1 Step 631 Loss 1.7041\n",
      "Epoch 1 Step 632 Loss 1.8089\n",
      "Epoch 1 Step 633 Loss 1.4470\n",
      "Epoch 1 Step 634 Loss 1.7709\n",
      "Epoch 1 Step 635 Loss 1.9173\n",
      "Epoch 1 Step 636 Loss 1.8859\n",
      "Epoch 1 Step 637 Loss 1.8945\n",
      "Epoch 1 Step 638 Loss 1.8116\n",
      "Epoch 1 Step 639 Loss 1.5302\n",
      "Epoch 1 Step 640 Loss 1.6589\n",
      "Epoch 1 Step 641 Loss 1.3101\n",
      "Epoch 1 Step 642 Loss 1.3084\n",
      "Epoch 1 Step 643 Loss 1.5074\n",
      "Epoch 1 Step 644 Loss 1.3450\n",
      "Epoch 1 Step 645 Loss 1.4307\n",
      "Epoch 1 Step 646 Loss 1.1658\n",
      "Epoch 1 Step 647 Loss 1.2150\n",
      "Epoch 1 Step 648 Loss 0.9779\n",
      "Epoch 1 Step 649 Loss 1.1102\n",
      "Epoch 1 Step 650 Loss 1.0558\n",
      "Epoch 1 Step 651 Loss 1.2988\n",
      "Epoch 1 Step 652 Loss 1.4727\n",
      "Epoch 1 Step 653 Loss 1.3498\n",
      "Epoch 1 Step 654 Loss 1.6578\n",
      "Epoch 1 Step 655 Loss 1.2949\n",
      "Epoch 1 Step 656 Loss 1.2573\n",
      "Epoch 1 Step 657 Loss 1.3838\n",
      "Epoch 1 Step 658 Loss 1.2578\n",
      "Epoch 1 Step 659 Loss 1.6607\n",
      "Epoch 1 Step 660 Loss 1.3097\n",
      "Epoch 1 Step 661 Loss 1.5510\n",
      "Epoch 1 Step 662 Loss 1.9016\n",
      "Epoch 1 Step 663 Loss 1.5256\n",
      "Epoch 1 Step 664 Loss 1.4560\n",
      "Epoch 1 Step 665 Loss 1.0030\n",
      "Epoch 1 Step 666 Loss 1.4632\n",
      "Epoch 1 Step 667 Loss 1.3716\n",
      "Epoch 1 Step 668 Loss 1.9089\n",
      "Epoch 1 Step 669 Loss 1.8528\n",
      "Epoch 1 Step 670 Loss 1.5379\n",
      "Epoch 1 Step 671 Loss 1.5206\n",
      "Epoch 1 Step 672 Loss 1.3575\n",
      "Epoch 1 Step 673 Loss 1.4717\n",
      "Epoch 1 Step 674 Loss 1.5673\n",
      "Epoch 1 Step 675 Loss 1.5721\n",
      "Epoch 1 Step 676 Loss 1.4796\n",
      "Epoch 1 Step 677 Loss 1.5030\n",
      "Epoch 1 Step 678 Loss 1.6451\n",
      "Epoch 1 Step 679 Loss 1.4126\n",
      "Epoch 1 Step 680 Loss 1.6426\n",
      "Epoch 1 Step 681 Loss 1.4523\n",
      "Epoch 1 Step 682 Loss 1.2306\n",
      "Epoch 1 Step 683 Loss 1.3842\n",
      "Epoch 1 Step 684 Loss 1.5891\n",
      "Epoch 1 Step 685 Loss 1.5033\n",
      "Epoch 1 Step 686 Loss 1.6529\n",
      "Epoch 1 Step 687 Loss 1.2896\n",
      "Epoch 1 Step 688 Loss 1.1001\n",
      "Epoch 1 Step 689 Loss 1.4710\n",
      "Epoch 1 Step 690 Loss 1.7573\n",
      "Epoch 1 Step 691 Loss 1.3072\n",
      "Epoch 1 Step 692 Loss 1.6849\n",
      "Epoch 1 Step 693 Loss 1.3687\n",
      "Epoch 1 Step 694 Loss 1.5464\n",
      "Epoch 1 Step 695 Loss 1.6050\n",
      "Epoch 1 Step 696 Loss 1.5700\n",
      "Epoch 1 Step 697 Loss 1.5362\n",
      "Epoch 1 Step 698 Loss 1.3232\n",
      "Epoch 1 Step 699 Loss 1.4029\n",
      "Epoch 1 Step 700 Loss 1.0572\n",
      "Epoch 1 Step 701 Loss 1.2824\n",
      "Epoch 1 Step 702 Loss 1.8364\n",
      "Epoch 1 Step 703 Loss 1.5726\n",
      "Epoch 1 Step 704 Loss 1.8652\n",
      "Epoch 1 Step 705 Loss 1.6241\n",
      "Epoch 1 Step 706 Loss 1.2019\n",
      "Epoch 1 Step 707 Loss 1.1740\n",
      "Epoch 1 Step 708 Loss 1.2318\n",
      "Epoch 1 Step 709 Loss 1.5046\n",
      "Epoch 1 Step 710 Loss 1.5299\n",
      "Epoch 1 Step 711 Loss 1.4889\n",
      "Epoch 1 Step 712 Loss 1.4164\n",
      "Epoch 1 Step 713 Loss 1.6776\n",
      "Epoch 1 Step 714 Loss 1.2898\n",
      "Epoch 1 Step 715 Loss 1.5255\n",
      "Epoch 1 Step 716 Loss 1.5855\n",
      "Epoch 1 Step 717 Loss 1.8324\n",
      "Epoch 1 Step 718 Loss 1.7188\n",
      "Epoch 1 Step 719 Loss 1.5574\n",
      "Epoch 1 Step 720 Loss 1.5753\n",
      "Epoch 1 Step 721 Loss 1.7367\n",
      "Epoch 1 Step 722 Loss 2.0025\n",
      "Epoch 1 Step 723 Loss 1.4939\n",
      "Epoch 1 Step 724 Loss 1.4608\n",
      "Epoch 1 Step 725 Loss 1.4951\n",
      "Epoch 1 Step 726 Loss 1.3941\n",
      "Epoch 1 Step 727 Loss 1.4743\n",
      "Epoch 1 Step 728 Loss 1.1908\n",
      "Epoch 1 Step 729 Loss 1.0438\n",
      "Epoch 1 Step 730 Loss 0.8313\n",
      "Epoch 1 Step 731 Loss 1.3839\n",
      "Epoch 1 Step 732 Loss 1.2933\n",
      "Epoch 1 Step 733 Loss 1.7528\n",
      "Epoch 1 Step 734 Loss 1.5880\n",
      "Epoch 1 Step 735 Loss 1.5357\n",
      "Epoch 1 Step 736 Loss 1.4109\n",
      "Epoch 1 Step 737 Loss 1.5954\n",
      "Epoch 1 Step 738 Loss 1.5024\n",
      "Epoch 1 Step 739 Loss 1.3060\n",
      "Epoch 1 Step 740 Loss 1.5635\n",
      "Epoch 1 Step 741 Loss 1.7406\n",
      "Epoch 1 Step 742 Loss 1.3389\n",
      "Epoch 1 Step 743 Loss 1.2416\n",
      "Epoch 1 Step 744 Loss 1.2508\n",
      "Epoch 1 Step 745 Loss 1.5026\n",
      "Epoch 1 Step 746 Loss 1.6831\n",
      "Epoch 1 Step 747 Loss 1.6551\n",
      "Epoch 1 Step 748 Loss 1.2692\n",
      "Epoch 1 Step 749 Loss 1.4262\n",
      "Epoch 1 Step 750 Loss 1.4874\n",
      "Epoch 1 Step 751 Loss 1.6830\n",
      "Epoch 1 Step 752 Loss 1.1936\n",
      "Epoch 1 Step 753 Loss 1.4260\n",
      "Epoch 1 Step 754 Loss 1.3787\n",
      "Epoch 1 Step 755 Loss 1.7622\n",
      "Epoch 1 Step 756 Loss 1.8014\n",
      "Epoch 1 Step 757 Loss 1.4614\n",
      "Epoch 1 Step 758 Loss 1.8386\n",
      "Epoch 1 Step 759 Loss 1.2952\n",
      "Epoch 1 Step 760 Loss 1.6709\n",
      "Epoch 1 Step 761 Loss 1.7044\n",
      "Epoch 1 Step 762 Loss 1.4946\n",
      "Epoch 1 Step 763 Loss 1.4930\n",
      "Epoch 1 Step 764 Loss 1.6387\n",
      "Epoch 1 Step 765 Loss 1.6235\n",
      "Epoch 1 Step 766 Loss 1.9939\n",
      "Epoch 1 Step 767 Loss 1.5759\n",
      "Epoch 1 Step 768 Loss 1.5259\n",
      "Epoch 1 Step 769 Loss 1.5916\n",
      "Epoch 1 Step 770 Loss 1.2966\n",
      "Epoch 1 Step 771 Loss 1.1731\n",
      "Epoch 1 Step 772 Loss 1.6367\n",
      "Epoch 1 Step 773 Loss 1.6743\n",
      "Epoch 1 Step 774 Loss 1.3161\n",
      "Epoch 1 Step 775 Loss 1.5896\n",
      "Epoch 1 Step 776 Loss 1.4574\n",
      "Epoch 1 Step 777 Loss 1.5932\n",
      "Epoch 1 Step 778 Loss 1.4739\n",
      "Epoch 1 Step 779 Loss 1.2754\n",
      "Epoch 1 Step 780 Loss 1.5250\n",
      "Epoch 1 Step 781 Loss 1.5851\n",
      "Epoch 1 Step 782 Loss 1.5728\n",
      "Epoch 1 Step 783 Loss 1.6740\n",
      "Epoch 1 Step 784 Loss 1.7132\n",
      "Epoch 1 Step 785 Loss 1.7350\n",
      "Epoch 1 Step 786 Loss 1.9789\n",
      "Epoch 1 Step 787 Loss 1.6443\n",
      "Epoch 1 Step 788 Loss 1.6374\n",
      "Epoch 1 Step 789 Loss 1.5369\n",
      "Epoch 1 Step 790 Loss 1.5604\n",
      "Epoch 1 Step 791 Loss 1.4880\n",
      "Epoch 1 Step 792 Loss 1.8589\n",
      "Epoch 1 Step 793 Loss 1.4763\n",
      "Epoch 1 Step 794 Loss 1.5699\n",
      "Epoch 1 Step 795 Loss 1.5716\n",
      "Epoch 1 Step 796 Loss 1.3964\n",
      "Epoch 1 Step 797 Loss 1.7708\n",
      "Epoch 1 Step 798 Loss 1.4484\n",
      "Epoch 1 Step 799 Loss 1.7368\n",
      "Epoch 1 Step 800 Loss 1.8987\n",
      "Epoch 1 Step 801 Loss 1.7650\n",
      "Epoch 1 Step 802 Loss 2.0257\n",
      "Epoch 1 Step 803 Loss 1.6486\n",
      "Epoch 1 Step 804 Loss 1.6036\n",
      "Epoch 1 Step 805 Loss 1.3614\n",
      "Epoch 1 Step 806 Loss 1.6170\n",
      "Epoch 1 Step 807 Loss 1.3427\n",
      "Epoch 1 Step 808 Loss 1.4014\n",
      "Epoch 1 Step 809 Loss 1.6171\n",
      "Epoch 1 Step 810 Loss 1.2956\n",
      "Epoch 1 Step 811 Loss 1.4631\n",
      "Epoch 1 Step 812 Loss 1.8017\n",
      "Epoch 1 Step 813 Loss 1.7482\n",
      "Epoch 1 Step 814 Loss 1.9786\n",
      "Epoch 1 Step 815 Loss 1.7004\n",
      "Epoch 1 Step 816 Loss 1.7317\n",
      "Epoch 1 Step 817 Loss 1.6474\n",
      "Epoch 1 Step 818 Loss 1.4863\n",
      "Epoch 1 Step 819 Loss 1.4269\n",
      "Epoch 1 Step 820 Loss 1.6224\n",
      "Epoch 1 Step 821 Loss 1.5252\n",
      "Epoch 1 Step 822 Loss 1.2034\n",
      "Epoch 1 Step 823 Loss 1.1722\n",
      "Epoch 1 Step 824 Loss 1.5199\n",
      "Epoch 1 Step 825 Loss 1.4766\n",
      "Epoch 1 Step 826 Loss 1.3751\n",
      "Epoch 1 Step 827 Loss 1.6694\n",
      "Epoch 1 Step 828 Loss 1.8410\n",
      "Epoch 1 Step 829 Loss 1.6659\n",
      "Epoch 1 Step 830 Loss 1.3386\n",
      "Epoch 1 Step 831 Loss 1.8458\n",
      "Epoch 1 Step 832 Loss 1.8041\n",
      "Epoch 1 Step 833 Loss 1.6163\n",
      "Epoch 1 Step 834 Loss 1.8527\n",
      "Epoch 1 Step 835 Loss 1.6534\n",
      "Epoch 1 Step 836 Loss 1.5738\n",
      "Epoch 1 Step 837 Loss 1.6006\n",
      "Epoch 1 Step 838 Loss 1.8077\n",
      "Epoch 1 Step 839 Loss 1.7695\n",
      "Epoch 1 Step 840 Loss 1.5002\n",
      "Epoch 1 Step 841 Loss 1.1087\n",
      "Epoch 1 Step 842 Loss 1.2904\n",
      "Epoch 1 Step 843 Loss 1.4877\n",
      "Epoch 1 Step 844 Loss 1.2840\n",
      "Epoch 1 Step 845 Loss 1.4380\n",
      "Epoch 1 Step 846 Loss 1.6090\n",
      "Epoch 1 Step 847 Loss 1.6484\n",
      "Epoch 1 Step 848 Loss 1.8632\n",
      "Epoch 1 Step 849 Loss 1.7863\n",
      "Epoch 1 Step 850 Loss 1.6377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 851 Loss 1.5697\n",
      "Epoch 1 Step 852 Loss 1.4947\n",
      "Epoch 1 Step 853 Loss 1.6987\n",
      "Epoch 1 Step 854 Loss 1.8966\n",
      "Epoch 1 Step 855 Loss 1.7274\n",
      "Epoch 1 Step 856 Loss 1.7030\n",
      "Epoch 1 Step 857 Loss 1.5574\n",
      "Epoch 1 Step 858 Loss 1.4223\n",
      "Epoch 1 Step 859 Loss 1.6918\n",
      "Epoch 1 Step 860 Loss 1.5309\n",
      "Epoch 1 Step 861 Loss 1.6159\n",
      "Epoch 1 Step 862 Loss 1.7107\n",
      "Epoch 1 Step 863 Loss 1.4933\n",
      "Epoch 1 Step 864 Loss 1.6759\n",
      "Epoch 1 Step 865 Loss 1.7065\n",
      "Epoch 1 Step 866 Loss 1.2270\n",
      "Epoch 1 Step 867 Loss 1.1809\n",
      "Epoch 1 Step 868 Loss 2.0018\n",
      "Epoch 1 Step 869 Loss 1.4630\n",
      "Epoch 1 Step 870 Loss 1.6152\n",
      "Epoch 1 Step 871 Loss 1.3056\n",
      "Epoch 1 Step 872 Loss 1.4176\n",
      "Epoch 1 Step 873 Loss 1.4448\n",
      "Epoch 1 Step 874 Loss 1.6528\n",
      "Epoch 1 Step 875 Loss 1.4799\n",
      "Epoch 1 Step 876 Loss 1.8080\n",
      "Epoch 1 Step 877 Loss 1.6132\n",
      "Epoch 1 Step 878 Loss 1.7325\n",
      "Epoch 1 Step 879 Loss 1.4573\n",
      "Epoch 1 Step 880 Loss 1.6093\n",
      "Epoch 1 Step 881 Loss 1.6043\n",
      "Epoch 1 Step 882 Loss 1.4053\n",
      "Epoch 1 Step 883 Loss 1.6657\n",
      "Epoch 1 Step 884 Loss 1.9029\n",
      "Epoch 1 Step 885 Loss 1.9395\n",
      "Epoch 1 Step 886 Loss 1.6729\n",
      "Epoch 1 Step 887 Loss 1.5596\n",
      "Epoch 1 Step 888 Loss 1.4662\n",
      "Epoch 1 Step 889 Loss 1.7303\n",
      "Epoch 1 Step 890 Loss 1.9455\n",
      "Epoch 1 Step 891 Loss 1.7265\n",
      "Epoch 1 Step 892 Loss 1.5294\n",
      "Epoch 1 Step 893 Loss 1.7629\n",
      "Epoch 1 Step 894 Loss 1.7022\n",
      "Epoch 1 Step 895 Loss 1.4980\n",
      "Epoch 1 Step 896 Loss 1.5234\n",
      "Epoch 1 Step 897 Loss 1.8119\n",
      "Epoch 1 Step 898 Loss 1.5033\n",
      "Epoch 1 Step 899 Loss 1.6213\n",
      "Epoch 1 Step 900 Loss 1.4965\n",
      "Epoch 1 Step 901 Loss 1.7476\n",
      "Epoch 1 Step 902 Loss 1.4595\n",
      "Epoch 1 Step 903 Loss 1.5108\n",
      "Epoch 1 Step 904 Loss 1.7862\n",
      "Epoch 1 Step 905 Loss 1.9644\n",
      "Epoch 1 Step 906 Loss 2.0051\n",
      "Epoch 1 Step 907 Loss 1.6249\n",
      "Epoch 1 Step 908 Loss 1.5248\n",
      "Epoch 1 Step 909 Loss 1.6888\n",
      "Epoch 1 Step 910 Loss 1.6494\n",
      "Epoch 1 Step 911 Loss 1.3363\n",
      "Epoch 1 Step 912 Loss 1.4050\n",
      "Epoch 1 Step 913 Loss 1.9362\n",
      "Epoch 1 Step 914 Loss 1.6335\n",
      "Epoch 1 Step 915 Loss 1.4161\n",
      "Epoch 1 Step 916 Loss 1.9390\n",
      "Epoch 1 Step 917 Loss 1.8158\n",
      "Epoch 1 Step 918 Loss 1.7428\n",
      "Epoch 1 Step 919 Loss 1.6975\n",
      "Epoch 1 Step 920 Loss 1.5552\n",
      "Epoch 1 Step 921 Loss 1.3516\n",
      "Epoch 1 Step 922 Loss 1.4544\n",
      "Epoch 1 Step 923 Loss 1.5046\n",
      "Epoch 1 Step 924 Loss 1.5668\n",
      "Epoch 1 Step 925 Loss 1.4517\n",
      "Epoch 1 Step 926 Loss 1.3245\n",
      "Epoch 1 Step 927 Loss 1.3614\n",
      "Epoch 1 Step 928 Loss 1.2539\n",
      "Epoch 1 Step 929 Loss 1.1888\n",
      "Epoch 1 Step 930 Loss 1.2442\n",
      "Epoch 1 Step 931 Loss 1.4853\n",
      "Epoch 1 Step 932 Loss 1.8873\n",
      "Epoch 1 Step 933 Loss 1.4257\n",
      "Epoch 1 Step 934 Loss 1.6925\n",
      "Epoch 1 Step 935 Loss 1.6212\n",
      "Epoch 1 Step 936 Loss 1.8031\n",
      "Epoch 1 Step 937 Loss 2.1875\n",
      "Epoch 1 Step 938 Loss 1.5902\n",
      "Epoch 1 Step 939 Loss 1.3813\n",
      "Epoch 1 Step 940 Loss 1.5238\n",
      "Epoch 1 Step 941 Loss 1.5233\n",
      "Epoch 1 Step 942 Loss 1.5635\n",
      "Epoch 1 Step 943 Loss 1.7509\n",
      "Epoch 1 Step 944 Loss 1.3127\n",
      "Epoch 1 Step 945 Loss 1.3249\n",
      "Epoch 1 Step 946 Loss 1.2762\n",
      "Epoch 1 Step 947 Loss 1.3041\n",
      "Epoch 1 Step 948 Loss 1.6631\n",
      "Epoch 1 Step 949 Loss 1.4228\n",
      "Epoch 1 Step 950 Loss 1.8702\n",
      "Epoch 1 Step 951 Loss 1.7640\n",
      "Epoch 1 Step 952 Loss 1.6977\n",
      "Epoch 1 Step 953 Loss 1.9013\n",
      "Epoch 1 Step 954 Loss 1.6386\n",
      "Epoch 1 Step 955 Loss 1.8018\n",
      "Epoch 1 Step 956 Loss 1.6503\n",
      "Epoch 1 Step 957 Loss 1.8792\n",
      "Epoch 1 Step 958 Loss 1.5361\n",
      "Epoch 1 Step 959 Loss 1.3556\n",
      "Epoch 1 Step 960 Loss 1.6163\n",
      "Epoch 1 Step 961 Loss 1.7894\n",
      "Epoch 1 Step 962 Loss 1.5386\n",
      "Epoch 1 Step 963 Loss 1.6804\n",
      "Epoch 1 Step 964 Loss 1.6549\n",
      "Epoch 1 Step 965 Loss 1.5751\n",
      "Epoch 1 Step 966 Loss 1.5015\n",
      "Epoch 1 Step 967 Loss 1.7490\n",
      "Epoch 1 Step 968 Loss 1.6447\n",
      "Epoch 1 Step 969 Loss 1.7222\n",
      "Epoch 1 Step 970 Loss 1.8306\n",
      "Epoch 1 Step 971 Loss 1.7047\n",
      "Epoch 1 Step 972 Loss 1.4831\n",
      "Epoch 1 Step 973 Loss 1.9971\n",
      "Epoch 1 Step 974 Loss 1.4250\n",
      "Epoch 1 Step 975 Loss 1.5792\n",
      "Epoch 1 Step 976 Loss 1.4747\n",
      "Epoch 1 Step 977 Loss 1.8554\n",
      "Epoch 1 Step 978 Loss 2.0210\n",
      "Epoch 1 Step 979 Loss 1.5141\n",
      "Epoch 1 Step 980 Loss 1.5819\n",
      "Epoch 1 Step 981 Loss 1.7072\n",
      "Epoch 1 Step 982 Loss 1.5489\n",
      "Epoch 1 Step 983 Loss 1.4844\n",
      "Epoch 1 Step 984 Loss 1.8705\n",
      "Epoch 1 Step 985 Loss 1.7639\n",
      "Epoch 1 Step 986 Loss 1.7685\n",
      "Epoch 1 Step 987 Loss 1.7211\n",
      "Epoch 1 Step 988 Loss 1.6976\n",
      "Epoch 1 Step 989 Loss 1.2383\n",
      "Epoch 1 Step 990 Loss 1.6076\n",
      "Epoch 1 Step 991 Loss 1.2996\n",
      "Epoch 1 Step 992 Loss 1.8058\n",
      "Epoch 1 Step 993 Loss 1.7460\n",
      "Epoch 1 Step 994 Loss 1.3890\n",
      "Epoch 1 Step 995 Loss 1.1882\n",
      "Epoch 1 Step 996 Loss 1.6229\n",
      "Epoch 1 Step 997 Loss 1.5837\n",
      "Epoch 1 Step 998 Loss 1.4017\n",
      "Epoch 1 Step 999 Loss 1.6473\n",
      "Epoch 1 Step 1000 Loss 1.6601\n",
      "Epoch 1 Step 1001 Loss 1.9204\n",
      "Epoch 1 Step 1002 Loss 1.7410\n",
      "Epoch 1 Step 1003 Loss 1.7747\n",
      "Epoch 1 Step 1004 Loss 1.8653\n",
      "Epoch 1 Step 1005 Loss 1.7156\n",
      "Epoch 1 Step 1006 Loss 1.5804\n",
      "Epoch 1 Step 1007 Loss 1.3979\n",
      "Epoch 1 Step 1008 Loss 1.5830\n",
      "Epoch 1 Step 1009 Loss 1.7714\n",
      "Epoch 1 Step 1010 Loss 1.7778\n",
      "Epoch 1 Step 1011 Loss 1.6719\n",
      "Epoch 1 Step 1012 Loss 1.2131\n",
      "Epoch 1 Step 1013 Loss 1.2082\n",
      "Epoch 1 Step 1014 Loss 1.4635\n",
      "Epoch 1 Step 1015 Loss 1.6637\n",
      "Epoch 1 Step 1016 Loss 1.6586\n",
      "Epoch 1 Step 1017 Loss 1.3790\n",
      "Epoch 1 Step 1018 Loss 1.6790\n",
      "Epoch 1 Step 1019 Loss 1.4076\n",
      "Epoch 1 Step 1020 Loss 1.7405\n",
      "Epoch 1 Step 1021 Loss 1.6771\n",
      "Epoch 1 Step 1022 Loss 1.8287\n",
      "Epoch 1 Step 1023 Loss 1.2593\n",
      "Epoch 1 Step 1024 Loss 1.4614\n",
      "Epoch 1 Step 1025 Loss 1.6411\n",
      "Epoch 1 Step 1026 Loss 1.4856\n",
      "Epoch 1 Step 1027 Loss 1.6784\n",
      "Epoch 1 Step 1028 Loss 1.4919\n",
      "Epoch 1 Step 1029 Loss 1.5561\n",
      "Epoch 1 Step 1030 Loss 1.9400\n",
      "Epoch 1 Step 1031 Loss 1.7437\n",
      "Epoch 1 Step 1032 Loss 1.6913\n",
      "Epoch 1 Step 1033 Loss 1.7440\n",
      "Epoch 1 Step 1034 Loss 1.7503\n",
      "Epoch 1 Step 1035 Loss 1.4224\n",
      "Epoch 1 Step 1036 Loss 1.7798\n",
      "Epoch 1 Step 1037 Loss 1.5962\n",
      "Epoch 1 Step 1038 Loss 2.0115\n",
      "Epoch 1 Step 1039 Loss 1.7171\n",
      "Epoch 1 Step 1040 Loss 1.6937\n",
      "Epoch 1 Step 1041 Loss 1.4596\n",
      "Epoch 1 Step 1042 Loss 1.3469\n",
      "Epoch 1 Step 1043 Loss 1.5384\n",
      "Epoch 1 Step 1044 Loss 1.6131\n",
      "Epoch 1 Step 1045 Loss 1.8139\n",
      "Epoch 1 Step 1046 Loss 1.8636\n",
      "Epoch 1 Step 1047 Loss 1.6692\n",
      "Epoch 1 Step 1048 Loss 2.0058\n",
      "Epoch 1 Step 1049 Loss 1.7855\n",
      "Epoch 1 Step 1050 Loss 1.7752\n",
      "Epoch 1 Step 1051 Loss 1.5896\n",
      "Epoch 1 Step 1052 Loss 1.8702\n",
      "Epoch 1 Step 1053 Loss 1.8798\n",
      "Epoch 1 Step 1054 Loss 1.6393\n",
      "Epoch 1 Step 1055 Loss 1.5059\n",
      "Epoch 1 Step 1056 Loss 1.3397\n",
      "Epoch 1 Step 1057 Loss 1.6404\n",
      "Epoch 1 Step 1058 Loss 1.3884\n",
      "Epoch 1 Step 1059 Loss 1.5963\n",
      "Epoch 1 Step 1060 Loss 1.5925\n",
      "Epoch 1 Step 1061 Loss 1.8492\n",
      "Epoch 1 Step 1062 Loss 2.1861\n",
      "Epoch 1 Step 1063 Loss 2.2395\n",
      "Epoch 1 Step 1064 Loss 1.9530\n",
      "Epoch 1 Step 1065 Loss 1.8862\n",
      "Epoch 1 Step 1066 Loss 1.4702\n",
      "Epoch 1 Step 1067 Loss 1.4934\n",
      "Epoch 1 Step 1068 Loss 1.7762\n",
      "Epoch 1 Step 1069 Loss 2.0336\n",
      "Epoch 1 Step 1070 Loss 2.1363\n",
      "Epoch 1 Step 1071 Loss 1.7997\n",
      "Epoch 1 Step 1072 Loss 1.6139\n",
      "Epoch 1 Step 1073 Loss 1.3926\n",
      "Epoch 1 Step 1074 Loss 1.7427\n",
      "Epoch 1 Step 1075 Loss 2.0936\n",
      "Epoch 1 Step 1076 Loss 2.3070\n",
      "Epoch 1 Step 1077 Loss 2.1364\n",
      "Epoch 1 Step 1078 Loss 2.0015\n",
      "Epoch 1 Step 1079 Loss 1.6969\n",
      "Epoch 1 Step 1080 Loss 1.3661\n",
      "Epoch 1 Step 1081 Loss 1.6847\n",
      "Epoch 1 Step 1082 Loss 1.9341\n",
      "Epoch 1 Step 1083 Loss 1.3641\n",
      "Epoch 1 Step 1084 Loss 1.7504\n",
      "Epoch 1 Step 1085 Loss 1.3421\n",
      "Epoch 1 Step 1086 Loss 1.4632\n",
      "Epoch 1 Step 1087 Loss 1.4931\n",
      "Epoch 1 Step 1088 Loss 1.7655\n",
      "Epoch 1 Step 1089 Loss 1.7261\n",
      "Epoch 1 Step 1090 Loss 1.9075\n",
      "Epoch 1 Step 1091 Loss 1.6152\n",
      "Epoch 1 Step 1092 Loss 1.5040\n",
      "Epoch 1 Step 1093 Loss 1.6283\n",
      "Epoch 1 Step 1094 Loss 1.6040\n",
      "Epoch 1 Step 1095 Loss 1.7939\n",
      "Epoch 1 Step 1096 Loss 1.6356\n",
      "Epoch 1 Step 1097 Loss 1.3112\n",
      "Epoch 1 Step 1098 Loss 1.2182\n",
      "Epoch 1 Step 1099 Loss 1.3608\n",
      "Epoch 1 Step 1100 Loss 1.4869\n",
      "Epoch 1 Step 1101 Loss 1.5207\n",
      "Epoch 1 Step 1102 Loss 1.5632\n",
      "Epoch 1 Step 1103 Loss 1.6343\n",
      "Epoch 1 Step 1104 Loss 2.0509\n",
      "Epoch 1 Step 1105 Loss 2.0284\n",
      "Epoch 1 Step 1106 Loss 1.5988\n",
      "Epoch 1 Step 1107 Loss 1.5397\n",
      "Epoch 1 Step 1108 Loss 1.5881\n",
      "Epoch 1 Step 1109 Loss 1.5394\n",
      "Epoch 1 Step 1110 Loss 1.2945\n",
      "Epoch 1 Step 1111 Loss 1.5859\n",
      "Epoch 1 Step 1112 Loss 1.5984\n",
      "Epoch 1 Step 1113 Loss 1.6309\n",
      "Epoch 1 Step 1114 Loss 1.2709\n",
      "Epoch 1 Step 1115 Loss 1.2853\n",
      "Epoch 1 Step 1116 Loss 1.7025\n",
      "Epoch 1 Step 1117 Loss 1.7119\n",
      "Epoch 1 Step 1118 Loss 2.1034\n",
      "Epoch 1 Step 1119 Loss 1.5425\n",
      "Epoch 1 Step 1120 Loss 1.5662\n",
      "Epoch 1 Step 1121 Loss 1.3063\n",
      "Epoch 1 Step 1122 Loss 1.2161\n",
      "Epoch 1 Step 1123 Loss 1.3459\n",
      "Epoch 1 Step 1124 Loss 1.1915\n",
      "Epoch 1 Step 1125 Loss 1.6343\n",
      "Epoch 1 Step 1126 Loss 1.4587\n",
      "Epoch 1 Step 1127 Loss 1.6722\n",
      "Epoch 1 Step 1128 Loss 1.3637\n",
      "Epoch 1 Step 1129 Loss 1.4284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1130 Loss 1.3041\n",
      "Epoch 1 Step 1131 Loss 1.5006\n",
      "Epoch 1 Step 1132 Loss 1.2238\n",
      "Epoch 1 Step 1133 Loss 1.5615\n",
      "Epoch 1 Step 1134 Loss 1.2758\n",
      "Epoch 1 Step 1135 Loss 1.4416\n",
      "Epoch 1 Step 1136 Loss 1.5162\n",
      "Epoch 1 Step 1137 Loss 1.7932\n",
      "Epoch 1 Step 1138 Loss 1.5023\n",
      "Epoch 1 Step 1139 Loss 1.3944\n",
      "Epoch 1 Step 1140 Loss 1.1727\n",
      "Epoch 1 Step 1141 Loss 1.3250\n",
      "Epoch 1 Step 1142 Loss 1.2207\n",
      "Epoch 1 Step 1143 Loss 1.2596\n",
      "Epoch 1 Step 1144 Loss 1.6981\n",
      "Epoch 1 Step 1145 Loss 1.4756\n",
      "Epoch 1 Step 1146 Loss 1.5538\n",
      "Epoch 1 Step 1147 Loss 1.8674\n",
      "Epoch 1 Step 1148 Loss 1.3044\n",
      "Epoch 1 Step 1149 Loss 1.6143\n",
      "Epoch 1 Step 1150 Loss 1.2896\n",
      "Epoch 1 Step 1151 Loss 1.6749\n",
      "Epoch 1 Step 1152 Loss 1.7667\n",
      "Epoch 1 Step 1153 Loss 1.4632\n",
      "Epoch 1 Step 1154 Loss 1.3810\n",
      "Epoch 1 Step 1155 Loss 1.6846\n",
      "Epoch 1 Step 1156 Loss 1.5247\n",
      "Epoch 1 Step 1157 Loss 1.5920\n",
      "Epoch 1 Step 1158 Loss 1.6724\n",
      "Epoch 1 Step 1159 Loss 1.7008\n",
      "Epoch 1 Step 1160 Loss 1.3471\n",
      "Epoch 1 Step 1161 Loss 1.4235\n",
      "Epoch 1 Step 1162 Loss 1.5317\n",
      "Epoch 1 Step 1163 Loss 1.1523\n",
      "Epoch 1 Step 1164 Loss 1.1520\n",
      "Epoch 1 Step 1165 Loss 0.9811\n",
      "Epoch 1 Step 1166 Loss 1.1994\n",
      "Epoch 1 Step 1167 Loss 1.3293\n",
      "Epoch 1 Step 1168 Loss 1.1930\n",
      "Epoch 1 Step 1169 Loss 1.3713\n",
      "Epoch 1 Step 1170 Loss 1.4397\n",
      "Epoch 1 Step 1171 Loss 1.4205\n",
      "Epoch 1 Step 1172 Loss 1.4798\n",
      "Epoch 1 Step 1173 Loss 1.6006\n",
      "Epoch 1 Step 1174 Loss 1.3941\n",
      "Epoch 1 Step 1175 Loss 1.3842\n",
      "Epoch 1 Step 1176 Loss 1.3845\n",
      "Epoch 1 Step 1177 Loss 1.4585\n",
      "Epoch 1 Step 1178 Loss 1.2534\n",
      "Epoch 1 Step 1179 Loss 1.2150\n",
      "Epoch 1 Step 1180 Loss 1.3964\n",
      "Epoch 1 Step 1181 Loss 1.4808\n",
      "Epoch 1 Step 1182 Loss 1.3067\n",
      "Epoch 1 Step 1183 Loss 1.3664\n",
      "Epoch 1 Step 1184 Loss 1.2339\n",
      "Epoch 1 Step 1185 Loss 1.1683\n",
      "Epoch 1 Step 1186 Loss 1.0383\n",
      "Epoch 1 Step 1187 Loss 1.2342\n",
      "Epoch 1 Step 1188 Loss 1.3717\n",
      "Epoch 1 Step 1189 Loss 1.4375\n",
      "Epoch 1 Step 1190 Loss 1.5878\n",
      "Epoch 1 Step 1191 Loss 1.5136\n",
      "Epoch 1 Step 1192 Loss 1.5949\n",
      "Epoch 1 Step 1193 Loss 1.7548\n",
      "Epoch 1 Step 1194 Loss 1.4283\n",
      "Epoch 1 Step 1195 Loss 1.4327\n",
      "Epoch 1 Step 1196 Loss 1.2372\n",
      "Epoch 1 Step 1197 Loss 1.6453\n",
      "Epoch 1 Step 1198 Loss 1.0537\n",
      "Epoch 1 Step 1199 Loss 1.1306\n",
      "Epoch 1 Step 1200 Loss 1.3506\n",
      "Epoch 1 Step 1201 Loss 1.5527\n",
      "Epoch 1 Step 1202 Loss 1.6376\n",
      "Epoch 1 Step 1203 Loss 1.5992\n",
      "Epoch 1 Step 1204 Loss 1.3454\n",
      "Epoch 1 Step 1205 Loss 1.3570\n",
      "Epoch 1 Step 1206 Loss 1.2597\n",
      "Epoch 1 Step 1207 Loss 1.5570\n",
      "Epoch 1 Step 1208 Loss 1.1925\n",
      "Epoch 1 Step 1209 Loss 1.0320\n",
      "Epoch 1 Step 1210 Loss 1.3183\n",
      "Epoch 1 Step 1211 Loss 1.3625\n",
      "Epoch 1 Step 1212 Loss 1.6506\n",
      "Epoch 1 Step 1213 Loss 1.2507\n",
      "Epoch 1 Step 1214 Loss 1.0791\n",
      "Epoch 1 Step 1215 Loss 1.3378\n",
      "Epoch 1 Step 1216 Loss 1.4200\n",
      "Epoch 1 Step 1217 Loss 1.0185\n",
      "Epoch 1 Step 1218 Loss 1.2467\n",
      "Epoch 1 Step 1219 Loss 1.2479\n",
      "Epoch 1 Step 1220 Loss 1.1163\n",
      "Epoch 1 Step 1221 Loss 1.3580\n",
      "Epoch 1 Step 1222 Loss 1.2978\n",
      "Epoch 1 Step 1223 Loss 1.1634\n",
      "Epoch 1 Step 1224 Loss 1.5377\n",
      "Epoch 1 Step 1225 Loss 1.9083\n",
      "Epoch 1 Step 1226 Loss 1.4907\n",
      "Epoch 1 Step 1227 Loss 1.7193\n",
      "Epoch 1 Step 1228 Loss 1.5509\n",
      "Epoch 1 Step 1229 Loss 1.4725\n",
      "Epoch 1 Step 1230 Loss 1.0098\n",
      "Epoch 1 Step 1231 Loss 1.1144\n",
      "Epoch 1 Step 1232 Loss 1.2447\n",
      "Epoch 1 Step 1233 Loss 1.4584\n",
      "Epoch 1 Step 1234 Loss 1.6353\n",
      "Epoch 1 Step 1235 Loss 1.7127\n",
      "Epoch 1 Step 1236 Loss 1.2041\n",
      "Epoch 1 Step 1237 Loss 1.1553\n",
      "Epoch 1 Step 1238 Loss 0.9315\n",
      "Epoch 1 Step 1239 Loss 1.0022\n",
      "Epoch 1 Step 1240 Loss 1.5151\n",
      "Epoch 1 Step 1241 Loss 1.4405\n",
      "Epoch 1 Step 1242 Loss 1.3510\n",
      "Epoch 1 Step 1243 Loss 1.3811\n",
      "Epoch 1 Step 1244 Loss 1.4438\n",
      "Epoch 1 Step 1245 Loss 1.6388\n",
      "Epoch 1 Step 1246 Loss 1.4294\n",
      "Epoch 1 Step 1247 Loss 1.2750\n",
      "Epoch 1 Step 1248 Loss 1.4881\n",
      "Epoch 1 Step 1249 Loss 1.3591\n",
      "Epoch 1 Step 1250 Loss 0.9819\n",
      "Epoch 1 Step 1251 Loss 1.3486\n",
      "Epoch 1 Step 1252 Loss 1.4773\n",
      "Epoch 1 Step 1253 Loss 1.2369\n",
      "Epoch 1 Step 1254 Loss 1.2660\n",
      "Epoch 1 Step 1255 Loss 1.5550\n",
      "Epoch 1 Step 1256 Loss 1.4357\n",
      "Epoch 1 Step 1257 Loss 1.1314\n",
      "Epoch 1 Step 1258 Loss 1.2821\n",
      "Epoch 1 Step 1259 Loss 1.5956\n",
      "Epoch 1 Step 1260 Loss 1.7676\n",
      "Epoch 1 Step 1261 Loss 1.4829\n",
      "Epoch 1 Step 1262 Loss 1.6871\n",
      "Epoch 1 Step 1263 Loss 1.8811\n",
      "Epoch 1 Step 1264 Loss 1.3706\n",
      "Epoch 1 Step 1265 Loss 1.4538\n",
      "Epoch 1 Step 1266 Loss 1.3917\n",
      "Epoch 1 Step 1267 Loss 1.2454\n",
      "Epoch 1 Step 1268 Loss 1.3195\n",
      "Epoch 1 Step 1269 Loss 1.3028\n",
      "Epoch 1 Step 1270 Loss 1.1152\n",
      "Epoch 1 Step 1271 Loss 1.2840\n",
      "Epoch 1 Step 1272 Loss 1.3809\n",
      "Epoch 1 Step 1273 Loss 1.5375\n",
      "Epoch 1 Step 1274 Loss 1.4755\n",
      "Epoch 1 Step 1275 Loss 1.7216\n",
      "Epoch 1 Step 1276 Loss 1.4303\n",
      "Epoch 1 Step 1277 Loss 1.4104\n",
      "Epoch 1 Step 1278 Loss 1.4002\n",
      "Epoch 1 Step 1279 Loss 1.0776\n",
      "Epoch 1 Step 1280 Loss 1.0259\n",
      "Epoch 1 Step 1281 Loss 1.2645\n",
      "Epoch 1 Step 1282 Loss 1.3165\n",
      "Epoch 1 Step 1283 Loss 1.2903\n",
      "Epoch 1 Step 1284 Loss 1.3771\n",
      "Epoch 1 Step 1285 Loss 1.4308\n",
      "Epoch 1 Step 1286 Loss 1.6504\n",
      "Epoch 1 Step 1287 Loss 1.0698\n",
      "Epoch 1 Step 1288 Loss 1.3947\n",
      "Epoch 1 Step 1289 Loss 1.3088\n",
      "Epoch 1 Step 1290 Loss 1.4341\n",
      "Epoch 1 Step 1291 Loss 1.7105\n",
      "Epoch 1 Step 1292 Loss 1.2098\n",
      "Epoch 1 Step 1293 Loss 1.0602\n",
      "Epoch 1 Loss 1.5510\n",
      "Time taken for 1 epoch 1260.1835074424744 sec\n",
      "\n",
      "Epoch 2 Step 0 Loss 2.0278\n",
      "Epoch 2 Step 1 Loss 1.9231\n",
      "Epoch 2 Step 2 Loss 1.8960\n",
      "Epoch 2 Step 3 Loss 1.4796\n",
      "Epoch 2 Step 4 Loss 1.8588\n",
      "Epoch 2 Step 5 Loss 1.6332\n",
      "Epoch 2 Step 6 Loss 1.0853\n",
      "Epoch 2 Step 7 Loss 1.1136\n",
      "Epoch 2 Step 8 Loss 1.1920\n",
      "Epoch 2 Step 9 Loss 1.6196\n",
      "Epoch 2 Step 10 Loss 1.8216\n",
      "Epoch 2 Step 11 Loss 1.3780\n",
      "Epoch 2 Step 12 Loss 1.3336\n",
      "Epoch 2 Step 13 Loss 1.3186\n",
      "Epoch 2 Step 14 Loss 1.7913\n",
      "Epoch 2 Step 15 Loss 1.6254\n",
      "Epoch 2 Step 16 Loss 1.1997\n",
      "Epoch 2 Step 17 Loss 0.9089\n",
      "Epoch 2 Step 18 Loss 0.9914\n",
      "Epoch 2 Step 19 Loss 1.0048\n",
      "Epoch 2 Step 20 Loss 1.1094\n",
      "Epoch 2 Step 21 Loss 0.9999\n",
      "Epoch 2 Step 22 Loss 1.2792\n",
      "Epoch 2 Step 23 Loss 1.5054\n",
      "Epoch 2 Step 24 Loss 1.3905\n",
      "Epoch 2 Step 25 Loss 1.1903\n",
      "Epoch 2 Step 26 Loss 1.4507\n",
      "Epoch 2 Step 27 Loss 1.6198\n",
      "Epoch 2 Step 28 Loss 2.0052\n",
      "Epoch 2 Step 29 Loss 1.6759\n",
      "Epoch 2 Step 30 Loss 1.4297\n",
      "Epoch 2 Step 31 Loss 1.5602\n",
      "Epoch 2 Step 32 Loss 1.4874\n",
      "Epoch 2 Step 33 Loss 1.2321\n",
      "Epoch 2 Step 34 Loss 1.4978\n",
      "Epoch 2 Step 35 Loss 1.4662\n",
      "Epoch 2 Step 36 Loss 1.5161\n",
      "Epoch 2 Step 37 Loss 2.0449\n",
      "Epoch 2 Step 38 Loss 1.4927\n",
      "Epoch 2 Step 39 Loss 1.6902\n",
      "Epoch 2 Step 40 Loss 1.3259\n",
      "Epoch 2 Step 41 Loss 1.3336\n",
      "Epoch 2 Step 42 Loss 1.5571\n",
      "Epoch 2 Step 43 Loss 1.7330\n",
      "Epoch 2 Step 44 Loss 1.4215\n",
      "Epoch 2 Step 45 Loss 1.3986\n",
      "Epoch 2 Step 46 Loss 1.3524\n",
      "Epoch 2 Step 47 Loss 1.0043\n",
      "Epoch 2 Step 48 Loss 1.5039\n",
      "Epoch 2 Step 49 Loss 1.8162\n",
      "Epoch 2 Step 50 Loss 1.8086\n",
      "Epoch 2 Step 51 Loss 1.6186\n",
      "Epoch 2 Step 52 Loss 1.4676\n",
      "Epoch 2 Step 53 Loss 1.6244\n",
      "Epoch 2 Step 54 Loss 1.5537\n",
      "Epoch 2 Step 55 Loss 0.9711\n",
      "Epoch 2 Step 56 Loss 1.5456\n",
      "Epoch 2 Step 57 Loss 1.4594\n",
      "Epoch 2 Step 58 Loss 1.1760\n",
      "Epoch 2 Step 59 Loss 1.6561\n",
      "Epoch 2 Step 60 Loss 1.2304\n",
      "Epoch 2 Step 61 Loss 1.1566\n",
      "Epoch 2 Step 62 Loss 1.5224\n",
      "Epoch 2 Step 63 Loss 1.1271\n",
      "Epoch 2 Step 64 Loss 1.2221\n",
      "Epoch 2 Step 65 Loss 1.4187\n",
      "Epoch 2 Step 66 Loss 1.3468\n",
      "Epoch 2 Step 67 Loss 1.5375\n",
      "Epoch 2 Step 68 Loss 1.4033\n",
      "Epoch 2 Step 69 Loss 1.4659\n",
      "Epoch 2 Step 70 Loss 1.4511\n",
      "Epoch 2 Step 71 Loss 1.3695\n",
      "Epoch 2 Step 72 Loss 1.5715\n",
      "Epoch 2 Step 73 Loss 1.4244\n",
      "Epoch 2 Step 74 Loss 1.9418\n",
      "Epoch 2 Step 75 Loss 1.5584\n",
      "Epoch 2 Step 76 Loss 1.3601\n",
      "Epoch 2 Step 77 Loss 1.3461\n",
      "Epoch 2 Step 78 Loss 1.5634\n",
      "Epoch 2 Step 79 Loss 1.6058\n",
      "Epoch 2 Step 80 Loss 1.7902\n",
      "Epoch 2 Step 81 Loss 1.1016\n",
      "Epoch 2 Step 82 Loss 1.1046\n",
      "Epoch 2 Step 83 Loss 1.4398\n",
      "Epoch 2 Step 84 Loss 1.4411\n",
      "Epoch 2 Step 85 Loss 1.4591\n",
      "Epoch 2 Step 86 Loss 1.4957\n",
      "Epoch 2 Step 87 Loss 1.3834\n",
      "Epoch 2 Step 88 Loss 1.3330\n",
      "Epoch 2 Step 89 Loss 1.3046\n",
      "Epoch 2 Step 90 Loss 1.1401\n",
      "Epoch 2 Step 91 Loss 0.9667\n",
      "Epoch 2 Step 92 Loss 0.9098\n",
      "Epoch 2 Step 93 Loss 1.0163\n",
      "Epoch 2 Step 94 Loss 1.1709\n",
      "Epoch 2 Step 95 Loss 1.3452\n",
      "Epoch 2 Step 96 Loss 1.4940\n",
      "Epoch 2 Step 97 Loss 1.6249\n",
      "Epoch 2 Step 98 Loss 1.4546\n",
      "Epoch 2 Step 99 Loss 1.5517\n",
      "Epoch 2 Step 100 Loss 1.2992\n",
      "Epoch 2 Step 101 Loss 1.1976\n",
      "Epoch 2 Step 102 Loss 1.1618\n",
      "Epoch 2 Step 103 Loss 1.3441\n",
      "Epoch 2 Step 104 Loss 1.2036\n",
      "Epoch 2 Step 105 Loss 1.4968\n",
      "Epoch 2 Step 106 Loss 1.1628\n",
      "Epoch 2 Step 107 Loss 1.5191\n",
      "Epoch 2 Step 108 Loss 1.5598\n",
      "Epoch 2 Step 109 Loss 1.2666\n",
      "Epoch 2 Step 110 Loss 1.3452\n",
      "Epoch 2 Step 111 Loss 1.3852\n",
      "Epoch 2 Step 112 Loss 1.2572\n",
      "Epoch 2 Step 113 Loss 1.2745\n",
      "Epoch 2 Step 114 Loss 1.3154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Step 115 Loss 1.2477\n",
      "Epoch 2 Step 116 Loss 0.9601\n",
      "Epoch 2 Step 117 Loss 1.0053\n",
      "Epoch 2 Step 118 Loss 1.2999\n",
      "Epoch 2 Step 119 Loss 1.4113\n",
      "Epoch 2 Step 120 Loss 1.4942\n",
      "Epoch 2 Step 121 Loss 1.6900\n",
      "Epoch 2 Step 122 Loss 1.2280\n",
      "Epoch 2 Step 123 Loss 1.2649\n",
      "Epoch 2 Step 124 Loss 1.3153\n",
      "Epoch 2 Step 125 Loss 1.3872\n",
      "Epoch 2 Step 126 Loss 1.9023\n",
      "Epoch 2 Step 127 Loss 1.2585\n",
      "Epoch 2 Step 128 Loss 1.4062\n",
      "Epoch 2 Step 129 Loss 1.2511\n",
      "Epoch 2 Step 130 Loss 1.0929\n",
      "Epoch 2 Step 131 Loss 1.2582\n",
      "Epoch 2 Step 132 Loss 1.1345\n",
      "Epoch 2 Step 133 Loss 1.0617\n",
      "Epoch 2 Step 134 Loss 1.7335\n",
      "Epoch 2 Step 135 Loss 1.8863\n",
      "Epoch 2 Step 136 Loss 1.7920\n",
      "Epoch 2 Step 137 Loss 1.3440\n",
      "Epoch 2 Step 138 Loss 1.3865\n",
      "Epoch 2 Step 139 Loss 1.3282\n",
      "Epoch 2 Step 140 Loss 1.4499\n",
      "Epoch 2 Step 141 Loss 1.5549\n",
      "Epoch 2 Step 142 Loss 1.4428\n",
      "Epoch 2 Step 143 Loss 1.2227\n",
      "Epoch 2 Step 144 Loss 1.2626\n",
      "Epoch 2 Step 145 Loss 1.1983\n",
      "Epoch 2 Step 146 Loss 1.2423\n",
      "Epoch 2 Step 147 Loss 1.1212\n",
      "Epoch 2 Step 148 Loss 1.3069\n",
      "Epoch 2 Step 149 Loss 1.1249\n",
      "Epoch 2 Step 150 Loss 1.4721\n",
      "Epoch 2 Step 151 Loss 1.0463\n",
      "Epoch 2 Step 152 Loss 1.1159\n",
      "Epoch 2 Step 153 Loss 1.3455\n",
      "Epoch 2 Step 154 Loss 1.5712\n",
      "Epoch 2 Step 155 Loss 1.1977\n",
      "Epoch 2 Step 156 Loss 1.3817\n",
      "Epoch 2 Step 157 Loss 1.4316\n",
      "Epoch 2 Step 158 Loss 1.4024\n",
      "Epoch 2 Step 159 Loss 1.1980\n",
      "Epoch 2 Step 160 Loss 1.6252\n",
      "Epoch 2 Step 161 Loss 1.3876\n",
      "Epoch 2 Step 162 Loss 1.1790\n",
      "Epoch 2 Step 163 Loss 1.4194\n",
      "Epoch 2 Step 164 Loss 1.3278\n",
      "Epoch 2 Step 165 Loss 1.2334\n",
      "Epoch 2 Step 166 Loss 1.3078\n",
      "Epoch 2 Step 167 Loss 1.1205\n",
      "Epoch 2 Step 168 Loss 1.3816\n",
      "Epoch 2 Step 169 Loss 1.5178\n",
      "Epoch 2 Step 170 Loss 1.3322\n",
      "Epoch 2 Step 171 Loss 1.2291\n",
      "Epoch 2 Step 172 Loss 1.3617\n",
      "Epoch 2 Step 173 Loss 1.6020\n",
      "Epoch 2 Step 174 Loss 1.6778\n",
      "Epoch 2 Step 175 Loss 1.0371\n",
      "Epoch 2 Step 176 Loss 1.0924\n",
      "Epoch 2 Step 177 Loss 1.2569\n",
      "Epoch 2 Step 178 Loss 1.5026\n",
      "Epoch 2 Step 179 Loss 1.3975\n",
      "Epoch 2 Step 180 Loss 1.4709\n",
      "Epoch 2 Step 181 Loss 1.5796\n",
      "Epoch 2 Step 182 Loss 1.4600\n",
      "Epoch 2 Step 183 Loss 1.4553\n",
      "Epoch 2 Step 184 Loss 1.6137\n",
      "Epoch 2 Step 185 Loss 1.3610\n",
      "Epoch 2 Step 186 Loss 1.3197\n",
      "Epoch 2 Step 187 Loss 1.2011\n",
      "Epoch 2 Step 188 Loss 0.9863\n",
      "Epoch 2 Step 189 Loss 1.1676\n",
      "Epoch 2 Step 190 Loss 1.0321\n",
      "Epoch 2 Step 191 Loss 1.1813\n",
      "Epoch 2 Step 192 Loss 1.5094\n",
      "Epoch 2 Step 193 Loss 1.2704\n",
      "Epoch 2 Step 194 Loss 1.6585\n",
      "Epoch 2 Step 195 Loss 1.3703\n",
      "Epoch 2 Step 196 Loss 1.3048\n",
      "Epoch 2 Step 197 Loss 1.3079\n",
      "Epoch 2 Step 198 Loss 1.0129\n",
      "Epoch 2 Step 199 Loss 1.4402\n",
      "Epoch 2 Step 200 Loss 1.3556\n",
      "Epoch 2 Step 201 Loss 1.0446\n",
      "Epoch 2 Step 202 Loss 1.2586\n",
      "Epoch 2 Step 203 Loss 1.5003\n",
      "Epoch 2 Step 204 Loss 1.6598\n",
      "Epoch 2 Step 205 Loss 1.3117\n",
      "Epoch 2 Step 206 Loss 1.7185\n",
      "Epoch 2 Step 207 Loss 1.3912\n",
      "Epoch 2 Step 208 Loss 1.5946\n",
      "Epoch 2 Step 209 Loss 1.2311\n",
      "Epoch 2 Step 210 Loss 1.5235\n",
      "Epoch 2 Step 211 Loss 1.4541\n",
      "Epoch 2 Step 212 Loss 1.5009\n",
      "Epoch 2 Step 213 Loss 1.3972\n",
      "Epoch 2 Step 214 Loss 1.7915\n",
      "Epoch 2 Step 215 Loss 1.4219\n",
      "Epoch 2 Step 216 Loss 1.3168\n",
      "Epoch 2 Step 217 Loss 1.1819\n",
      "Epoch 2 Step 218 Loss 1.3800\n",
      "Epoch 2 Step 219 Loss 1.5822\n",
      "Epoch 2 Step 220 Loss 1.5901\n",
      "Epoch 2 Step 221 Loss 1.4871\n",
      "Epoch 2 Step 222 Loss 1.2996\n",
      "Epoch 2 Step 223 Loss 1.4989\n",
      "Epoch 2 Step 224 Loss 1.6420\n",
      "Epoch 2 Step 225 Loss 1.6032\n",
      "Epoch 2 Step 226 Loss 1.4020\n",
      "Epoch 2 Step 227 Loss 1.4137\n",
      "Epoch 2 Step 228 Loss 1.2465\n",
      "Epoch 2 Step 229 Loss 1.2832\n",
      "Epoch 2 Step 230 Loss 1.2283\n",
      "Epoch 2 Step 231 Loss 1.3925\n",
      "Epoch 2 Step 232 Loss 1.3985\n",
      "Epoch 2 Step 233 Loss 1.4304\n",
      "Epoch 2 Step 234 Loss 1.5012\n",
      "Epoch 2 Step 235 Loss 1.4487\n",
      "Epoch 2 Step 236 Loss 1.2495\n",
      "Epoch 2 Step 237 Loss 1.4542\n",
      "Epoch 2 Step 238 Loss 1.2496\n",
      "Epoch 2 Step 239 Loss 1.0774\n",
      "Epoch 2 Step 240 Loss 1.3481\n",
      "Epoch 2 Step 241 Loss 1.4046\n",
      "Epoch 2 Step 242 Loss 1.7109\n",
      "Epoch 2 Step 243 Loss 1.2033\n",
      "Epoch 2 Step 244 Loss 1.0211\n",
      "Epoch 2 Step 245 Loss 1.2787\n",
      "Epoch 2 Step 246 Loss 1.4767\n",
      "Epoch 2 Step 247 Loss 1.3543\n",
      "Epoch 2 Step 248 Loss 1.1904\n",
      "Epoch 2 Step 249 Loss 1.0749\n",
      "Epoch 2 Step 250 Loss 1.2603\n",
      "Epoch 2 Step 251 Loss 0.9107\n",
      "Epoch 2 Step 252 Loss 1.0258\n",
      "Epoch 2 Step 253 Loss 1.5697\n",
      "Epoch 2 Step 254 Loss 1.5942\n",
      "Epoch 2 Step 255 Loss 1.4202\n",
      "Epoch 2 Step 256 Loss 1.1953\n",
      "Epoch 2 Step 257 Loss 1.3119\n",
      "Epoch 2 Step 258 Loss 1.3963\n",
      "Epoch 2 Step 259 Loss 1.2017\n",
      "Epoch 2 Step 260 Loss 1.0968\n",
      "Epoch 2 Step 261 Loss 1.2908\n",
      "Epoch 2 Step 262 Loss 1.1347\n",
      "Epoch 2 Step 263 Loss 1.0962\n",
      "Epoch 2 Step 264 Loss 1.2798\n",
      "Epoch 2 Step 265 Loss 1.3888\n",
      "Epoch 2 Step 266 Loss 1.6344\n",
      "Epoch 2 Step 267 Loss 1.6043\n",
      "Epoch 2 Step 268 Loss 1.3637\n",
      "Epoch 2 Step 269 Loss 1.1739\n",
      "Epoch 2 Step 270 Loss 1.1659\n",
      "Epoch 2 Step 271 Loss 1.2770\n",
      "Epoch 2 Step 272 Loss 1.3023\n",
      "Epoch 2 Step 273 Loss 1.5313\n",
      "Epoch 2 Step 274 Loss 1.7211\n",
      "Epoch 2 Step 275 Loss 1.4052\n",
      "Epoch 2 Step 276 Loss 1.2911\n",
      "Epoch 2 Step 277 Loss 1.3126\n",
      "Epoch 2 Step 278 Loss 1.3254\n",
      "Epoch 2 Step 279 Loss 1.4352\n",
      "Epoch 2 Step 280 Loss 1.2492\n",
      "Epoch 2 Step 281 Loss 1.5215\n",
      "Epoch 2 Step 282 Loss 1.2608\n",
      "Epoch 2 Step 283 Loss 1.2518\n",
      "Epoch 2 Step 284 Loss 1.2474\n",
      "Epoch 2 Step 285 Loss 1.0708\n",
      "Epoch 2 Step 286 Loss 1.0697\n",
      "Epoch 2 Step 287 Loss 1.2402\n",
      "Epoch 2 Step 288 Loss 1.5856\n",
      "Epoch 2 Step 289 Loss 1.4423\n",
      "Epoch 2 Step 290 Loss 1.4065\n",
      "Epoch 2 Step 291 Loss 1.4654\n",
      "Epoch 2 Step 292 Loss 1.3948\n",
      "Epoch 2 Step 293 Loss 1.2420\n",
      "Epoch 2 Step 294 Loss 1.3363\n",
      "Epoch 2 Step 295 Loss 1.4285\n",
      "Epoch 2 Step 296 Loss 1.3875\n",
      "Epoch 2 Step 297 Loss 1.5585\n",
      "Epoch 2 Step 298 Loss 1.3644\n",
      "Epoch 2 Step 299 Loss 1.3228\n",
      "Epoch 2 Step 300 Loss 1.3553\n",
      "Epoch 2 Step 301 Loss 1.5338\n",
      "Epoch 2 Step 302 Loss 1.6994\n",
      "Epoch 2 Step 303 Loss 1.5738\n",
      "Epoch 2 Step 304 Loss 1.2885\n",
      "Epoch 2 Step 305 Loss 1.3764\n",
      "Epoch 2 Step 306 Loss 1.2996\n",
      "Epoch 2 Step 307 Loss 1.4743\n",
      "Epoch 2 Step 308 Loss 1.3294\n",
      "Epoch 2 Step 309 Loss 1.3576\n",
      "Epoch 2 Step 310 Loss 1.2233\n",
      "Epoch 2 Step 311 Loss 1.3007\n",
      "Epoch 2 Step 312 Loss 1.1707\n",
      "Epoch 2 Step 313 Loss 1.2439\n",
      "Epoch 2 Step 314 Loss 1.5107\n",
      "Epoch 2 Step 315 Loss 1.6355\n",
      "Epoch 2 Step 316 Loss 1.4819\n",
      "Epoch 2 Step 317 Loss 1.3607\n",
      "Epoch 2 Step 318 Loss 1.3908\n",
      "Epoch 2 Step 319 Loss 1.3457\n",
      "Epoch 2 Step 320 Loss 1.3189\n",
      "Epoch 2 Step 321 Loss 1.3354\n",
      "Epoch 2 Step 322 Loss 1.1674\n",
      "Epoch 2 Step 323 Loss 1.4772\n",
      "Epoch 2 Step 324 Loss 1.3740\n",
      "Epoch 2 Step 325 Loss 1.5491\n",
      "Epoch 2 Step 326 Loss 1.4827\n",
      "Epoch 2 Step 327 Loss 1.5973\n",
      "Epoch 2 Step 328 Loss 1.4910\n",
      "Epoch 2 Step 329 Loss 1.2907\n",
      "Epoch 2 Step 330 Loss 1.5766\n",
      "Epoch 2 Step 331 Loss 1.4669\n",
      "Epoch 2 Step 332 Loss 1.4405\n",
      "Epoch 2 Step 333 Loss 1.6815\n",
      "Epoch 2 Step 334 Loss 1.3493\n",
      "Epoch 2 Step 335 Loss 1.1866\n",
      "Epoch 2 Step 336 Loss 1.3538\n",
      "Epoch 2 Step 337 Loss 1.4374\n",
      "Epoch 2 Step 338 Loss 1.3196\n",
      "Epoch 2 Step 339 Loss 1.4975\n",
      "Epoch 2 Step 340 Loss 1.4333\n",
      "Epoch 2 Step 341 Loss 1.5564\n",
      "Epoch 2 Step 342 Loss 1.5666\n",
      "Epoch 2 Step 343 Loss 1.1346\n",
      "Epoch 2 Step 344 Loss 1.5397\n",
      "Epoch 2 Step 345 Loss 1.7387\n",
      "Epoch 2 Step 346 Loss 1.5699\n",
      "Epoch 2 Step 347 Loss 1.4371\n",
      "Epoch 2 Step 348 Loss 1.0730\n",
      "Epoch 2 Step 349 Loss 1.2448\n",
      "Epoch 2 Step 350 Loss 1.2674\n",
      "Epoch 2 Step 351 Loss 1.3462\n",
      "Epoch 2 Step 352 Loss 1.3268\n",
      "Epoch 2 Step 353 Loss 1.5642\n",
      "Epoch 2 Step 354 Loss 1.6531\n",
      "Epoch 2 Step 355 Loss 1.4288\n",
      "Epoch 2 Step 356 Loss 1.2434\n",
      "Epoch 2 Step 357 Loss 1.2718\n",
      "Epoch 2 Step 358 Loss 1.4406\n",
      "Epoch 2 Step 359 Loss 1.2965\n",
      "Epoch 2 Step 360 Loss 1.6725\n",
      "Epoch 2 Step 361 Loss 1.3325\n",
      "Epoch 2 Step 362 Loss 1.5946\n",
      "Epoch 2 Step 363 Loss 1.2495\n",
      "Epoch 2 Step 364 Loss 1.2713\n",
      "Epoch 2 Step 365 Loss 1.1052\n",
      "Epoch 2 Step 366 Loss 1.0455\n",
      "Epoch 2 Step 367 Loss 1.1337\n",
      "Epoch 2 Step 368 Loss 0.9874\n",
      "Epoch 2 Step 369 Loss 0.9704\n",
      "Epoch 2 Step 370 Loss 1.1802\n",
      "Epoch 2 Step 371 Loss 1.3935\n",
      "Epoch 2 Step 372 Loss 1.5537\n",
      "Epoch 2 Step 373 Loss 1.4148\n",
      "Epoch 2 Step 374 Loss 1.2627\n",
      "Epoch 2 Step 375 Loss 1.2753\n",
      "Epoch 2 Step 376 Loss 1.0930\n",
      "Epoch 2 Step 377 Loss 1.3306\n",
      "Epoch 2 Step 378 Loss 1.3090\n",
      "Epoch 2 Step 379 Loss 1.5103\n",
      "Epoch 2 Step 380 Loss 1.4966\n",
      "Epoch 2 Step 381 Loss 1.6944\n",
      "Epoch 2 Step 382 Loss 1.3455\n",
      "Epoch 2 Step 383 Loss 1.5060\n",
      "Epoch 2 Step 384 Loss 1.6964\n",
      "Epoch 2 Step 385 Loss 1.5941\n",
      "Epoch 2 Step 386 Loss 1.6462\n",
      "Epoch 2 Step 387 Loss 1.6081\n",
      "Epoch 2 Step 388 Loss 1.4065\n",
      "Epoch 2 Step 389 Loss 1.3532\n",
      "Epoch 2 Step 390 Loss 1.3858\n",
      "Epoch 2 Step 391 Loss 1.4320\n",
      "Epoch 2 Step 392 Loss 1.4401\n",
      "Epoch 2 Step 393 Loss 1.2569\n",
      "Epoch 2 Step 394 Loss 1.3851\n",
      "Epoch 2 Step 395 Loss 1.3693\n",
      "Epoch 2 Step 396 Loss 1.1848\n",
      "Epoch 2 Step 397 Loss 0.9346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Step 398 Loss 1.0727\n",
      "Epoch 2 Step 399 Loss 1.2374\n",
      "Epoch 2 Step 400 Loss 1.2682\n",
      "Epoch 2 Step 401 Loss 1.1327\n",
      "Epoch 2 Step 402 Loss 1.1833\n",
      "Epoch 2 Step 403 Loss 1.5080\n",
      "Epoch 2 Step 404 Loss 1.5451\n",
      "Epoch 2 Step 405 Loss 1.6545\n",
      "Epoch 2 Step 406 Loss 1.3269\n",
      "Epoch 2 Step 407 Loss 1.5589\n",
      "Epoch 2 Step 408 Loss 1.5570\n",
      "Epoch 2 Step 409 Loss 1.9131\n",
      "Epoch 2 Step 410 Loss 1.5480\n",
      "Epoch 2 Step 411 Loss 1.2065\n",
      "Epoch 2 Step 412 Loss 1.3845\n",
      "Epoch 2 Step 413 Loss 1.6209\n",
      "Epoch 2 Step 414 Loss 1.4569\n",
      "Epoch 2 Step 415 Loss 1.5612\n",
      "Epoch 2 Step 416 Loss 1.2086\n",
      "Epoch 2 Step 417 Loss 1.2382\n",
      "Epoch 2 Step 418 Loss 1.2134\n",
      "Epoch 2 Step 419 Loss 1.0444\n",
      "Epoch 2 Step 420 Loss 1.3883\n",
      "Epoch 2 Step 421 Loss 1.3934\n",
      "Epoch 2 Step 422 Loss 1.3694\n",
      "Epoch 2 Step 423 Loss 1.3063\n",
      "Epoch 2 Step 424 Loss 1.2933\n",
      "Epoch 2 Step 425 Loss 1.4188\n",
      "Epoch 2 Step 426 Loss 1.3730\n",
      "Epoch 2 Step 427 Loss 1.3535\n",
      "Epoch 2 Step 428 Loss 1.1966\n",
      "Epoch 2 Step 429 Loss 1.2620\n",
      "Epoch 2 Step 430 Loss 1.4871\n",
      "Epoch 2 Step 431 Loss 1.5845\n",
      "Epoch 2 Step 432 Loss 1.4265\n",
      "Epoch 2 Step 433 Loss 1.5442\n",
      "Epoch 2 Step 434 Loss 1.1945\n",
      "Epoch 2 Step 435 Loss 1.0830\n",
      "Epoch 2 Step 436 Loss 1.1411\n",
      "Epoch 2 Step 437 Loss 1.1231\n",
      "Epoch 2 Step 438 Loss 1.2971\n",
      "Epoch 2 Step 439 Loss 1.2049\n",
      "Epoch 2 Step 440 Loss 1.1323\n",
      "Epoch 2 Step 441 Loss 1.2157\n",
      "Epoch 2 Step 442 Loss 1.5120\n",
      "Epoch 2 Step 443 Loss 1.3181\n",
      "Epoch 2 Step 444 Loss 1.1231\n",
      "Epoch 2 Step 445 Loss 1.3048\n",
      "Epoch 2 Step 446 Loss 1.2957\n",
      "Epoch 2 Step 447 Loss 1.1969\n",
      "Epoch 2 Step 448 Loss 1.3948\n",
      "Epoch 2 Step 449 Loss 1.2147\n",
      "Epoch 2 Step 450 Loss 1.2421\n",
      "Epoch 2 Step 451 Loss 1.4786\n",
      "Epoch 2 Step 452 Loss 1.4706\n",
      "Epoch 2 Step 453 Loss 1.6740\n",
      "Epoch 2 Step 454 Loss 1.5035\n",
      "Epoch 2 Step 455 Loss 1.4778\n",
      "Epoch 2 Step 456 Loss 1.4841\n",
      "Epoch 2 Step 457 Loss 1.4519\n",
      "Epoch 2 Step 458 Loss 1.3853\n",
      "Epoch 2 Step 459 Loss 1.5121\n",
      "Epoch 2 Step 460 Loss 1.5635\n",
      "Epoch 2 Step 461 Loss 1.3848\n",
      "Epoch 2 Step 462 Loss 1.4474\n",
      "Epoch 2 Step 463 Loss 1.5469\n",
      "Epoch 2 Step 464 Loss 1.6782\n",
      "Epoch 2 Step 465 Loss 1.7416\n",
      "Epoch 2 Step 466 Loss 1.5556\n",
      "Epoch 2 Step 467 Loss 1.7972\n",
      "Epoch 2 Step 468 Loss 1.6658\n",
      "Epoch 2 Step 469 Loss 1.4924\n",
      "Epoch 2 Step 470 Loss 1.4264\n",
      "Epoch 2 Step 471 Loss 1.1248\n",
      "Epoch 2 Step 472 Loss 1.4421\n",
      "Epoch 2 Step 473 Loss 1.4130\n",
      "Epoch 2 Step 474 Loss 1.5678\n",
      "Epoch 2 Step 475 Loss 1.2926\n",
      "Epoch 2 Step 476 Loss 1.4090\n",
      "Epoch 2 Step 477 Loss 1.3956\n",
      "Epoch 2 Step 478 Loss 1.5692\n",
      "Epoch 2 Step 479 Loss 1.6195\n",
      "Epoch 2 Step 480 Loss 1.4633\n",
      "Epoch 2 Step 481 Loss 1.4095\n",
      "Epoch 2 Step 482 Loss 1.3964\n",
      "Epoch 2 Step 483 Loss 1.5496\n",
      "Epoch 2 Step 484 Loss 1.1429\n",
      "Epoch 2 Step 485 Loss 1.2065\n",
      "Epoch 2 Step 486 Loss 1.4083\n",
      "Epoch 2 Step 487 Loss 1.4596\n",
      "Epoch 2 Step 488 Loss 1.1634\n",
      "Epoch 2 Step 489 Loss 1.2902\n",
      "Epoch 2 Step 490 Loss 1.2291\n",
      "Epoch 2 Step 491 Loss 1.3704\n",
      "Epoch 2 Step 492 Loss 1.2205\n",
      "Epoch 2 Step 493 Loss 1.4849\n",
      "Epoch 2 Step 494 Loss 1.2221\n",
      "Epoch 2 Step 495 Loss 1.3296\n",
      "Epoch 2 Step 496 Loss 1.3192\n",
      "Epoch 2 Step 497 Loss 1.0485\n",
      "Epoch 2 Step 498 Loss 1.3004\n",
      "Epoch 2 Step 499 Loss 1.3790\n",
      "Epoch 2 Step 500 Loss 1.2783\n",
      "Epoch 2 Step 501 Loss 1.3274\n",
      "Epoch 2 Step 502 Loss 1.4927\n",
      "Epoch 2 Step 503 Loss 1.4877\n",
      "Epoch 2 Step 504 Loss 1.3630\n",
      "Epoch 2 Step 505 Loss 1.2568\n",
      "Epoch 2 Step 506 Loss 1.4305\n",
      "Epoch 2 Step 507 Loss 1.4264\n",
      "Epoch 2 Step 508 Loss 1.4804\n",
      "Epoch 2 Step 509 Loss 1.2354\n",
      "Epoch 2 Step 510 Loss 1.4714\n",
      "Epoch 2 Step 511 Loss 1.5977\n",
      "Epoch 2 Step 512 Loss 1.3890\n",
      "Epoch 2 Step 513 Loss 1.2435\n",
      "Epoch 2 Step 514 Loss 1.1212\n",
      "Epoch 2 Step 515 Loss 1.2094\n",
      "Epoch 2 Step 516 Loss 1.3032\n",
      "Epoch 2 Step 517 Loss 1.4227\n",
      "Epoch 2 Step 518 Loss 1.1947\n",
      "Epoch 2 Step 519 Loss 1.4063\n",
      "Epoch 2 Step 520 Loss 1.0689\n",
      "Epoch 2 Step 521 Loss 1.1639\n",
      "Epoch 2 Step 522 Loss 1.0591\n",
      "Epoch 2 Step 523 Loss 1.1299\n",
      "Epoch 2 Step 524 Loss 1.7195\n",
      "Epoch 2 Step 525 Loss 1.6459\n",
      "Epoch 2 Step 526 Loss 1.7550\n",
      "Epoch 2 Step 527 Loss 1.3994\n",
      "Epoch 2 Step 528 Loss 1.2519\n",
      "Epoch 2 Step 529 Loss 1.3883\n",
      "Epoch 2 Step 530 Loss 1.2396\n",
      "Epoch 2 Step 531 Loss 1.0625\n",
      "Epoch 2 Step 532 Loss 1.1816\n",
      "Epoch 2 Step 533 Loss 1.1194\n",
      "Epoch 2 Step 534 Loss 1.1109\n",
      "Epoch 2 Step 535 Loss 1.0131\n",
      "Epoch 2 Step 536 Loss 1.2658\n",
      "Epoch 2 Step 537 Loss 1.3809\n",
      "Epoch 2 Step 538 Loss 1.4178\n",
      "Epoch 2 Step 539 Loss 1.1080\n",
      "Epoch 2 Step 540 Loss 1.4807\n",
      "Epoch 2 Step 541 Loss 1.5405\n",
      "Epoch 2 Step 542 Loss 1.3684\n",
      "Epoch 2 Step 543 Loss 1.3757\n",
      "Epoch 2 Step 544 Loss 1.4825\n",
      "Epoch 2 Step 545 Loss 1.4467\n",
      "Epoch 2 Step 546 Loss 1.4086\n",
      "Epoch 2 Step 547 Loss 1.8675\n",
      "Epoch 2 Step 548 Loss 1.4992\n",
      "Epoch 2 Step 549 Loss 1.1939\n",
      "Epoch 2 Step 550 Loss 1.2174\n",
      "Epoch 2 Step 551 Loss 1.1344\n",
      "Epoch 2 Step 552 Loss 1.4882\n",
      "Epoch 2 Step 553 Loss 1.5026\n",
      "Epoch 2 Step 554 Loss 1.6807\n",
      "Epoch 2 Step 555 Loss 1.4581\n",
      "Epoch 2 Step 556 Loss 1.1481\n",
      "Epoch 2 Step 557 Loss 1.4961\n",
      "Epoch 2 Step 558 Loss 1.3670\n",
      "Epoch 2 Step 559 Loss 1.3890\n",
      "Epoch 2 Step 560 Loss 1.5359\n",
      "Epoch 2 Step 561 Loss 1.4045\n",
      "Epoch 2 Step 562 Loss 1.5178\n",
      "Epoch 2 Step 563 Loss 1.5091\n",
      "Epoch 2 Step 564 Loss 1.5469\n",
      "Epoch 2 Step 565 Loss 1.6860\n",
      "Epoch 2 Step 566 Loss 1.5925\n",
      "Epoch 2 Step 567 Loss 1.3748\n",
      "Epoch 2 Step 568 Loss 1.3603\n",
      "Epoch 2 Step 569 Loss 1.7309\n",
      "Epoch 2 Step 570 Loss 1.5207\n",
      "Epoch 2 Step 571 Loss 1.1885\n",
      "Epoch 2 Step 572 Loss 1.0266\n",
      "Epoch 2 Step 573 Loss 1.1289\n",
      "Epoch 2 Step 574 Loss 1.4275\n",
      "Epoch 2 Step 575 Loss 1.3936\n",
      "Epoch 2 Step 576 Loss 1.4191\n",
      "Epoch 2 Step 577 Loss 1.4193\n",
      "Epoch 2 Step 578 Loss 1.5166\n",
      "Epoch 2 Step 579 Loss 1.5192\n",
      "Epoch 2 Step 580 Loss 1.1397\n",
      "Epoch 2 Step 581 Loss 1.3773\n",
      "Epoch 2 Step 582 Loss 1.3985\n",
      "Epoch 2 Step 583 Loss 1.3299\n",
      "Epoch 2 Step 584 Loss 1.3141\n",
      "Epoch 2 Step 585 Loss 1.3117\n",
      "Epoch 2 Step 586 Loss 1.2494\n",
      "Epoch 2 Step 587 Loss 0.8922\n",
      "Epoch 2 Step 588 Loss 1.1015\n",
      "Epoch 2 Step 589 Loss 1.1186\n",
      "Epoch 2 Step 590 Loss 1.3454\n",
      "Epoch 2 Step 591 Loss 1.3567\n",
      "Epoch 2 Step 592 Loss 1.5054\n",
      "Epoch 2 Step 593 Loss 1.4146\n",
      "Epoch 2 Step 594 Loss 1.5338\n",
      "Epoch 2 Step 595 Loss 1.3919\n",
      "Epoch 2 Step 596 Loss 1.6415\n",
      "Epoch 2 Step 597 Loss 1.4058\n",
      "Epoch 2 Step 598 Loss 1.1560\n",
      "Epoch 2 Step 599 Loss 1.3738\n",
      "Epoch 2 Step 600 Loss 1.2997\n",
      "Epoch 2 Step 601 Loss 1.5226\n",
      "Epoch 2 Step 602 Loss 1.2185\n",
      "Epoch 2 Step 603 Loss 1.0776\n",
      "Epoch 2 Step 604 Loss 1.3060\n",
      "Epoch 2 Step 605 Loss 1.3950\n",
      "Epoch 2 Step 606 Loss 1.4855\n",
      "Epoch 2 Step 607 Loss 1.4892\n",
      "Epoch 2 Step 608 Loss 1.4663\n",
      "Epoch 2 Step 609 Loss 1.3897\n",
      "Epoch 2 Step 610 Loss 1.3152\n",
      "Epoch 2 Step 611 Loss 1.2201\n",
      "Epoch 2 Step 612 Loss 1.3791\n",
      "Epoch 2 Step 613 Loss 1.4530\n",
      "Epoch 2 Step 614 Loss 1.2896\n",
      "Epoch 2 Step 615 Loss 1.9254\n",
      "Epoch 2 Step 616 Loss 1.4664\n",
      "Epoch 2 Step 617 Loss 1.5209\n",
      "Epoch 2 Step 618 Loss 1.2163\n",
      "Epoch 2 Step 619 Loss 1.3184\n",
      "Epoch 2 Step 620 Loss 1.2686\n",
      "Epoch 2 Step 621 Loss 1.4237\n",
      "Epoch 2 Step 622 Loss 1.4788\n",
      "Epoch 2 Step 623 Loss 1.5675\n",
      "Epoch 2 Step 624 Loss 1.3730\n",
      "Epoch 2 Step 625 Loss 1.5029\n",
      "Epoch 2 Step 626 Loss 1.4769\n",
      "Epoch 2 Step 627 Loss 1.5925\n",
      "Epoch 2 Step 628 Loss 1.1695\n",
      "Epoch 2 Step 629 Loss 1.4308\n",
      "Epoch 2 Step 630 Loss 1.3387\n",
      "Epoch 2 Step 631 Loss 1.5393\n",
      "Epoch 2 Step 632 Loss 1.6091\n",
      "Epoch 2 Step 633 Loss 1.3361\n",
      "Epoch 2 Step 634 Loss 1.6519\n",
      "Epoch 2 Step 635 Loss 1.5441\n",
      "Epoch 2 Step 636 Loss 1.7819\n",
      "Epoch 2 Step 637 Loss 1.4663\n",
      "Epoch 2 Step 638 Loss 1.7864\n",
      "Epoch 2 Step 639 Loss 1.4197\n",
      "Epoch 2 Step 640 Loss 1.4560\n",
      "Epoch 2 Step 641 Loss 1.2598\n",
      "Epoch 2 Step 642 Loss 1.0014\n",
      "Epoch 2 Step 643 Loss 1.1734\n",
      "Epoch 2 Step 644 Loss 1.4310\n",
      "Epoch 2 Step 645 Loss 1.1433\n",
      "Epoch 2 Step 646 Loss 1.0735\n",
      "Epoch 2 Step 647 Loss 0.9357\n",
      "Epoch 2 Step 648 Loss 0.9036\n",
      "Epoch 2 Step 649 Loss 1.0344\n",
      "Epoch 2 Step 650 Loss 1.1301\n",
      "Epoch 2 Step 651 Loss 1.0723\n",
      "Epoch 2 Step 652 Loss 1.2239\n",
      "Epoch 2 Step 653 Loss 1.2139\n",
      "Epoch 2 Step 654 Loss 1.5029\n",
      "Epoch 2 Step 655 Loss 1.3423\n",
      "Epoch 2 Step 656 Loss 0.9951\n",
      "Epoch 2 Step 657 Loss 1.1080\n",
      "Epoch 2 Step 658 Loss 1.2073\n",
      "Epoch 2 Step 659 Loss 1.2317\n",
      "Epoch 2 Step 660 Loss 1.3252\n",
      "Epoch 2 Step 661 Loss 1.2325\n",
      "Epoch 2 Step 662 Loss 1.5616\n",
      "Epoch 2 Step 663 Loss 1.3977\n",
      "Epoch 2 Step 664 Loss 1.2234\n",
      "Epoch 2 Step 665 Loss 0.9279\n",
      "Epoch 2 Step 666 Loss 1.1823\n",
      "Epoch 2 Step 667 Loss 1.2688\n",
      "Epoch 2 Step 668 Loss 1.4898\n",
      "Epoch 2 Step 669 Loss 1.9268\n",
      "Epoch 2 Step 670 Loss 1.3827\n",
      "Epoch 2 Step 671 Loss 1.3693\n",
      "Epoch 2 Step 672 Loss 1.3329\n",
      "Epoch 2 Step 673 Loss 1.2781\n",
      "Epoch 2 Step 674 Loss 1.4124\n",
      "Epoch 2 Step 675 Loss 1.3124\n",
      "Epoch 2 Step 676 Loss 1.2344\n",
      "Epoch 2 Step 677 Loss 1.2787\n",
      "Epoch 2 Step 678 Loss 1.3860\n",
      "Epoch 2 Step 679 Loss 1.2842\n",
      "Epoch 2 Step 680 Loss 1.4095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Step 681 Loss 1.5401\n",
      "Epoch 2 Step 682 Loss 1.1190\n",
      "Epoch 2 Step 683 Loss 1.3389\n",
      "Epoch 2 Step 684 Loss 1.2423\n",
      "Epoch 2 Step 685 Loss 1.4725\n",
      "Epoch 2 Step 686 Loss 1.4918\n",
      "Epoch 2 Step 687 Loss 1.1671\n",
      "Epoch 2 Step 688 Loss 0.8373\n",
      "Epoch 2 Step 689 Loss 1.2598\n",
      "Epoch 2 Step 690 Loss 1.4465\n",
      "Epoch 2 Step 691 Loss 1.2563\n",
      "Epoch 2 Step 692 Loss 1.4369\n",
      "Epoch 2 Step 693 Loss 1.3136\n",
      "Epoch 2 Step 694 Loss 1.3515\n",
      "Epoch 2 Step 695 Loss 1.3226\n",
      "Epoch 2 Step 696 Loss 1.2912\n",
      "Epoch 2 Step 697 Loss 1.3642\n",
      "Epoch 2 Step 698 Loss 1.3943\n",
      "Epoch 2 Step 699 Loss 1.1426\n",
      "Epoch 2 Step 700 Loss 1.0780\n",
      "Epoch 2 Step 701 Loss 1.2616\n",
      "Epoch 2 Step 702 Loss 1.3106\n",
      "Epoch 2 Step 703 Loss 1.7230\n",
      "Epoch 2 Step 704 Loss 1.4909\n",
      "Epoch 2 Step 705 Loss 1.2866\n",
      "Epoch 2 Step 706 Loss 1.1702\n",
      "Epoch 2 Step 707 Loss 0.9458\n",
      "Epoch 2 Step 708 Loss 1.1398\n",
      "Epoch 2 Step 709 Loss 1.3713\n",
      "Epoch 2 Step 710 Loss 1.2544\n",
      "Epoch 2 Step 711 Loss 1.4465\n",
      "Epoch 2 Step 712 Loss 1.2190\n",
      "Epoch 2 Step 713 Loss 1.2865\n",
      "Epoch 2 Step 714 Loss 1.2020\n",
      "Epoch 2 Step 715 Loss 1.3501\n",
      "Epoch 2 Step 716 Loss 1.4460\n",
      "Epoch 2 Step 717 Loss 1.6049\n",
      "Epoch 2 Step 718 Loss 1.4389\n",
      "Epoch 2 Step 719 Loss 1.6088\n",
      "Epoch 2 Step 720 Loss 1.2695\n",
      "Epoch 2 Step 721 Loss 1.5961\n",
      "Epoch 2 Step 722 Loss 1.7579\n",
      "Epoch 2 Step 723 Loss 1.4061\n",
      "Epoch 2 Step 724 Loss 1.1214\n",
      "Epoch 2 Step 725 Loss 1.2895\n",
      "Epoch 2 Step 726 Loss 1.2650\n",
      "Epoch 2 Step 727 Loss 1.2746\n",
      "Epoch 2 Step 728 Loss 1.1709\n",
      "Epoch 2 Step 729 Loss 0.8954\n",
      "Epoch 2 Step 730 Loss 0.8071\n",
      "Epoch 2 Step 731 Loss 1.1134\n",
      "Epoch 2 Step 732 Loss 1.1137\n",
      "Epoch 2 Step 733 Loss 1.4822\n",
      "Epoch 2 Step 734 Loss 1.4112\n",
      "Epoch 2 Step 735 Loss 1.2309\n",
      "Epoch 2 Step 736 Loss 1.4233\n",
      "Epoch 2 Step 737 Loss 1.5253\n",
      "Epoch 2 Step 738 Loss 1.2688\n",
      "Epoch 2 Step 739 Loss 1.1675\n",
      "Epoch 2 Step 740 Loss 1.3885\n",
      "Epoch 2 Step 741 Loss 1.4574\n",
      "Epoch 2 Step 742 Loss 1.1156\n",
      "Epoch 2 Step 743 Loss 1.0542\n",
      "Epoch 2 Step 744 Loss 1.1381\n",
      "Epoch 2 Step 745 Loss 1.4127\n",
      "Epoch 2 Step 746 Loss 1.4784\n",
      "Epoch 2 Step 747 Loss 1.2350\n",
      "Epoch 2 Step 748 Loss 1.1386\n",
      "Epoch 2 Step 749 Loss 1.2942\n",
      "Epoch 2 Step 750 Loss 1.2814\n",
      "Epoch 2 Step 751 Loss 1.3921\n",
      "Epoch 2 Step 752 Loss 1.3183\n",
      "Epoch 2 Step 753 Loss 1.1382\n",
      "Epoch 2 Step 754 Loss 1.3035\n",
      "Epoch 2 Step 755 Loss 1.4177\n",
      "Epoch 2 Step 756 Loss 1.5258\n",
      "Epoch 2 Step 757 Loss 1.5532\n",
      "Epoch 2 Step 758 Loss 1.4598\n",
      "Epoch 2 Step 759 Loss 1.2548\n",
      "Epoch 2 Step 760 Loss 1.3002\n",
      "Epoch 2 Step 761 Loss 1.4853\n",
      "Epoch 2 Step 762 Loss 1.4162\n",
      "Epoch 2 Step 763 Loss 1.2629\n",
      "Epoch 2 Step 764 Loss 1.8070\n",
      "Epoch 2 Step 765 Loss 1.3213\n",
      "Epoch 2 Step 766 Loss 1.4924\n",
      "Epoch 2 Step 767 Loss 1.5250\n",
      "Epoch 2 Step 768 Loss 1.3758\n",
      "Epoch 2 Step 769 Loss 1.5483\n",
      "Epoch 2 Step 770 Loss 1.1917\n",
      "Epoch 2 Step 771 Loss 1.0651\n",
      "Epoch 2 Step 772 Loss 1.4461\n",
      "Epoch 2 Step 773 Loss 1.4408\n",
      "Epoch 2 Step 774 Loss 1.3160\n",
      "Epoch 2 Step 775 Loss 1.3202\n",
      "Epoch 2 Step 776 Loss 1.2476\n",
      "Epoch 2 Step 777 Loss 1.6218\n",
      "Epoch 2 Step 778 Loss 1.2126\n",
      "Epoch 2 Step 779 Loss 1.0064\n",
      "Epoch 2 Step 780 Loss 1.2745\n",
      "Epoch 2 Step 781 Loss 1.4075\n",
      "Epoch 2 Step 782 Loss 1.4477\n",
      "Epoch 2 Step 783 Loss 1.3535\n",
      "Epoch 2 Step 784 Loss 1.5410\n",
      "Epoch 2 Step 785 Loss 1.5573\n",
      "Epoch 2 Step 786 Loss 1.7049\n",
      "Epoch 2 Step 787 Loss 1.5657\n",
      "Epoch 2 Step 788 Loss 1.4391\n",
      "Epoch 2 Step 789 Loss 1.4816\n",
      "Epoch 2 Step 790 Loss 1.2848\n",
      "Epoch 2 Step 791 Loss 1.4118\n",
      "Epoch 2 Step 792 Loss 1.5299\n",
      "Epoch 2 Step 793 Loss 1.3953\n",
      "Epoch 2 Step 794 Loss 1.2661\n",
      "Epoch 2 Step 795 Loss 1.5785\n",
      "Epoch 2 Step 796 Loss 1.3368\n",
      "Epoch 2 Step 797 Loss 1.4839\n",
      "Epoch 2 Step 798 Loss 1.2597\n",
      "Epoch 2 Step 799 Loss 1.6265\n",
      "Epoch 2 Step 800 Loss 1.8045\n",
      "Epoch 2 Step 801 Loss 1.2818\n",
      "Epoch 2 Step 802 Loss 1.8812\n",
      "Epoch 2 Step 803 Loss 1.2693\n",
      "Epoch 2 Step 804 Loss 1.5914\n",
      "Epoch 2 Step 805 Loss 1.2489\n",
      "Epoch 2 Step 806 Loss 1.4070\n",
      "Epoch 2 Step 807 Loss 1.3571\n",
      "Epoch 2 Step 808 Loss 1.1914\n",
      "Epoch 2 Step 809 Loss 1.4191\n",
      "Epoch 2 Step 810 Loss 1.3332\n",
      "Epoch 2 Step 811 Loss 1.3056\n",
      "Epoch 2 Step 812 Loss 1.4063\n",
      "Epoch 2 Step 813 Loss 1.4331\n",
      "Epoch 2 Step 814 Loss 1.7843\n",
      "Epoch 2 Step 815 Loss 1.5670\n",
      "Epoch 2 Step 816 Loss 1.5161\n",
      "Epoch 2 Step 817 Loss 1.5334\n",
      "Epoch 2 Step 818 Loss 1.3676\n",
      "Epoch 2 Step 819 Loss 1.4810\n",
      "Epoch 2 Step 820 Loss 1.3838\n",
      "Epoch 2 Step 821 Loss 1.2778\n",
      "Epoch 2 Step 822 Loss 1.0134\n",
      "Epoch 2 Step 823 Loss 0.8920\n",
      "Epoch 2 Step 824 Loss 1.5090\n",
      "Epoch 2 Step 825 Loss 1.4245\n",
      "Epoch 2 Step 826 Loss 1.2253\n",
      "Epoch 2 Step 827 Loss 1.5060\n",
      "Epoch 2 Step 828 Loss 1.3643\n",
      "Epoch 2 Step 829 Loss 1.4185\n",
      "Epoch 2 Step 830 Loss 1.3152\n",
      "Epoch 2 Step 831 Loss 1.7139\n",
      "Epoch 2 Step 832 Loss 1.7239\n",
      "Epoch 2 Step 833 Loss 1.6215\n",
      "Epoch 2 Step 834 Loss 1.4299\n",
      "Epoch 2 Step 835 Loss 1.5123\n",
      "Epoch 2 Step 836 Loss 1.2708\n",
      "Epoch 2 Step 837 Loss 1.4827\n",
      "Epoch 2 Step 838 Loss 1.5888\n",
      "Epoch 2 Step 839 Loss 1.4735\n",
      "Epoch 2 Step 840 Loss 1.2289\n",
      "Epoch 2 Step 841 Loss 1.1670\n",
      "Epoch 2 Step 842 Loss 1.1530\n",
      "Epoch 2 Step 843 Loss 1.2107\n",
      "Epoch 2 Step 844 Loss 1.1488\n",
      "Epoch 2 Step 845 Loss 1.2405\n",
      "Epoch 2 Step 846 Loss 1.4583\n",
      "Epoch 2 Step 847 Loss 1.4160\n",
      "Epoch 2 Step 848 Loss 1.4346\n",
      "Epoch 2 Step 849 Loss 1.6926\n",
      "Epoch 2 Step 850 Loss 1.5468\n",
      "Epoch 2 Step 851 Loss 1.5616\n",
      "Epoch 2 Step 852 Loss 1.2199\n",
      "Epoch 2 Step 853 Loss 1.6314\n",
      "Epoch 2 Step 854 Loss 1.4566\n",
      "Epoch 2 Step 855 Loss 1.5126\n",
      "Epoch 2 Step 856 Loss 1.6127\n",
      "Epoch 2 Step 857 Loss 1.4099\n",
      "Epoch 2 Step 858 Loss 1.3048\n",
      "Epoch 2 Step 859 Loss 1.3572\n",
      "Epoch 2 Step 860 Loss 1.2604\n",
      "Epoch 2 Step 861 Loss 1.3559\n",
      "Epoch 2 Step 862 Loss 1.2120\n",
      "Epoch 2 Step 863 Loss 1.5401\n",
      "Epoch 2 Step 864 Loss 1.5426\n",
      "Epoch 2 Step 865 Loss 1.5361\n",
      "Epoch 2 Step 866 Loss 1.1998\n",
      "Epoch 2 Step 867 Loss 1.1376\n",
      "Epoch 2 Step 868 Loss 1.6692\n",
      "Epoch 2 Step 869 Loss 1.4854\n",
      "Epoch 2 Step 870 Loss 1.3123\n",
      "Epoch 2 Step 871 Loss 0.9925\n",
      "Epoch 2 Step 872 Loss 1.2140\n",
      "Epoch 2 Step 873 Loss 1.3373\n",
      "Epoch 2 Step 874 Loss 1.3751\n",
      "Epoch 2 Step 875 Loss 1.5264\n",
      "Epoch 2 Step 876 Loss 1.6193\n",
      "Epoch 2 Step 877 Loss 1.3593\n",
      "Epoch 2 Step 878 Loss 1.3708\n",
      "Epoch 2 Step 879 Loss 1.3911\n",
      "Epoch 2 Step 880 Loss 1.4862\n",
      "Epoch 2 Step 881 Loss 1.3682\n",
      "Epoch 2 Step 882 Loss 1.2037\n",
      "Epoch 2 Step 883 Loss 1.4119\n",
      "Epoch 2 Step 884 Loss 1.7495\n",
      "Epoch 2 Step 885 Loss 1.5681\n",
      "Epoch 2 Step 886 Loss 1.8252\n",
      "Epoch 2 Step 887 Loss 1.3143\n",
      "Epoch 2 Step 888 Loss 1.3704\n",
      "Epoch 2 Step 889 Loss 1.5842\n",
      "Epoch 2 Step 890 Loss 1.6119\n",
      "Epoch 2 Step 891 Loss 1.5427\n",
      "Epoch 2 Step 892 Loss 1.4653\n",
      "Epoch 2 Step 893 Loss 1.5523\n",
      "Epoch 2 Step 894 Loss 1.4900\n",
      "Epoch 2 Step 895 Loss 1.2734\n",
      "Epoch 2 Step 896 Loss 1.6205\n",
      "Epoch 2 Step 897 Loss 1.2768\n",
      "Epoch 2 Step 898 Loss 1.3164\n",
      "Epoch 2 Step 899 Loss 1.4471\n",
      "Epoch 2 Step 900 Loss 1.3164\n",
      "Epoch 2 Step 901 Loss 1.4669\n",
      "Epoch 2 Step 902 Loss 1.3276\n",
      "Epoch 2 Step 903 Loss 1.4493\n",
      "Epoch 2 Step 904 Loss 1.6422\n",
      "Epoch 2 Step 905 Loss 1.5290\n",
      "Epoch 2 Step 906 Loss 1.7825\n",
      "Epoch 2 Step 907 Loss 1.4112\n",
      "Epoch 2 Step 908 Loss 1.5023\n",
      "Epoch 2 Step 909 Loss 1.3107\n",
      "Epoch 2 Step 910 Loss 1.1244\n",
      "Epoch 2 Step 911 Loss 1.2565\n",
      "Epoch 2 Step 912 Loss 1.4925\n",
      "Epoch 2 Step 913 Loss 1.5227\n",
      "Epoch 2 Step 914 Loss 1.2864\n",
      "Epoch 2 Step 915 Loss 1.5125\n",
      "Epoch 2 Step 916 Loss 1.6900\n",
      "Epoch 2 Step 917 Loss 1.6439\n",
      "Epoch 2 Step 918 Loss 1.3747\n",
      "Epoch 2 Step 919 Loss 1.2838\n",
      "Epoch 2 Step 920 Loss 1.3722\n",
      "Epoch 2 Step 921 Loss 1.1243\n",
      "Epoch 2 Step 922 Loss 1.3721\n",
      "Epoch 2 Step 923 Loss 1.2966\n",
      "Epoch 2 Step 924 Loss 1.6056\n",
      "Epoch 2 Step 925 Loss 1.0491\n",
      "Epoch 2 Step 926 Loss 1.2843\n",
      "Epoch 2 Step 927 Loss 1.1624\n",
      "Epoch 2 Step 928 Loss 1.1438\n",
      "Epoch 2 Step 929 Loss 1.0399\n",
      "Epoch 2 Step 930 Loss 1.1594\n",
      "Epoch 2 Step 931 Loss 1.4155\n",
      "Epoch 2 Step 932 Loss 1.4682\n",
      "Epoch 2 Step 933 Loss 1.2952\n",
      "Epoch 2 Step 934 Loss 1.2924\n",
      "Epoch 2 Step 935 Loss 1.4611\n",
      "Epoch 2 Step 936 Loss 1.7054\n",
      "Epoch 2 Step 937 Loss 1.5853\n",
      "Epoch 2 Step 938 Loss 1.4474\n",
      "Epoch 2 Step 939 Loss 1.3665\n",
      "Epoch 2 Step 940 Loss 1.3944\n",
      "Epoch 2 Step 941 Loss 1.4026\n",
      "Epoch 2 Step 942 Loss 1.4704\n",
      "Epoch 2 Step 943 Loss 1.5162\n",
      "Epoch 2 Step 944 Loss 1.1298\n",
      "Epoch 2 Step 945 Loss 1.1825\n",
      "Epoch 2 Step 946 Loss 0.9868\n",
      "Epoch 2 Step 947 Loss 1.2203\n",
      "Epoch 2 Step 948 Loss 1.4178\n",
      "Epoch 2 Step 949 Loss 1.4183\n",
      "Epoch 2 Step 950 Loss 1.3800\n",
      "Epoch 2 Step 951 Loss 1.7163\n",
      "Epoch 2 Step 952 Loss 1.5010\n",
      "Epoch 2 Step 953 Loss 1.5586\n",
      "Epoch 2 Step 954 Loss 1.7765\n",
      "Epoch 2 Step 955 Loss 1.6726\n",
      "Epoch 2 Step 956 Loss 1.4895\n",
      "Epoch 2 Step 957 Loss 1.3507\n",
      "Epoch 2 Step 958 Loss 1.0706\n",
      "Epoch 2 Step 959 Loss 1.5729\n",
      "Epoch 2 Step 960 Loss 1.3841\n",
      "Epoch 2 Step 961 Loss 1.3667\n",
      "Epoch 2 Step 962 Loss 1.4123\n",
      "Epoch 2 Step 963 Loss 1.5870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Step 964 Loss 1.3941\n",
      "Epoch 2 Step 965 Loss 1.3389\n",
      "Epoch 2 Step 966 Loss 1.4772\n",
      "Epoch 2 Step 967 Loss 1.5092\n",
      "Epoch 2 Step 968 Loss 1.5010\n",
      "Epoch 2 Step 969 Loss 1.5444\n",
      "Epoch 2 Step 970 Loss 1.5623\n",
      "Epoch 2 Step 971 Loss 1.5168\n",
      "Epoch 2 Step 972 Loss 1.5240\n",
      "Epoch 2 Step 973 Loss 1.6286\n",
      "Epoch 2 Step 974 Loss 1.3423\n",
      "Epoch 2 Step 975 Loss 1.3782\n",
      "Epoch 2 Step 976 Loss 1.2018\n",
      "Epoch 2 Step 977 Loss 1.8924\n",
      "Epoch 2 Step 978 Loss 1.6991\n",
      "Epoch 2 Step 979 Loss 1.3151\n",
      "Epoch 2 Step 980 Loss 1.5550\n",
      "Epoch 2 Step 981 Loss 1.5112\n",
      "Epoch 2 Step 982 Loss 1.2432\n",
      "Epoch 2 Step 983 Loss 1.3353\n",
      "Epoch 2 Step 984 Loss 1.7030\n",
      "Epoch 2 Step 985 Loss 1.7976\n",
      "Epoch 2 Step 986 Loss 1.2433\n",
      "Epoch 2 Step 987 Loss 1.4414\n",
      "Epoch 2 Step 988 Loss 1.6484\n",
      "Epoch 2 Step 989 Loss 1.1843\n",
      "Epoch 2 Step 990 Loss 1.3600\n",
      "Epoch 2 Step 991 Loss 1.2062\n",
      "Epoch 2 Step 992 Loss 1.6143\n",
      "Epoch 2 Step 993 Loss 1.3164\n",
      "Epoch 2 Step 994 Loss 1.2919\n",
      "Epoch 2 Step 995 Loss 1.1616\n",
      "Epoch 2 Step 996 Loss 1.4395\n",
      "Epoch 2 Step 997 Loss 1.2829\n",
      "Epoch 2 Step 998 Loss 1.1995\n",
      "Epoch 2 Step 999 Loss 1.6773\n",
      "Epoch 2 Step 1000 Loss 1.6066\n",
      "Epoch 2 Step 1001 Loss 1.5504\n",
      "Epoch 2 Step 1002 Loss 1.5154\n",
      "Epoch 2 Step 1003 Loss 1.7136\n",
      "Epoch 2 Step 1004 Loss 1.6195\n",
      "Epoch 2 Step 1005 Loss 1.5482\n",
      "Epoch 2 Step 1006 Loss 1.3078\n",
      "Epoch 2 Step 1007 Loss 1.3443\n",
      "Epoch 2 Step 1008 Loss 1.2405\n",
      "Epoch 2 Step 1009 Loss 1.5897\n",
      "Epoch 2 Step 1010 Loss 1.3352\n",
      "Epoch 2 Step 1011 Loss 1.4343\n",
      "Epoch 2 Step 1012 Loss 1.2452\n",
      "Epoch 2 Step 1013 Loss 1.1403\n",
      "Epoch 2 Step 1014 Loss 1.4176\n",
      "Epoch 2 Step 1015 Loss 1.4091\n",
      "Epoch 2 Step 1016 Loss 1.2806\n",
      "Epoch 2 Step 1017 Loss 1.2603\n",
      "Epoch 2 Step 1018 Loss 1.3258\n",
      "Epoch 2 Step 1019 Loss 1.5203\n",
      "Epoch 2 Step 1020 Loss 1.3811\n",
      "Epoch 2 Step 1021 Loss 1.5058\n",
      "Epoch 2 Step 1022 Loss 1.4723\n",
      "Epoch 2 Step 1023 Loss 1.4883\n",
      "Epoch 2 Step 1024 Loss 1.0598\n",
      "Epoch 2 Step 1025 Loss 1.5005\n",
      "Epoch 2 Step 1026 Loss 1.3729\n",
      "Epoch 2 Step 1027 Loss 1.4851\n",
      "Epoch 2 Step 1028 Loss 1.4435\n",
      "Epoch 2 Step 1029 Loss 1.3369\n",
      "Epoch 2 Step 1030 Loss 1.6253\n",
      "Epoch 2 Step 1031 Loss 1.5730\n",
      "Epoch 2 Step 1032 Loss 1.4542\n",
      "Epoch 2 Step 1033 Loss 1.6299\n",
      "Epoch 2 Step 1034 Loss 1.4989\n",
      "Epoch 2 Step 1035 Loss 1.3699\n",
      "Epoch 2 Step 1036 Loss 1.3681\n",
      "Epoch 2 Step 1037 Loss 1.7585\n",
      "Epoch 2 Step 1038 Loss 1.5012\n",
      "Epoch 2 Step 1039 Loss 1.5791\n",
      "Epoch 2 Step 1040 Loss 1.4764\n",
      "Epoch 2 Step 1041 Loss 1.2894\n",
      "Epoch 2 Step 1042 Loss 1.2248\n",
      "Epoch 2 Step 1043 Loss 1.4014\n",
      "Epoch 2 Step 1044 Loss 1.3626\n",
      "Epoch 2 Step 1045 Loss 1.4909\n",
      "Epoch 2 Step 1046 Loss 1.8815\n",
      "Epoch 2 Step 1047 Loss 1.6465\n",
      "Epoch 2 Step 1048 Loss 1.5688\n",
      "Epoch 2 Step 1049 Loss 1.6873\n",
      "Epoch 2 Step 1050 Loss 1.4429\n",
      "Epoch 2 Step 1051 Loss 1.3831\n",
      "Epoch 2 Step 1052 Loss 1.7192\n",
      "Epoch 2 Step 1053 Loss 1.6114\n",
      "Epoch 2 Step 1054 Loss 1.5028\n",
      "Epoch 2 Step 1055 Loss 1.1283\n",
      "Epoch 2 Step 1056 Loss 1.3109\n",
      "Epoch 2 Step 1057 Loss 1.5601\n",
      "Epoch 2 Step 1058 Loss 1.3727\n",
      "Epoch 2 Step 1059 Loss 1.2818\n",
      "Epoch 2 Step 1060 Loss 1.5173\n",
      "Epoch 2 Step 1061 Loss 1.6036\n",
      "Epoch 2 Step 1062 Loss 2.2929\n",
      "Epoch 2 Step 1063 Loss 1.8000\n",
      "Epoch 2 Step 1064 Loss 1.7978\n",
      "Epoch 2 Step 1065 Loss 1.6336\n",
      "Epoch 2 Step 1066 Loss 1.3522\n",
      "Epoch 2 Step 1067 Loss 1.1955\n",
      "Epoch 2 Step 1068 Loss 1.4653\n",
      "Epoch 2 Step 1069 Loss 1.9160\n",
      "Epoch 2 Step 1070 Loss 1.9369\n",
      "Epoch 2 Step 1071 Loss 1.5588\n",
      "Epoch 2 Step 1072 Loss 1.5605\n",
      "Epoch 2 Step 1073 Loss 1.3636\n",
      "Epoch 2 Step 1074 Loss 1.6893\n",
      "Epoch 2 Step 1075 Loss 1.5853\n",
      "Epoch 2 Step 1076 Loss 2.0102\n",
      "Epoch 2 Step 1077 Loss 1.9982\n",
      "Epoch 2 Step 1078 Loss 1.7327\n",
      "Epoch 2 Step 1079 Loss 1.3767\n",
      "Epoch 2 Step 1080 Loss 1.3118\n",
      "Epoch 2 Step 1081 Loss 1.6152\n",
      "Epoch 2 Step 1082 Loss 1.5433\n",
      "Epoch 2 Step 1083 Loss 1.3605\n",
      "Epoch 2 Step 1084 Loss 1.3736\n",
      "Epoch 2 Step 1085 Loss 1.3223\n",
      "Epoch 2 Step 1086 Loss 1.1397\n",
      "Epoch 2 Step 1087 Loss 1.3961\n",
      "Epoch 2 Step 1088 Loss 1.5994\n",
      "Epoch 2 Step 1089 Loss 1.6040\n",
      "Epoch 2 Step 1090 Loss 1.7427\n",
      "Epoch 2 Step 1091 Loss 1.6308\n",
      "Epoch 2 Step 1092 Loss 1.3347\n",
      "Epoch 2 Step 1093 Loss 1.2287\n",
      "Epoch 2 Step 1094 Loss 1.7092\n",
      "Epoch 2 Step 1095 Loss 1.5598\n",
      "Epoch 2 Step 1096 Loss 1.3476\n",
      "Epoch 2 Step 1097 Loss 1.0325\n",
      "Epoch 2 Step 1098 Loss 1.0865\n",
      "Epoch 2 Step 1099 Loss 1.2198\n",
      "Epoch 2 Step 1100 Loss 1.3205\n",
      "Epoch 2 Step 1101 Loss 1.2328\n",
      "Epoch 2 Step 1102 Loss 1.4463\n",
      "Epoch 2 Step 1103 Loss 1.5514\n",
      "Epoch 2 Step 1104 Loss 1.7450\n",
      "Epoch 2 Step 1105 Loss 1.7905\n",
      "Epoch 2 Step 1106 Loss 1.5111\n",
      "Epoch 2 Step 1107 Loss 1.5349\n",
      "Epoch 2 Step 1108 Loss 1.4483\n",
      "Epoch 2 Step 1109 Loss 1.3739\n",
      "Epoch 2 Step 1110 Loss 0.9800\n",
      "Epoch 2 Step 1111 Loss 1.4071\n",
      "Epoch 2 Step 1112 Loss 1.2265\n",
      "Epoch 2 Step 1113 Loss 1.4250\n",
      "Epoch 2 Step 1114 Loss 1.2533\n",
      "Epoch 2 Step 1115 Loss 1.4478\n",
      "Epoch 2 Step 1116 Loss 1.3481\n",
      "Epoch 2 Step 1117 Loss 1.6262\n",
      "Epoch 2 Step 1118 Loss 1.7492\n",
      "Epoch 2 Step 1119 Loss 1.3591\n",
      "Epoch 2 Step 1120 Loss 1.3228\n",
      "Epoch 2 Step 1121 Loss 1.2406\n",
      "Epoch 2 Step 1122 Loss 1.0331\n",
      "Epoch 2 Step 1123 Loss 1.3421\n",
      "Epoch 2 Step 1124 Loss 1.2263\n",
      "Epoch 2 Step 1125 Loss 1.1067\n",
      "Epoch 2 Step 1126 Loss 1.3777\n",
      "Epoch 2 Step 1127 Loss 1.2747\n",
      "Epoch 2 Step 1128 Loss 1.2508\n",
      "Epoch 2 Step 1129 Loss 1.3971\n",
      "Epoch 2 Step 1130 Loss 1.2353\n",
      "Epoch 2 Step 1131 Loss 1.2176\n",
      "Epoch 2 Step 1132 Loss 1.0716\n",
      "Epoch 2 Step 1133 Loss 1.3772\n",
      "Epoch 2 Step 1134 Loss 1.1741\n",
      "Epoch 2 Step 1135 Loss 1.3283\n",
      "Epoch 2 Step 1136 Loss 1.5219\n",
      "Epoch 2 Step 1137 Loss 1.4167\n",
      "Epoch 2 Step 1138 Loss 1.2918\n",
      "Epoch 2 Step 1139 Loss 1.0677\n",
      "Epoch 2 Step 1140 Loss 1.2006\n",
      "Epoch 2 Step 1141 Loss 1.1870\n",
      "Epoch 2 Step 1142 Loss 1.0456\n",
      "Epoch 2 Step 1143 Loss 1.1166\n",
      "Epoch 2 Step 1144 Loss 1.5393\n",
      "Epoch 2 Step 1145 Loss 1.3266\n",
      "Epoch 2 Step 1146 Loss 1.4491\n",
      "Epoch 2 Step 1147 Loss 1.4135\n",
      "Epoch 2 Step 1148 Loss 1.3578\n",
      "Epoch 2 Step 1149 Loss 1.2104\n",
      "Epoch 2 Step 1150 Loss 1.2689\n",
      "Epoch 2 Step 1151 Loss 1.6889\n",
      "Epoch 2 Step 1152 Loss 1.6136\n",
      "Epoch 2 Step 1153 Loss 1.1377\n",
      "Epoch 2 Step 1154 Loss 1.2397\n",
      "Epoch 2 Step 1155 Loss 1.2844\n",
      "Epoch 2 Step 1156 Loss 1.4100\n",
      "Epoch 2 Step 1157 Loss 1.4994\n",
      "Epoch 2 Step 1158 Loss 1.6126\n",
      "Epoch 2 Step 1159 Loss 1.3138\n",
      "Epoch 2 Step 1160 Loss 1.3461\n",
      "Epoch 2 Step 1161 Loss 1.1819\n",
      "Epoch 2 Step 1162 Loss 1.3155\n",
      "Epoch 2 Step 1163 Loss 1.0745\n",
      "Epoch 2 Step 1164 Loss 0.8932\n",
      "Epoch 2 Step 1165 Loss 0.9618\n",
      "Epoch 2 Step 1166 Loss 1.0239\n",
      "Epoch 2 Step 1167 Loss 1.1518\n",
      "Epoch 2 Step 1168 Loss 1.2844\n",
      "Epoch 2 Step 1169 Loss 1.1750\n",
      "Epoch 2 Step 1170 Loss 1.3150\n",
      "Epoch 2 Step 1171 Loss 1.3350\n",
      "Epoch 2 Step 1172 Loss 1.3079\n",
      "Epoch 2 Step 1173 Loss 1.2409\n",
      "Epoch 2 Step 1174 Loss 1.3325\n",
      "Epoch 2 Step 1175 Loss 1.3365\n",
      "Epoch 2 Step 1176 Loss 1.1262\n",
      "Epoch 2 Step 1177 Loss 0.9997\n",
      "Epoch 2 Step 1178 Loss 1.0503\n",
      "Epoch 2 Step 1179 Loss 1.1895\n",
      "Epoch 2 Step 1180 Loss 1.3624\n",
      "Epoch 2 Step 1181 Loss 1.3401\n",
      "Epoch 2 Step 1182 Loss 1.1380\n",
      "Epoch 2 Step 1183 Loss 1.0413\n",
      "Epoch 2 Step 1184 Loss 1.1818\n",
      "Epoch 2 Step 1185 Loss 1.0699\n",
      "Epoch 2 Step 1186 Loss 1.1211\n",
      "Epoch 2 Step 1187 Loss 1.1127\n",
      "Epoch 2 Step 1188 Loss 1.2836\n",
      "Epoch 2 Step 1189 Loss 1.1290\n",
      "Epoch 2 Step 1190 Loss 1.2879\n",
      "Epoch 2 Step 1191 Loss 1.4535\n",
      "Epoch 2 Step 1192 Loss 1.5043\n",
      "Epoch 2 Step 1193 Loss 1.5168\n",
      "Epoch 2 Step 1194 Loss 1.3305\n",
      "Epoch 2 Step 1195 Loss 1.1814\n",
      "Epoch 2 Step 1196 Loss 1.0481\n",
      "Epoch 2 Step 1197 Loss 1.3283\n",
      "Epoch 2 Step 1198 Loss 1.0675\n",
      "Epoch 2 Step 1199 Loss 0.9707\n",
      "Epoch 2 Step 1200 Loss 1.3130\n",
      "Epoch 2 Step 1201 Loss 1.2997\n",
      "Epoch 2 Step 1202 Loss 1.3899\n",
      "Epoch 2 Step 1203 Loss 1.4671\n",
      "Epoch 2 Step 1204 Loss 1.3817\n",
      "Epoch 2 Step 1205 Loss 1.1168\n",
      "Epoch 2 Step 1206 Loss 1.1831\n",
      "Epoch 2 Step 1207 Loss 1.4240\n",
      "Epoch 2 Step 1208 Loss 0.9226\n",
      "Epoch 2 Step 1209 Loss 1.0329\n",
      "Epoch 2 Step 1210 Loss 0.9804\n",
      "Epoch 2 Step 1211 Loss 1.2285\n",
      "Epoch 2 Step 1212 Loss 1.5106\n",
      "Epoch 2 Step 1213 Loss 1.1600\n",
      "Epoch 2 Step 1214 Loss 0.9929\n",
      "Epoch 2 Step 1215 Loss 1.1375\n",
      "Epoch 2 Step 1216 Loss 1.0881\n",
      "Epoch 2 Step 1217 Loss 1.0773\n",
      "Epoch 2 Step 1218 Loss 1.1335\n",
      "Epoch 2 Step 1219 Loss 1.0825\n",
      "Epoch 2 Step 1220 Loss 0.8459\n",
      "Epoch 2 Step 1221 Loss 1.1574\n",
      "Epoch 2 Step 1222 Loss 1.3313\n",
      "Epoch 2 Step 1223 Loss 1.1539\n",
      "Epoch 2 Step 1224 Loss 1.1115\n",
      "Epoch 2 Step 1225 Loss 1.7804\n",
      "Epoch 2 Step 1226 Loss 1.2994\n",
      "Epoch 2 Step 1227 Loss 1.5012\n",
      "Epoch 2 Step 1228 Loss 1.3484\n",
      "Epoch 2 Step 1229 Loss 1.1949\n",
      "Epoch 2 Step 1230 Loss 1.0447\n",
      "Epoch 2 Step 1231 Loss 1.1165\n",
      "Epoch 2 Step 1232 Loss 1.1072\n",
      "Epoch 2 Step 1233 Loss 1.2401\n",
      "Epoch 2 Step 1234 Loss 1.4207\n",
      "Epoch 2 Step 1235 Loss 1.5157\n",
      "Epoch 2 Step 1236 Loss 1.0885\n",
      "Epoch 2 Step 1237 Loss 0.9344\n",
      "Epoch 2 Step 1238 Loss 0.9145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Step 1239 Loss 0.9701\n",
      "Epoch 2 Step 1240 Loss 1.2955\n",
      "Epoch 2 Step 1241 Loss 1.1022\n",
      "Epoch 2 Step 1242 Loss 1.0581\n",
      "Epoch 2 Step 1243 Loss 1.5035\n",
      "Epoch 2 Step 1244 Loss 1.4444\n",
      "Epoch 2 Step 1245 Loss 1.0435\n",
      "Epoch 2 Step 1246 Loss 1.2159\n",
      "Epoch 2 Step 1247 Loss 1.3651\n",
      "Epoch 2 Step 1248 Loss 1.2752\n",
      "Epoch 2 Step 1249 Loss 1.0896\n",
      "Epoch 2 Step 1250 Loss 1.0371\n",
      "Epoch 2 Step 1251 Loss 1.1840\n",
      "Epoch 2 Step 1252 Loss 1.1478\n",
      "Epoch 2 Step 1253 Loss 1.1368\n",
      "Epoch 2 Step 1254 Loss 1.2614\n",
      "Epoch 2 Step 1255 Loss 1.3705\n",
      "Epoch 2 Step 1256 Loss 1.0719\n",
      "Epoch 2 Step 1257 Loss 1.1358\n",
      "Epoch 2 Step 1258 Loss 1.1116\n",
      "Epoch 2 Step 1259 Loss 1.3290\n",
      "Epoch 2 Step 1260 Loss 1.4149\n",
      "Epoch 2 Step 1261 Loss 1.3824\n",
      "Epoch 2 Step 1262 Loss 1.6379\n",
      "Epoch 2 Step 1263 Loss 1.5932\n",
      "Epoch 2 Step 1264 Loss 1.3232\n",
      "Epoch 2 Step 1265 Loss 1.2185\n",
      "Epoch 2 Step 1266 Loss 1.3543\n",
      "Epoch 2 Step 1267 Loss 1.1244\n",
      "Epoch 2 Step 1268 Loss 1.1191\n",
      "Epoch 2 Step 1269 Loss 1.0153\n",
      "Epoch 2 Step 1270 Loss 1.1007\n",
      "Epoch 2 Step 1271 Loss 1.1421\n",
      "Epoch 2 Step 1272 Loss 1.3020\n",
      "Epoch 2 Step 1273 Loss 1.2154\n",
      "Epoch 2 Step 1274 Loss 1.7266\n",
      "Epoch 2 Step 1275 Loss 1.2836\n",
      "Epoch 2 Step 1276 Loss 1.2534\n",
      "Epoch 2 Step 1277 Loss 1.2644\n",
      "Epoch 2 Step 1278 Loss 1.2203\n",
      "Epoch 2 Step 1279 Loss 0.9758\n",
      "Epoch 2 Step 1280 Loss 0.7965\n",
      "Epoch 2 Step 1281 Loss 1.0831\n",
      "Epoch 2 Step 1282 Loss 1.1597\n",
      "Epoch 2 Step 1283 Loss 1.2126\n",
      "Epoch 2 Step 1284 Loss 1.2912\n",
      "Epoch 2 Step 1285 Loss 1.3404\n",
      "Epoch 2 Step 1286 Loss 1.2289\n",
      "Epoch 2 Step 1287 Loss 1.1570\n",
      "Epoch 2 Step 1288 Loss 1.0414\n",
      "Epoch 2 Step 1289 Loss 1.3486\n",
      "Epoch 2 Step 1290 Loss 1.3383\n",
      "Epoch 2 Step 1291 Loss 1.3598\n",
      "Epoch 2 Step 1292 Loss 1.0514\n",
      "Epoch 2 Step 1293 Loss 1.1333\n",
      "Saving checkpoint for epoch 2 at ./../data/checkpoints/beam_search_training_checkpoints_mask_loss_dim300_seq/ckpt\n",
      "Epoch 2 Loss 1.3685\n",
      "Time taken for 1 epoch 1172.891119480133 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = params[\"epochs\"]\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)):\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    print ('Latest checkpoint restored!!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            print('Epoch {} Step {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             checkpoint_prefix))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypothesis:\n",
    "    \"\"\" Class designed to hold hypothesises throughout the beamSearch decoding \"\"\"\n",
    "\n",
    "    def __init__(self, tokens, log_probs, hidden, attn_dists):\n",
    "        self.tokens = tokens  # list of all the tokens from time 0 to the current time step t\n",
    "        self.log_probs = log_probs  # list of the log probabilities of the tokens of the tokens\n",
    "        self.hidden = hidden  # decoder hidden state after the last token decoding\n",
    "        self.attn_dists = attn_dists  # attention dists of all the tokens\n",
    "        self.abstract = \"\"\n",
    "\n",
    "    def extend(self, token, log_prob, hidden, attn_dist):\n",
    "        \"\"\"Method to extend the current hypothesis by adding the next decoded token and all the informations associated with it\"\"\"\n",
    "        return Hypothesis(tokens=self.tokens + [token],  # we add the decoded token\n",
    "                          log_probs=self.log_probs + [log_prob],  # we add the log prob of the decoded token\n",
    "                          hidden=hidden,  # we update the state\n",
    "                          attn_dists=self.attn_dists + [attn_dist])\n",
    "\n",
    "    @property\n",
    "    def latest_token(self):\n",
    "        return self.tokens[-1]\n",
    "\n",
    "    @property\n",
    "    def tot_log_prob(self):\n",
    "        return sum(self.log_probs)\n",
    "\n",
    "    @property\n",
    "    def avg_log_prob(self):\n",
    "        return self.tot_log_prob / len(self.tokens)\n",
    "\n",
    "\n",
    "def beam_decode(model, batch, vocab, params):\n",
    "    # 初始化mask\n",
    "    start_index = vocab.word_to_id(vocab.START_DECODING)\n",
    "    stop_index = vocab.word_to_id(vocab.STOP_DECODING)\n",
    "    unk_index = vocab.word_to_id(vocab.UNKNOWN_TOKEN)\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    # 单步decoder\n",
    "    def decoder_onestep(enc_output, dec_input, dec_hidden, enc_extended_inp, batch_oov_len):\n",
    "        # 单个时间步 运行\n",
    "        # dec_input, dec_hidden, enc_output, enc_extended_inp, batch_oov_len\n",
    "        final_preds, dec_hidden, context_vector, attention_weights, p_gens = model.call_decoder_onestep(dec_input,\n",
    "                                                                                                        dec_hidden,\n",
    "                                                                                                        enc_output,\n",
    "                                                                                                        enc_extended_inp,\n",
    "                                                                                                        batch_oov_len)\n",
    "        # 拿到top k个index 和 概率\n",
    "        top_k_probs, top_k_ids = tf.nn.top_k(tf.squeeze(final_preds), k=params[\"beam_size\"] * 2)\n",
    "        # 计算log概率\n",
    "        top_k_log_probs = tf.math.log(top_k_probs)\n",
    "\n",
    "        results = {\n",
    "            # 'final_dists': preds,\n",
    "            \"last_context_vector\": context_vector,\n",
    "            \"dec_hidden\": dec_hidden,\n",
    "            \"attention_weights\": attention_weights,\n",
    "            \"top_k_ids\": top_k_ids,\n",
    "            \"top_k_log_probs\": top_k_log_probs,\n",
    "            \"p_gen\": p_gens}\n",
    "\n",
    "        # 返回需要保存的中间结果和概率\n",
    "        return results\n",
    "\n",
    "    # 测试数据的输入\n",
    "    enc_input = batch[0][\"enc_input\"]\n",
    "\n",
    "    # 计算第encoder的输出\n",
    "    enc_output, enc_hidden = model.call_encoder(enc_input)\n",
    "\n",
    "    # 初始化batch size个 假设对象\n",
    "    hyps = [Hypothesis(tokens=[start_index],\n",
    "                       log_probs=[0.0],\n",
    "                       hidden=enc_hidden[0],\n",
    "                       attn_dists=[],\n",
    "                       ) for _ in range(batch_size)]\n",
    "    # 初始化结果集\n",
    "    results = []  # list to hold the top beam_size hypothesises\n",
    "    # 遍历步数\n",
    "    steps = 0  # initial step\n",
    "\n",
    "    enc_extended_inp = batch[0][\"extended_enc_input\"]\n",
    "    batch_oov_len = batch[0][\"max_oov_len\"]\n",
    "\n",
    "    # 长度还不够 并且 结果还不够 继续搜索\n",
    "    while steps < params['max_dec_len'] and len(results) < params['beam_size']:\n",
    "        # 获取最新待使用的token\n",
    "        latest_tokens = [h.latest_token for h in hyps]\n",
    "        # 替换掉 oov token unknown token\n",
    "        latest_tokens = [t if t in vocab.id2word else unk_index for t in latest_tokens]\n",
    "        # 获取所以隐藏层状态\n",
    "        hiddens = [h.hidden for h in hyps]\n",
    "\n",
    "        dec_input = tf.expand_dims(latest_tokens, axis=1)\n",
    "        dec_hidden = tf.stack(hiddens, axis=0)\n",
    "\n",
    "        # 单步运行decoder 计算需要的值\n",
    "        decoder_results = decoder_onestep(enc_output,\n",
    "                                          dec_input,\n",
    "                                          dec_hidden,\n",
    "                                          enc_extended_inp,\n",
    "                                          batch_oov_len)\n",
    "\n",
    "        # preds = decoder_results['final_dists']\n",
    "        # context_vector = decoder_results['last_context_vector']\n",
    "\n",
    "        dec_hidden = decoder_results['dec_hidden']\n",
    "        attention_weights = decoder_results['attention_weights']\n",
    "        top_k_log_probs = decoder_results['top_k_log_probs']\n",
    "        top_k_ids = decoder_results['top_k_ids']\n",
    "\n",
    "        # print('top_k_ids {}'.format(top_k_ids))\n",
    "\n",
    "        # 现阶段全部可能情况\n",
    "        all_hyps = []\n",
    "        # 原有的可能情况数量\n",
    "        num_orig_hyps = 1 if steps == 0 else len(hyps)\n",
    "\n",
    "        # 遍历添加所有可能结果\n",
    "        for i in range(num_orig_hyps):\n",
    "            h, new_hidden, attn_dist = hyps[i], dec_hidden[i], attention_weights[i]\n",
    "            # 分裂 添加 beam size 种可能性\n",
    "            for j in range(params['beam_size'] * 2):\n",
    "                # 构造可能的情况\n",
    "                new_hyp = h.extend(token=top_k_ids[i, j].numpy(),\n",
    "                                   log_prob=top_k_log_probs[i, j],\n",
    "                                   hidden=new_hidden,\n",
    "                                   attn_dist=attn_dist)\n",
    "                # 添加可能情况\n",
    "                all_hyps.append(new_hyp)\n",
    "\n",
    "        # 重置\n",
    "        hyps = []\n",
    "        # 按照概率来排序\n",
    "        sorted_hyps = sorted(all_hyps, key=lambda h: h.avg_log_prob, reverse=True)\n",
    "\n",
    "        # 筛选top前beam_size句话\n",
    "        for h in sorted_hyps:\n",
    "            if h.latest_token == stop_index:\n",
    "                # 长度符合预期,遇到句尾,添加到结果集\n",
    "                if steps >= params['min_dec_steps']:\n",
    "                    results.append(h)\n",
    "            else:\n",
    "                # 未到结束 ,添加到假设集\n",
    "                hyps.append(h)\n",
    "\n",
    "            # 如果假设句子正好等于beam_size 或者结果集正好等于beam_size 就不在添加\n",
    "            if len(hyps) == params['beam_size'] or len(results) == params['beam_size']:\n",
    "                break\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    if len(results) == 0:\n",
    "        results = hyps\n",
    "\n",
    "    hyps_sorted = sorted(results, key=lambda h: h.avg_log_prob, reverse=True)\n",
    "    best_hyp = hyps_sorted[0]\n",
    "    best_hyp.abstract = \" \".join([vocab.id_to_word(index) for index in best_hyp.tokens])\n",
    "    best_hyp.text = batch[0][\"article\"].numpy()[0].decode()\n",
    "    \n",
    "    return best_hyp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
