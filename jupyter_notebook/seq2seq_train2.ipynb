{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append(\"/home/lcz/lenlp/text-generation/py_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root path: /home/lcz/lenlp/text-generation\n"
     ]
    }
   ],
   "source": [
    "from utils.data_loader import load_dataset\n",
    "from utils.data_loader import load_test_dataset\n",
    "from utils.linux_config import embedding_matrix_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-24 08:46:09,463 : INFO : loading Word2Vec object from ./../data/word2vec.model\n",
      "2020-05-24 08:46:11,903 : INFO : loading wv recursively from ./../data/word2vec.model.wv.* with mmap=None\n",
      "2020-05-24 08:46:11,908 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-05-24 08:46:11,913 : INFO : loading vocabulary recursively from ./../data/word2vec.model.vocabulary.* with mmap=None\n",
      "2020-05-24 08:46:11,914 : INFO : loading trainables recursively from ./../data/word2vec.model.trainables.* with mmap=None\n",
      "2020-05-24 08:46:11,915 : INFO : setting ignored attribute cum_table to None\n",
      "2020-05-24 08:46:11,917 : INFO : loaded ./../data/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "from utils.wv_loader import get_embedding_matrix\n",
    "wv_model_path = \"./../data/word2vec.model\"\n",
    "embedding_matrix = get_embedding_matrix(wv_model_path)\n",
    "import numpy as np\n",
    "np.save(\"./../data/embedding_matrix.npy\", embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.78204274e-01,  6.03120551e-02,  3.60607475e-01, ...,\n",
       "        -3.83604765e-01, -2.35126719e-01, -3.39127541e-01],\n",
       "       [ 2.48232469e-01, -1.09330297e-01, -1.07853085e-01, ...,\n",
       "        -4.05686855e-01, -3.84583443e-01,  8.72177184e-01],\n",
       "       [-2.61794060e-01,  1.58479378e-01, -3.25242400e-01, ...,\n",
       "        -1.04319692e+00, -3.70189011e-01, -6.24906011e-02],\n",
       "       ...,\n",
       "       [ 3.13954473e-01,  6.27483010e-01,  8.70058371e-04, ...,\n",
       "        -6.59883201e-01,  1.04633875e-01,  5.38641512e-02],\n",
       "       [-3.03027540e-01, -6.11446559e-01, -1.08515084e+00, ...,\n",
       "         1.52157664e-01,  3.57411057e-01, -8.81655991e-01],\n",
       "       [ 3.11857313e-01, -5.35719246e-02, -3.20000172e-01, ...,\n",
       "        -1.04822442e-01,  1.92798018e-01, -4.42412287e-01]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.wv_loader import load_embedding_matrix, load_vocab\n",
    "embedding_matrix2 = load_embedding_matrix(\"./../data/embedding_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.78204274e-01,  6.03120551e-02,  3.60607475e-01, ...,\n",
       "        -3.83604765e-01, -2.35126719e-01, -3.39127541e-01],\n",
       "       [ 2.48232469e-01, -1.09330297e-01, -1.07853085e-01, ...,\n",
       "        -4.05686855e-01, -3.84583443e-01,  8.72177184e-01],\n",
       "       [-2.61794060e-01,  1.58479378e-01, -3.25242400e-01, ...,\n",
       "        -1.04319692e+00, -3.70189011e-01, -6.24906011e-02],\n",
       "       ...,\n",
       "       [ 3.13954473e-01,  6.27483010e-01,  8.70058371e-04, ...,\n",
       "        -6.59883201e-01,  1.04633875e-01,  5.38641512e-02],\n",
       "       [-3.03027540e-01, -6.11446559e-01, -1.08515084e+00, ...,\n",
       "         1.52157664e-01,  3.57411057e-01, -8.81655991e-01],\n",
       "       [ 3.11857313e-01, -5.35719246e-02, -3.20000172e-01, ...,\n",
       "        -1.04822442e-01,  1.92798018e-01, -4.42412287e-01]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = \"./../data/vocab.txt\"\n",
    "vocab, reverse_vocab = load_vocab(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32909"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 1024\n",
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"embed_size\"] = 300\n",
    "params[\"enc_units\"] = units\n",
    "params[\"attn_units\"] = units\n",
    "params[\"dec_units\"] = units\n",
    "params[\"batch_size\"] = 64\n",
    "params[\"epochs\"] = 2\n",
    "params[\"max_enc_len\"] = 200\n",
    "params[\"max_dec_len\"] = 41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "steps_per_epoch = len(train_X) // params[\"batch_size\"]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_Y)).shuffle(params[\"batch_size\"])\n",
    "dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq.layers import Encoder, BahdanauAttention, Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 200, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1024])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(params[\"vocab_size\"], params[\"embed_size\"], embedding_matrix, params[\"enc_units\"], params[\"batch_size\"])\n",
    "enc_hidden = encoder.initialize_hidden_state()\n",
    "example_input_batch = tf.ones(shape=(params[\"batch_size\"], params[\"max_enc_len\"]), dtype=tf.int32)\n",
    "sample_output, sample_hidden = encoder(example_input_batch, enc_hidden)\n",
    "sample_output.shape\n",
    "sample_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 32909])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(params[\"vocab_size\"], params[\"embed_size\"], embedding_matrix, params[\"enc_units\"], params[\"batch_size\"])\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)), sample_hidden, sample_output)\n",
    "sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存点设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./../data/checkpoints/training_checkpoints_mask_loss_dim300_seq\"\n",
    "import os\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "nuk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    nuk_mask = tf.math.equal(real, nuk_index)\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,nuk_mask))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 构建encoder\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        # 2. 复制\n",
    "        dec_hidden = enc_hidden\n",
    "        # 3. <START> * BATCH_SIZE \n",
    "        dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "        \n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # decoder(x, hidden, enc_output)\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n",
      "Epoch 1 Step 0 Loss 2.2436\n",
      "Epoch 1 Step 1 Loss 2.4755\n",
      "Epoch 1 Step 2 Loss 2.3152\n",
      "Epoch 1 Step 3 Loss 2.0714\n",
      "Epoch 1 Step 4 Loss 2.0473\n",
      "Epoch 1 Step 5 Loss 1.9664\n",
      "Epoch 1 Step 6 Loss 1.3815\n",
      "Epoch 1 Step 7 Loss 1.5031\n",
      "Epoch 1 Step 8 Loss 1.1857\n",
      "Epoch 1 Step 9 Loss 1.8654\n",
      "Epoch 1 Step 10 Loss 1.8284\n",
      "Epoch 1 Step 11 Loss 1.7517\n",
      "Epoch 1 Step 12 Loss 1.5243\n",
      "Epoch 1 Step 13 Loss 1.8855\n",
      "Epoch 1 Step 14 Loss 1.7766\n",
      "Epoch 1 Step 15 Loss 1.9253\n",
      "Epoch 1 Step 16 Loss 1.5869\n",
      "Epoch 1 Step 17 Loss 1.3390\n",
      "Epoch 1 Step 18 Loss 0.9561\n",
      "Epoch 1 Step 19 Loss 1.0416\n",
      "Epoch 1 Step 20 Loss 1.2915\n",
      "Epoch 1 Step 21 Loss 1.3680\n",
      "Epoch 1 Step 22 Loss 1.4378\n",
      "Epoch 1 Step 23 Loss 2.0461\n",
      "Epoch 1 Step 24 Loss 1.4714\n",
      "Epoch 1 Step 25 Loss 1.5752\n",
      "Epoch 1 Step 26 Loss 1.5738\n",
      "Epoch 1 Step 27 Loss 2.0820\n",
      "Epoch 1 Step 28 Loss 2.2571\n",
      "Epoch 1 Step 29 Loss 1.9164\n",
      "Epoch 1 Step 30 Loss 1.8014\n",
      "Epoch 1 Step 31 Loss 1.7590\n",
      "Epoch 1 Step 32 Loss 1.7748\n",
      "Epoch 1 Step 33 Loss 1.3229\n",
      "Epoch 1 Step 34 Loss 1.5937\n",
      "Epoch 1 Step 35 Loss 1.9929\n",
      "Epoch 1 Step 36 Loss 1.8127\n",
      "Epoch 1 Step 37 Loss 2.2323\n",
      "Epoch 1 Step 38 Loss 1.8956\n",
      "Epoch 1 Step 39 Loss 1.7745\n",
      "Epoch 1 Step 40 Loss 2.0534\n",
      "Epoch 1 Step 41 Loss 1.3067\n",
      "Epoch 1 Step 42 Loss 2.0065\n",
      "Epoch 1 Step 43 Loss 1.8131\n",
      "Epoch 1 Step 44 Loss 1.7465\n",
      "Epoch 1 Step 45 Loss 1.8230\n",
      "Epoch 1 Step 46 Loss 1.5675\n",
      "Epoch 1 Step 47 Loss 1.1474\n",
      "Epoch 1 Step 48 Loss 1.6774\n",
      "Epoch 1 Step 49 Loss 2.0385\n",
      "Epoch 1 Step 50 Loss 2.4594\n",
      "Epoch 1 Step 51 Loss 2.0399\n",
      "Epoch 1 Step 52 Loss 1.7316\n",
      "Epoch 1 Step 53 Loss 1.8048\n",
      "Epoch 1 Step 54 Loss 1.9527\n",
      "Epoch 1 Step 55 Loss 1.1612\n",
      "Epoch 1 Step 56 Loss 1.7927\n",
      "Epoch 1 Step 57 Loss 1.8358\n",
      "Epoch 1 Step 58 Loss 1.7526\n",
      "Epoch 1 Step 59 Loss 1.6303\n",
      "Epoch 1 Step 60 Loss 1.7120\n",
      "Epoch 1 Step 61 Loss 1.4181\n",
      "Epoch 1 Step 62 Loss 1.6016\n",
      "Epoch 1 Step 63 Loss 1.4697\n",
      "Epoch 1 Step 64 Loss 1.3834\n",
      "Epoch 1 Step 65 Loss 1.7793\n",
      "Epoch 1 Step 66 Loss 1.5788\n",
      "Epoch 1 Step 67 Loss 1.8368\n",
      "Epoch 1 Step 68 Loss 1.7753\n",
      "Epoch 1 Step 69 Loss 1.8540\n",
      "Epoch 1 Step 70 Loss 1.5301\n",
      "Epoch 1 Step 71 Loss 1.4996\n",
      "Epoch 1 Step 72 Loss 1.7870\n",
      "Epoch 1 Step 73 Loss 2.1584\n",
      "Epoch 1 Step 74 Loss 1.8622\n",
      "Epoch 1 Step 75 Loss 2.0464\n",
      "Epoch 1 Step 76 Loss 1.5906\n",
      "Epoch 1 Step 77 Loss 1.6385\n",
      "Epoch 1 Step 78 Loss 2.1946\n",
      "Epoch 1 Step 79 Loss 1.9637\n",
      "Epoch 1 Step 80 Loss 1.8613\n",
      "Epoch 1 Step 81 Loss 1.3410\n",
      "Epoch 1 Step 82 Loss 1.5553\n",
      "Epoch 1 Step 83 Loss 1.7825\n",
      "Epoch 1 Step 84 Loss 1.6919\n",
      "Epoch 1 Step 85 Loss 1.6920\n",
      "Epoch 1 Step 86 Loss 1.5571\n",
      "Epoch 1 Step 87 Loss 1.9009\n",
      "Epoch 1 Step 88 Loss 1.7073\n",
      "Epoch 1 Step 89 Loss 1.2226\n",
      "Epoch 1 Step 90 Loss 1.5769\n",
      "Epoch 1 Step 91 Loss 1.2366\n",
      "Epoch 1 Step 92 Loss 1.2011\n",
      "Epoch 1 Step 93 Loss 1.2039\n",
      "Epoch 1 Step 94 Loss 1.4647\n",
      "Epoch 1 Step 95 Loss 1.7845\n",
      "Epoch 1 Step 96 Loss 1.8777\n",
      "Epoch 1 Step 97 Loss 1.9228\n",
      "Epoch 1 Step 98 Loss 1.9085\n",
      "Epoch 1 Step 99 Loss 1.7963\n",
      "Epoch 1 Step 100 Loss 1.6778\n",
      "Epoch 1 Step 101 Loss 1.2946\n",
      "Epoch 1 Step 102 Loss 1.3870\n",
      "Epoch 1 Step 103 Loss 1.7869\n",
      "Epoch 1 Step 104 Loss 1.5634\n",
      "Epoch 1 Step 105 Loss 1.7008\n",
      "Epoch 1 Step 106 Loss 1.5777\n",
      "Epoch 1 Step 107 Loss 1.8367\n",
      "Epoch 1 Step 108 Loss 1.7356\n",
      "Epoch 1 Step 109 Loss 1.7081\n",
      "Epoch 1 Step 110 Loss 1.5851\n",
      "Epoch 1 Step 111 Loss 1.4959\n",
      "Epoch 1 Step 112 Loss 1.4404\n",
      "Epoch 1 Step 113 Loss 1.6236\n",
      "Epoch 1 Step 114 Loss 1.7608\n",
      "Epoch 1 Step 115 Loss 1.5522\n",
      "Epoch 1 Step 116 Loss 1.2212\n",
      "Epoch 1 Step 117 Loss 1.3465\n",
      "Epoch 1 Step 118 Loss 1.4098\n",
      "Epoch 1 Step 119 Loss 1.7230\n",
      "Epoch 1 Step 120 Loss 1.5735\n",
      "Epoch 1 Step 121 Loss 1.8395\n",
      "Epoch 1 Step 122 Loss 1.7862\n",
      "Epoch 1 Step 123 Loss 1.5227\n",
      "Epoch 1 Step 124 Loss 1.6394\n",
      "Epoch 1 Step 125 Loss 1.7490\n",
      "Epoch 1 Step 126 Loss 1.9846\n",
      "Epoch 1 Step 127 Loss 1.9645\n",
      "Epoch 1 Step 128 Loss 1.5429\n",
      "Epoch 1 Step 129 Loss 1.3538\n",
      "Epoch 1 Step 130 Loss 1.5299\n",
      "Epoch 1 Step 131 Loss 1.2482\n",
      "Epoch 1 Step 132 Loss 1.6068\n",
      "Epoch 1 Step 133 Loss 1.5102\n",
      "Epoch 1 Step 134 Loss 1.8524\n",
      "Epoch 1 Step 135 Loss 2.2746\n",
      "Epoch 1 Step 136 Loss 1.9886\n",
      "Epoch 1 Step 137 Loss 1.9964\n",
      "Epoch 1 Step 138 Loss 1.8462\n",
      "Epoch 1 Step 139 Loss 1.6128\n",
      "Epoch 1 Step 140 Loss 1.6808\n",
      "Epoch 1 Step 141 Loss 1.8665\n",
      "Epoch 1 Step 142 Loss 1.3529\n",
      "Epoch 1 Step 143 Loss 1.5115\n",
      "Epoch 1 Step 144 Loss 1.6337\n",
      "Epoch 1 Step 145 Loss 1.5428\n",
      "Epoch 1 Step 146 Loss 1.5113\n",
      "Epoch 1 Step 147 Loss 1.5227\n",
      "Epoch 1 Step 148 Loss 1.4363\n",
      "Epoch 1 Step 149 Loss 1.4021\n",
      "Epoch 1 Step 150 Loss 1.6710\n",
      "Epoch 1 Step 151 Loss 1.5183\n",
      "Epoch 1 Step 152 Loss 1.4965\n",
      "Epoch 1 Step 153 Loss 1.6771\n",
      "Epoch 1 Step 154 Loss 1.8234\n",
      "Epoch 1 Step 155 Loss 1.6369\n",
      "Epoch 1 Step 156 Loss 1.6221\n",
      "Epoch 1 Step 157 Loss 1.6657\n",
      "Epoch 1 Step 158 Loss 1.6085\n",
      "Epoch 1 Step 159 Loss 1.2480\n",
      "Epoch 1 Step 160 Loss 1.9469\n",
      "Epoch 1 Step 161 Loss 1.6786\n",
      "Epoch 1 Step 162 Loss 1.3286\n",
      "Epoch 1 Step 163 Loss 1.6091\n",
      "Epoch 1 Step 164 Loss 1.7071\n",
      "Epoch 1 Step 165 Loss 1.4761\n",
      "Epoch 1 Step 166 Loss 1.6565\n",
      "Epoch 1 Step 167 Loss 1.4598\n",
      "Epoch 1 Step 168 Loss 1.5981\n",
      "Epoch 1 Step 169 Loss 1.9554\n",
      "Epoch 1 Step 170 Loss 1.6399\n",
      "Epoch 1 Step 171 Loss 1.3652\n",
      "Epoch 1 Step 172 Loss 1.6167\n",
      "Epoch 1 Step 173 Loss 1.9245\n",
      "Epoch 1 Step 174 Loss 1.6855\n",
      "Epoch 1 Step 175 Loss 1.5429\n",
      "Epoch 1 Step 176 Loss 1.3538\n",
      "Epoch 1 Step 177 Loss 1.5751\n",
      "Epoch 1 Step 178 Loss 1.5940\n",
      "Epoch 1 Step 179 Loss 2.0173\n",
      "Epoch 1 Step 180 Loss 1.7272\n",
      "Epoch 1 Step 181 Loss 1.7973\n",
      "Epoch 1 Step 182 Loss 1.7523\n",
      "Epoch 1 Step 183 Loss 1.7632\n",
      "Epoch 1 Step 184 Loss 1.7262\n",
      "Epoch 1 Step 185 Loss 1.7732\n",
      "Epoch 1 Step 186 Loss 1.6127\n",
      "Epoch 1 Step 187 Loss 1.2568\n",
      "Epoch 1 Step 188 Loss 1.1654\n",
      "Epoch 1 Step 189 Loss 1.6172\n",
      "Epoch 1 Step 190 Loss 1.5009\n",
      "Epoch 1 Step 191 Loss 1.6111\n",
      "Epoch 1 Step 192 Loss 1.4783\n",
      "Epoch 1 Step 193 Loss 1.6264\n",
      "Epoch 1 Step 194 Loss 1.7605\n",
      "Epoch 1 Step 195 Loss 1.7110\n",
      "Epoch 1 Step 196 Loss 1.4082\n",
      "Epoch 1 Step 197 Loss 1.5437\n",
      "Epoch 1 Step 198 Loss 1.1146\n",
      "Epoch 1 Step 199 Loss 1.7699\n",
      "Epoch 1 Step 200 Loss 1.6686\n",
      "Epoch 1 Step 201 Loss 1.2297\n",
      "Epoch 1 Step 202 Loss 1.3837\n",
      "Epoch 1 Step 203 Loss 1.9110\n",
      "Epoch 1 Step 204 Loss 1.8769\n",
      "Epoch 1 Step 205 Loss 1.8271\n",
      "Epoch 1 Step 206 Loss 2.0064\n",
      "Epoch 1 Step 207 Loss 1.5975\n",
      "Epoch 1 Step 208 Loss 1.5645\n",
      "Epoch 1 Step 209 Loss 1.7009\n",
      "Epoch 1 Step 210 Loss 1.8292\n",
      "Epoch 1 Step 211 Loss 1.8713\n",
      "Epoch 1 Step 212 Loss 1.7311\n",
      "Epoch 1 Step 213 Loss 1.7066\n",
      "Epoch 1 Step 214 Loss 2.0344\n",
      "Epoch 1 Step 215 Loss 1.6615\n",
      "Epoch 1 Step 216 Loss 1.7645\n",
      "Epoch 1 Step 217 Loss 1.4337\n",
      "Epoch 1 Step 218 Loss 1.6722\n",
      "Epoch 1 Step 219 Loss 1.9896\n",
      "Epoch 1 Step 220 Loss 1.9471\n",
      "Epoch 1 Step 221 Loss 1.6455\n",
      "Epoch 1 Step 222 Loss 1.6658\n",
      "Epoch 1 Step 223 Loss 1.7154\n",
      "Epoch 1 Step 224 Loss 2.0148\n",
      "Epoch 1 Step 225 Loss 1.8826\n",
      "Epoch 1 Step 226 Loss 1.8190\n",
      "Epoch 1 Step 227 Loss 1.7967\n",
      "Epoch 1 Step 228 Loss 1.5413\n",
      "Epoch 1 Step 229 Loss 1.5963\n",
      "Epoch 1 Step 230 Loss 1.3130\n",
      "Epoch 1 Step 231 Loss 1.5552\n",
      "Epoch 1 Step 232 Loss 2.0078\n",
      "Epoch 1 Step 233 Loss 1.7318\n",
      "Epoch 1 Step 234 Loss 2.1205\n",
      "Epoch 1 Step 235 Loss 1.8222\n",
      "Epoch 1 Step 236 Loss 1.6710\n",
      "Epoch 1 Step 237 Loss 1.6029\n",
      "Epoch 1 Step 238 Loss 1.4359\n",
      "Epoch 1 Step 239 Loss 1.4107\n",
      "Epoch 1 Step 240 Loss 1.5264\n",
      "Epoch 1 Step 241 Loss 1.8810\n",
      "Epoch 1 Step 242 Loss 1.7961\n",
      "Epoch 1 Step 243 Loss 1.5380\n",
      "Epoch 1 Step 244 Loss 1.3851\n",
      "Epoch 1 Step 245 Loss 1.4191\n",
      "Epoch 1 Step 246 Loss 1.8631\n",
      "Epoch 1 Step 247 Loss 1.6692\n",
      "Epoch 1 Step 248 Loss 1.2482\n",
      "Epoch 1 Step 249 Loss 1.6699\n",
      "Epoch 1 Step 250 Loss 1.3125\n",
      "Epoch 1 Step 251 Loss 1.1326\n",
      "Epoch 1 Step 252 Loss 1.3268\n",
      "Epoch 1 Step 253 Loss 1.8432\n",
      "Epoch 1 Step 254 Loss 1.8047\n",
      "Epoch 1 Step 255 Loss 1.7437\n",
      "Epoch 1 Step 256 Loss 1.4108\n",
      "Epoch 1 Step 257 Loss 1.5291\n",
      "Epoch 1 Step 258 Loss 1.9227\n",
      "Epoch 1 Step 259 Loss 1.6814\n",
      "Epoch 1 Step 260 Loss 1.2794\n",
      "Epoch 1 Step 261 Loss 1.3148\n",
      "Epoch 1 Step 262 Loss 1.4736\n",
      "Epoch 1 Step 263 Loss 1.4882\n",
      "Epoch 1 Step 264 Loss 1.6631\n",
      "Epoch 1 Step 265 Loss 1.5447\n",
      "Epoch 1 Step 266 Loss 2.0924\n",
      "Epoch 1 Step 267 Loss 1.9048\n",
      "Epoch 1 Step 268 Loss 1.5839\n",
      "Epoch 1 Step 269 Loss 1.4571\n",
      "Epoch 1 Step 270 Loss 1.4582\n",
      "Epoch 1 Step 271 Loss 1.5061\n",
      "Epoch 1 Step 272 Loss 1.6355\n",
      "Epoch 1 Step 273 Loss 2.1130\n",
      "Epoch 1 Step 274 Loss 2.0326\n",
      "Epoch 1 Step 275 Loss 1.8362\n",
      "Epoch 1 Step 276 Loss 1.5101\n",
      "Epoch 1 Step 277 Loss 1.6597\n",
      "Epoch 1 Step 278 Loss 1.4882\n",
      "Epoch 1 Step 279 Loss 1.8442\n",
      "Epoch 1 Step 280 Loss 1.7130\n",
      "Epoch 1 Step 281 Loss 1.5281\n",
      "Epoch 1 Step 282 Loss 1.6080\n",
      "Epoch 1 Step 283 Loss 1.6872\n",
      "Epoch 1 Step 284 Loss 1.6708\n",
      "Epoch 1 Step 285 Loss 1.2271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 286 Loss 1.4635\n",
      "Epoch 1 Step 287 Loss 1.6524\n",
      "Epoch 1 Step 288 Loss 1.9989\n",
      "Epoch 1 Step 289 Loss 1.8142\n",
      "Epoch 1 Step 290 Loss 1.9594\n",
      "Epoch 1 Step 291 Loss 1.3868\n",
      "Epoch 1 Step 292 Loss 1.5491\n",
      "Epoch 1 Step 293 Loss 1.6022\n",
      "Epoch 1 Step 294 Loss 1.5234\n",
      "Epoch 1 Step 295 Loss 1.7511\n",
      "Epoch 1 Step 296 Loss 1.9255\n",
      "Epoch 1 Step 297 Loss 1.7385\n",
      "Epoch 1 Step 298 Loss 1.7829\n",
      "Epoch 1 Step 299 Loss 1.6567\n",
      "Epoch 1 Step 300 Loss 1.7034\n",
      "Epoch 1 Step 301 Loss 1.8462\n",
      "Epoch 1 Step 302 Loss 1.7274\n",
      "Epoch 1 Step 303 Loss 1.9971\n",
      "Epoch 1 Step 304 Loss 1.6847\n",
      "Epoch 1 Step 305 Loss 1.5394\n",
      "Epoch 1 Step 306 Loss 1.8865\n",
      "Epoch 1 Step 307 Loss 1.5472\n",
      "Epoch 1 Step 308 Loss 1.6995\n",
      "Epoch 1 Step 309 Loss 1.7516\n",
      "Epoch 1 Step 310 Loss 1.4110\n",
      "Epoch 1 Step 311 Loss 1.5036\n",
      "Epoch 1 Step 312 Loss 1.4445\n",
      "Epoch 1 Step 313 Loss 1.6686\n",
      "Epoch 1 Step 314 Loss 1.8867\n",
      "Epoch 1 Step 315 Loss 2.0305\n",
      "Epoch 1 Step 316 Loss 1.7440\n",
      "Epoch 1 Step 317 Loss 1.6946\n",
      "Epoch 1 Step 318 Loss 1.7450\n",
      "Epoch 1 Step 319 Loss 1.6756\n",
      "Epoch 1 Step 320 Loss 1.6879\n",
      "Epoch 1 Step 321 Loss 1.3721\n",
      "Epoch 1 Step 322 Loss 1.5244\n",
      "Epoch 1 Step 323 Loss 1.6996\n",
      "Epoch 1 Step 324 Loss 1.8221\n",
      "Epoch 1 Step 325 Loss 1.6093\n",
      "Epoch 1 Step 326 Loss 1.9030\n",
      "Epoch 1 Step 327 Loss 1.9320\n",
      "Epoch 1 Step 328 Loss 2.1446\n",
      "Epoch 1 Step 329 Loss 1.8561\n",
      "Epoch 1 Step 330 Loss 1.6818\n",
      "Epoch 1 Step 331 Loss 2.0327\n",
      "Epoch 1 Step 332 Loss 1.7570\n",
      "Epoch 1 Step 333 Loss 1.7110\n",
      "Epoch 1 Step 334 Loss 1.4796\n",
      "Epoch 1 Step 335 Loss 1.5754\n",
      "Epoch 1 Step 336 Loss 1.7702\n",
      "Epoch 1 Step 337 Loss 1.5195\n",
      "Epoch 1 Step 338 Loss 1.4706\n",
      "Epoch 1 Step 339 Loss 1.8413\n",
      "Epoch 1 Step 340 Loss 1.8234\n",
      "Epoch 1 Step 341 Loss 1.7727\n",
      "Epoch 1 Step 342 Loss 1.6602\n",
      "Epoch 1 Step 343 Loss 1.5125\n",
      "Epoch 1 Step 344 Loss 1.8345\n",
      "Epoch 1 Step 345 Loss 2.0245\n",
      "Epoch 1 Step 346 Loss 1.8303\n",
      "Epoch 1 Step 347 Loss 1.7850\n",
      "Epoch 1 Step 348 Loss 1.5153\n",
      "Epoch 1 Step 349 Loss 1.7322\n",
      "Epoch 1 Step 350 Loss 1.5655\n",
      "Epoch 1 Step 351 Loss 1.4708\n",
      "Epoch 1 Step 352 Loss 1.3576\n",
      "Epoch 1 Step 353 Loss 1.8660\n",
      "Epoch 1 Step 354 Loss 2.1400\n",
      "Epoch 1 Step 355 Loss 1.8077\n",
      "Epoch 1 Step 356 Loss 1.4617\n",
      "Epoch 1 Step 357 Loss 1.5975\n",
      "Epoch 1 Step 358 Loss 1.7036\n",
      "Epoch 1 Step 359 Loss 1.6856\n",
      "Epoch 1 Step 360 Loss 1.9632\n",
      "Epoch 1 Step 361 Loss 1.9156\n",
      "Epoch 1 Step 362 Loss 1.8989\n",
      "Epoch 1 Step 363 Loss 1.5341\n",
      "Epoch 1 Step 364 Loss 1.6627\n",
      "Epoch 1 Step 365 Loss 1.3533\n",
      "Epoch 1 Step 366 Loss 1.1963\n",
      "Epoch 1 Step 367 Loss 1.2476\n",
      "Epoch 1 Step 368 Loss 1.1593\n",
      "Epoch 1 Step 369 Loss 1.1759\n",
      "Epoch 1 Step 370 Loss 1.5333\n",
      "Epoch 1 Step 371 Loss 1.6784\n",
      "Epoch 1 Step 372 Loss 1.7622\n",
      "Epoch 1 Step 373 Loss 1.6557\n",
      "Epoch 1 Step 374 Loss 1.4469\n",
      "Epoch 1 Step 375 Loss 1.7872\n",
      "Epoch 1 Step 376 Loss 1.5117\n",
      "Epoch 1 Step 377 Loss 1.2838\n",
      "Epoch 1 Step 378 Loss 1.6865\n",
      "Epoch 1 Step 379 Loss 1.8582\n",
      "Epoch 1 Step 380 Loss 1.8684\n",
      "Epoch 1 Step 381 Loss 1.9249\n",
      "Epoch 1 Step 382 Loss 1.5872\n",
      "Epoch 1 Step 383 Loss 1.8674\n",
      "Epoch 1 Step 384 Loss 2.2248\n",
      "Epoch 1 Step 385 Loss 1.9401\n",
      "Epoch 1 Step 386 Loss 1.9259\n",
      "Epoch 1 Step 387 Loss 1.8672\n",
      "Epoch 1 Step 388 Loss 1.8149\n",
      "Epoch 1 Step 389 Loss 2.0169\n",
      "Epoch 1 Step 390 Loss 1.5557\n",
      "Epoch 1 Step 391 Loss 1.6512\n",
      "Epoch 1 Step 392 Loss 1.8835\n",
      "Epoch 1 Step 393 Loss 1.6886\n",
      "Epoch 1 Step 394 Loss 1.5792\n",
      "Epoch 1 Step 395 Loss 1.4392\n",
      "Epoch 1 Step 396 Loss 1.3128\n",
      "Epoch 1 Step 397 Loss 1.3487\n",
      "Epoch 1 Step 398 Loss 1.3593\n",
      "Epoch 1 Step 399 Loss 1.2722\n",
      "Epoch 1 Step 400 Loss 1.6943\n",
      "Epoch 1 Step 401 Loss 1.5078\n",
      "Epoch 1 Step 402 Loss 1.4450\n",
      "Epoch 1 Step 403 Loss 1.6595\n",
      "Epoch 1 Step 404 Loss 1.9856\n",
      "Epoch 1 Step 405 Loss 1.7506\n",
      "Epoch 1 Step 406 Loss 1.8946\n",
      "Epoch 1 Step 407 Loss 1.7592\n",
      "Epoch 1 Step 408 Loss 2.0159\n",
      "Epoch 1 Step 409 Loss 1.9864\n",
      "Epoch 1 Step 410 Loss 1.7708\n",
      "Epoch 1 Step 411 Loss 1.4969\n",
      "Epoch 1 Step 412 Loss 1.5376\n",
      "Epoch 1 Step 413 Loss 1.6455\n",
      "Epoch 1 Step 414 Loss 2.1176\n",
      "Epoch 1 Step 415 Loss 2.0540\n",
      "Epoch 1 Step 416 Loss 1.6735\n",
      "Epoch 1 Step 417 Loss 1.5702\n",
      "Epoch 1 Step 418 Loss 1.4550\n",
      "Epoch 1 Step 419 Loss 1.7015\n",
      "Epoch 1 Step 420 Loss 1.6284\n",
      "Epoch 1 Step 421 Loss 1.6212\n",
      "Epoch 1 Step 422 Loss 1.7516\n",
      "Epoch 1 Step 423 Loss 1.7445\n",
      "Epoch 1 Step 424 Loss 1.5490\n",
      "Epoch 1 Step 425 Loss 1.6821\n",
      "Epoch 1 Step 426 Loss 1.7987\n",
      "Epoch 1 Step 427 Loss 1.4866\n",
      "Epoch 1 Step 428 Loss 1.2209\n",
      "Epoch 1 Step 429 Loss 1.5526\n",
      "Epoch 1 Step 430 Loss 1.8033\n",
      "Epoch 1 Step 431 Loss 1.9331\n",
      "Epoch 1 Step 432 Loss 1.7441\n",
      "Epoch 1 Step 433 Loss 1.9072\n",
      "Epoch 1 Step 434 Loss 1.5892\n",
      "Epoch 1 Step 435 Loss 1.2264\n",
      "Epoch 1 Step 436 Loss 1.4452\n",
      "Epoch 1 Step 437 Loss 1.4409\n",
      "Epoch 1 Step 438 Loss 1.5150\n",
      "Epoch 1 Step 439 Loss 1.6183\n",
      "Epoch 1 Step 440 Loss 1.5647\n",
      "Epoch 1 Step 441 Loss 1.4852\n",
      "Epoch 1 Step 442 Loss 1.7932\n",
      "Epoch 1 Step 443 Loss 1.5586\n",
      "Epoch 1 Step 444 Loss 1.3586\n",
      "Epoch 1 Step 445 Loss 1.5364\n",
      "Epoch 1 Step 446 Loss 1.6035\n",
      "Epoch 1 Step 447 Loss 1.4863\n",
      "Epoch 1 Step 448 Loss 1.3772\n",
      "Epoch 1 Step 449 Loss 1.6648\n",
      "Epoch 1 Step 450 Loss 1.4375\n",
      "Epoch 1 Step 451 Loss 1.8155\n",
      "Epoch 1 Step 452 Loss 1.9220\n",
      "Epoch 1 Step 453 Loss 2.0755\n",
      "Epoch 1 Step 454 Loss 1.8340\n",
      "Epoch 1 Step 455 Loss 1.8108\n",
      "Epoch 1 Step 456 Loss 1.5986\n",
      "Epoch 1 Step 457 Loss 1.8560\n",
      "Epoch 1 Step 458 Loss 1.7971\n",
      "Epoch 1 Step 459 Loss 1.7113\n",
      "Epoch 1 Step 460 Loss 1.7936\n",
      "Epoch 1 Step 461 Loss 1.6116\n",
      "Epoch 1 Step 462 Loss 1.7267\n",
      "Epoch 1 Step 463 Loss 1.9997\n",
      "Epoch 1 Step 464 Loss 2.0276\n",
      "Epoch 1 Step 465 Loss 2.0291\n",
      "Epoch 1 Step 466 Loss 1.8590\n",
      "Epoch 1 Step 467 Loss 2.1558\n",
      "Epoch 1 Step 468 Loss 2.1106\n",
      "Epoch 1 Step 469 Loss 1.7469\n",
      "Epoch 1 Step 470 Loss 1.6256\n",
      "Epoch 1 Step 471 Loss 1.4408\n",
      "Epoch 1 Step 472 Loss 1.6085\n",
      "Epoch 1 Step 473 Loss 2.0179\n",
      "Epoch 1 Step 474 Loss 1.6536\n",
      "Epoch 1 Step 475 Loss 1.6087\n",
      "Epoch 1 Step 476 Loss 1.7917\n",
      "Epoch 1 Step 477 Loss 1.6589\n",
      "Epoch 1 Step 478 Loss 2.0202\n",
      "Epoch 1 Step 479 Loss 1.6259\n",
      "Epoch 1 Step 480 Loss 1.6700\n",
      "Epoch 1 Step 481 Loss 1.9975\n",
      "Epoch 1 Step 482 Loss 1.7091\n",
      "Epoch 1 Step 483 Loss 1.8032\n",
      "Epoch 1 Step 484 Loss 1.5042\n",
      "Epoch 1 Step 485 Loss 1.4197\n",
      "Epoch 1 Step 486 Loss 1.8647\n",
      "Epoch 1 Step 487 Loss 1.5859\n",
      "Epoch 1 Step 488 Loss 1.1705\n",
      "Epoch 1 Step 489 Loss 1.4942\n",
      "Epoch 1 Step 490 Loss 1.3820\n",
      "Epoch 1 Step 491 Loss 1.9634\n",
      "Epoch 1 Step 492 Loss 1.7235\n",
      "Epoch 1 Step 493 Loss 1.7643\n",
      "Epoch 1 Step 494 Loss 1.4339\n",
      "Epoch 1 Step 495 Loss 1.5422\n",
      "Epoch 1 Step 496 Loss 1.5598\n",
      "Epoch 1 Step 497 Loss 1.3288\n",
      "Epoch 1 Step 498 Loss 1.7191\n",
      "Epoch 1 Step 499 Loss 1.6821\n",
      "Epoch 1 Step 500 Loss 1.7061\n",
      "Epoch 1 Step 501 Loss 1.5188\n",
      "Epoch 1 Step 502 Loss 1.8513\n",
      "Epoch 1 Step 503 Loss 1.7869\n",
      "Epoch 1 Step 504 Loss 1.6680\n",
      "Epoch 1 Step 505 Loss 1.4563\n",
      "Epoch 1 Step 506 Loss 1.7593\n",
      "Epoch 1 Step 507 Loss 1.7727\n",
      "Epoch 1 Step 508 Loss 1.7660\n",
      "Epoch 1 Step 509 Loss 1.5820\n",
      "Epoch 1 Step 510 Loss 1.5422\n",
      "Epoch 1 Step 511 Loss 1.8087\n",
      "Epoch 1 Step 512 Loss 1.7869\n",
      "Epoch 1 Step 513 Loss 1.4663\n",
      "Epoch 1 Step 514 Loss 1.3473\n",
      "Epoch 1 Step 515 Loss 1.6169\n",
      "Epoch 1 Step 516 Loss 1.7166\n",
      "Epoch 1 Step 517 Loss 1.2946\n",
      "Epoch 1 Step 518 Loss 1.5630\n",
      "Epoch 1 Step 519 Loss 1.6051\n",
      "Epoch 1 Step 520 Loss 1.4485\n",
      "Epoch 1 Step 521 Loss 1.4086\n",
      "Epoch 1 Step 522 Loss 1.1322\n",
      "Epoch 1 Step 523 Loss 1.5576\n",
      "Epoch 1 Step 524 Loss 1.8379\n",
      "Epoch 1 Step 525 Loss 1.9351\n",
      "Epoch 1 Step 526 Loss 1.9018\n",
      "Epoch 1 Step 527 Loss 1.7623\n",
      "Epoch 1 Step 528 Loss 1.3596\n",
      "Epoch 1 Step 529 Loss 1.7314\n",
      "Epoch 1 Step 530 Loss 1.6037\n",
      "Epoch 1 Step 531 Loss 1.1584\n",
      "Epoch 1 Step 532 Loss 1.3808\n",
      "Epoch 1 Step 533 Loss 1.5619\n",
      "Epoch 1 Step 534 Loss 1.5279\n",
      "Epoch 1 Step 535 Loss 1.3550\n",
      "Epoch 1 Step 536 Loss 1.6577\n",
      "Epoch 1 Step 537 Loss 1.6610\n",
      "Epoch 1 Step 538 Loss 1.7468\n",
      "Epoch 1 Step 539 Loss 1.3660\n",
      "Epoch 1 Step 540 Loss 1.9127\n",
      "Epoch 1 Step 541 Loss 1.5152\n",
      "Epoch 1 Step 542 Loss 1.6805\n",
      "Epoch 1 Step 543 Loss 1.7832\n",
      "Epoch 1 Step 544 Loss 2.0327\n",
      "Epoch 1 Step 545 Loss 1.6361\n",
      "Epoch 1 Step 546 Loss 1.9060\n",
      "Epoch 1 Step 547 Loss 2.0119\n",
      "Epoch 1 Step 548 Loss 1.7716\n",
      "Epoch 1 Step 549 Loss 1.5097\n",
      "Epoch 1 Step 550 Loss 1.7440\n",
      "Epoch 1 Step 551 Loss 1.5581\n",
      "Epoch 1 Step 552 Loss 1.8467\n",
      "Epoch 1 Step 553 Loss 1.9625\n",
      "Epoch 1 Step 554 Loss 1.9710\n",
      "Epoch 1 Step 555 Loss 1.5032\n",
      "Epoch 1 Step 556 Loss 1.4372\n",
      "Epoch 1 Step 557 Loss 1.7336\n",
      "Epoch 1 Step 558 Loss 1.4760\n",
      "Epoch 1 Step 559 Loss 1.7180\n",
      "Epoch 1 Step 560 Loss 2.0312\n",
      "Epoch 1 Step 561 Loss 1.7574\n",
      "Epoch 1 Step 562 Loss 1.9229\n",
      "Epoch 1 Step 563 Loss 1.7820\n",
      "Epoch 1 Step 564 Loss 1.8940\n",
      "Epoch 1 Step 565 Loss 2.0435\n",
      "Epoch 1 Step 566 Loss 1.8313\n",
      "Epoch 1 Step 567 Loss 1.8830\n",
      "Epoch 1 Step 568 Loss 1.8544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 569 Loss 1.9687\n",
      "Epoch 1 Step 570 Loss 1.8417\n",
      "Epoch 1 Step 571 Loss 1.6521\n",
      "Epoch 1 Step 572 Loss 1.4176\n",
      "Epoch 1 Step 573 Loss 1.2709\n",
      "Epoch 1 Step 574 Loss 1.6076\n",
      "Epoch 1 Step 575 Loss 1.7721\n",
      "Epoch 1 Step 576 Loss 1.6970\n",
      "Epoch 1 Step 577 Loss 1.8982\n",
      "Epoch 1 Step 578 Loss 1.8142\n",
      "Epoch 1 Step 579 Loss 1.7504\n",
      "Epoch 1 Step 580 Loss 1.6760\n",
      "Epoch 1 Step 581 Loss 1.5898\n",
      "Epoch 1 Step 582 Loss 1.6699\n",
      "Epoch 1 Step 583 Loss 1.8077\n",
      "Epoch 1 Step 584 Loss 1.5253\n",
      "Epoch 1 Step 585 Loss 1.3114\n",
      "Epoch 1 Step 586 Loss 1.4344\n",
      "Epoch 1 Step 587 Loss 1.3075\n",
      "Epoch 1 Step 588 Loss 1.2855\n",
      "Epoch 1 Step 589 Loss 1.3504\n",
      "Epoch 1 Step 590 Loss 1.4149\n",
      "Epoch 1 Step 591 Loss 1.8048\n",
      "Epoch 1 Step 592 Loss 1.7571\n",
      "Epoch 1 Step 593 Loss 1.6703\n",
      "Epoch 1 Step 594 Loss 1.8602\n",
      "Epoch 1 Step 595 Loss 2.0419\n",
      "Epoch 1 Step 596 Loss 1.7697\n",
      "Epoch 1 Step 597 Loss 1.7983\n",
      "Epoch 1 Step 598 Loss 1.4311\n",
      "Epoch 1 Step 599 Loss 1.3124\n",
      "Epoch 1 Step 600 Loss 1.7091\n",
      "Epoch 1 Step 601 Loss 1.8946\n",
      "Epoch 1 Step 602 Loss 1.2047\n",
      "Epoch 1 Step 603 Loss 1.4567\n",
      "Epoch 1 Step 604 Loss 1.7313\n",
      "Epoch 1 Step 605 Loss 1.6411\n",
      "Epoch 1 Step 606 Loss 1.7685\n",
      "Epoch 1 Step 607 Loss 1.6899\n",
      "Epoch 1 Step 608 Loss 1.5104\n",
      "Epoch 1 Step 609 Loss 1.7665\n",
      "Epoch 1 Step 610 Loss 1.5202\n",
      "Epoch 1 Step 611 Loss 1.5782\n",
      "Epoch 1 Step 612 Loss 1.7567\n",
      "Epoch 1 Step 613 Loss 1.4975\n",
      "Epoch 1 Step 614 Loss 1.8574\n",
      "Epoch 1 Step 615 Loss 1.9555\n",
      "Epoch 1 Step 616 Loss 1.7581\n",
      "Epoch 1 Step 617 Loss 1.7667\n",
      "Epoch 1 Step 618 Loss 1.6126\n",
      "Epoch 1 Step 619 Loss 1.5925\n",
      "Epoch 1 Step 620 Loss 1.5979\n",
      "Epoch 1 Step 621 Loss 1.4720\n",
      "Epoch 1 Step 622 Loss 1.9424\n",
      "Epoch 1 Step 623 Loss 1.6575\n",
      "Epoch 1 Step 624 Loss 1.6446\n",
      "Epoch 1 Step 625 Loss 1.7234\n",
      "Epoch 1 Step 626 Loss 1.8567\n",
      "Epoch 1 Step 627 Loss 1.8090\n",
      "Epoch 1 Step 628 Loss 1.4307\n",
      "Epoch 1 Step 629 Loss 1.7249\n",
      "Epoch 1 Step 630 Loss 1.6605\n",
      "Epoch 1 Step 631 Loss 1.8067\n",
      "Epoch 1 Step 632 Loss 1.9724\n",
      "Epoch 1 Step 633 Loss 1.7236\n",
      "Epoch 1 Step 634 Loss 1.7182\n",
      "Epoch 1 Step 635 Loss 1.8871\n",
      "Epoch 1 Step 636 Loss 2.3115\n",
      "Epoch 1 Step 637 Loss 2.0869\n",
      "Epoch 1 Step 638 Loss 1.7327\n",
      "Epoch 1 Step 639 Loss 1.7610\n",
      "Epoch 1 Step 640 Loss 1.8686\n",
      "Epoch 1 Step 641 Loss 1.5076\n",
      "Epoch 1 Step 642 Loss 1.3994\n",
      "Epoch 1 Step 643 Loss 1.4940\n",
      "Epoch 1 Step 644 Loss 1.4296\n",
      "Epoch 1 Step 645 Loss 1.2977\n",
      "Epoch 1 Step 646 Loss 1.3527\n",
      "Epoch 1 Step 647 Loss 1.2673\n",
      "Epoch 1 Step 648 Loss 1.0954\n",
      "Epoch 1 Step 649 Loss 1.2346\n",
      "Epoch 1 Step 650 Loss 1.2009\n",
      "Epoch 1 Step 651 Loss 1.2431\n",
      "Epoch 1 Step 652 Loss 1.4626\n",
      "Epoch 1 Step 653 Loss 1.5315\n",
      "Epoch 1 Step 654 Loss 1.9084\n",
      "Epoch 1 Step 655 Loss 1.5443\n",
      "Epoch 1 Step 656 Loss 1.3709\n",
      "Epoch 1 Step 657 Loss 1.4328\n",
      "Epoch 1 Step 658 Loss 1.6339\n",
      "Epoch 1 Step 659 Loss 1.6952\n",
      "Epoch 1 Step 660 Loss 1.3990\n",
      "Epoch 1 Step 661 Loss 1.7013\n",
      "Epoch 1 Step 662 Loss 1.7970\n",
      "Epoch 1 Step 663 Loss 1.6751\n",
      "Epoch 1 Step 664 Loss 1.3247\n",
      "Epoch 1 Step 665 Loss 1.4006\n",
      "Epoch 1 Step 666 Loss 1.5205\n",
      "Epoch 1 Step 667 Loss 1.6878\n",
      "Epoch 1 Step 668 Loss 1.8536\n",
      "Epoch 1 Step 669 Loss 1.9986\n",
      "Epoch 1 Step 670 Loss 1.7804\n",
      "Epoch 1 Step 671 Loss 1.6476\n",
      "Epoch 1 Step 672 Loss 1.4370\n",
      "Epoch 1 Step 673 Loss 1.5983\n",
      "Epoch 1 Step 674 Loss 1.6768\n",
      "Epoch 1 Step 675 Loss 1.6823\n",
      "Epoch 1 Step 676 Loss 1.5704\n",
      "Epoch 1 Step 677 Loss 1.6619\n",
      "Epoch 1 Step 678 Loss 1.4328\n",
      "Epoch 1 Step 679 Loss 1.6397\n",
      "Epoch 1 Step 680 Loss 1.8634\n",
      "Epoch 1 Step 681 Loss 1.4815\n",
      "Epoch 1 Step 682 Loss 1.5416\n",
      "Epoch 1 Step 683 Loss 1.4041\n",
      "Epoch 1 Step 684 Loss 1.4262\n",
      "Epoch 1 Step 685 Loss 1.6372\n",
      "Epoch 1 Step 686 Loss 1.9819\n",
      "Epoch 1 Step 687 Loss 1.2615\n",
      "Epoch 1 Step 688 Loss 1.0465\n",
      "Epoch 1 Step 689 Loss 1.8413\n",
      "Epoch 1 Step 690 Loss 1.7001\n",
      "Epoch 1 Step 691 Loss 1.5778\n",
      "Epoch 1 Step 692 Loss 1.6762\n",
      "Epoch 1 Step 693 Loss 1.3761\n",
      "Epoch 1 Step 694 Loss 1.7919\n",
      "Epoch 1 Step 695 Loss 1.8351\n",
      "Epoch 1 Step 696 Loss 1.7971\n",
      "Epoch 1 Step 697 Loss 1.3676\n",
      "Epoch 1 Step 698 Loss 1.3925\n",
      "Epoch 1 Step 699 Loss 1.4713\n",
      "Epoch 1 Step 700 Loss 1.3488\n",
      "Epoch 1 Step 701 Loss 1.4914\n",
      "Epoch 1 Step 702 Loss 1.4491\n",
      "Epoch 1 Step 703 Loss 2.2171\n",
      "Epoch 1 Step 704 Loss 2.0508\n",
      "Epoch 1 Step 705 Loss 1.5540\n",
      "Epoch 1 Step 706 Loss 1.3291\n",
      "Epoch 1 Step 707 Loss 1.3753\n",
      "Epoch 1 Step 708 Loss 1.3045\n",
      "Epoch 1 Step 709 Loss 1.5834\n",
      "Epoch 1 Step 710 Loss 1.5606\n",
      "Epoch 1 Step 711 Loss 1.5772\n",
      "Epoch 1 Step 712 Loss 1.6377\n",
      "Epoch 1 Step 713 Loss 1.7452\n",
      "Epoch 1 Step 714 Loss 1.3934\n",
      "Epoch 1 Step 715 Loss 1.5877\n",
      "Epoch 1 Step 716 Loss 1.7278\n",
      "Epoch 1 Step 717 Loss 1.9962\n",
      "Epoch 1 Step 718 Loss 1.7780\n",
      "Epoch 1 Step 719 Loss 1.7605\n",
      "Epoch 1 Step 720 Loss 1.6878\n",
      "Epoch 1 Step 721 Loss 2.1127\n",
      "Epoch 1 Step 722 Loss 1.9082\n",
      "Epoch 1 Step 723 Loss 1.9287\n",
      "Epoch 1 Step 724 Loss 1.3678\n",
      "Epoch 1 Step 725 Loss 1.6475\n",
      "Epoch 1 Step 726 Loss 1.6196\n",
      "Epoch 1 Step 727 Loss 1.6051\n",
      "Epoch 1 Step 728 Loss 1.1776\n",
      "Epoch 1 Step 729 Loss 1.1872\n",
      "Epoch 1 Step 730 Loss 1.0355\n",
      "Epoch 1 Step 731 Loss 1.2310\n",
      "Epoch 1 Step 732 Loss 1.6076\n",
      "Epoch 1 Step 733 Loss 1.8598\n",
      "Epoch 1 Step 734 Loss 1.9296\n",
      "Epoch 1 Step 735 Loss 1.6260\n",
      "Epoch 1 Step 736 Loss 1.5973\n",
      "Epoch 1 Step 737 Loss 1.9676\n",
      "Epoch 1 Step 738 Loss 1.5128\n",
      "Epoch 1 Step 739 Loss 1.3773\n",
      "Epoch 1 Step 740 Loss 1.5177\n",
      "Epoch 1 Step 741 Loss 1.8534\n",
      "Epoch 1 Step 742 Loss 1.1831\n",
      "Epoch 1 Step 743 Loss 1.2978\n",
      "Epoch 1 Step 744 Loss 1.4965\n",
      "Epoch 1 Step 745 Loss 1.8819\n",
      "Epoch 1 Step 746 Loss 1.9125\n",
      "Epoch 1 Step 747 Loss 1.5601\n",
      "Epoch 1 Step 748 Loss 1.5176\n",
      "Epoch 1 Step 749 Loss 1.5287\n",
      "Epoch 1 Step 750 Loss 1.5698\n",
      "Epoch 1 Step 751 Loss 1.4598\n",
      "Epoch 1 Step 752 Loss 1.5369\n",
      "Epoch 1 Step 753 Loss 1.3466\n",
      "Epoch 1 Step 754 Loss 1.6499\n",
      "Epoch 1 Step 755 Loss 1.6204\n",
      "Epoch 1 Step 756 Loss 1.8092\n",
      "Epoch 1 Step 757 Loss 1.9557\n",
      "Epoch 1 Step 758 Loss 1.9468\n",
      "Epoch 1 Step 759 Loss 1.4633\n",
      "Epoch 1 Step 760 Loss 1.5351\n",
      "Epoch 1 Step 761 Loss 1.6911\n",
      "Epoch 1 Step 762 Loss 1.8496\n",
      "Epoch 1 Step 763 Loss 1.5340\n",
      "Epoch 1 Step 764 Loss 1.8631\n",
      "Epoch 1 Step 765 Loss 1.6748\n",
      "Epoch 1 Step 766 Loss 1.9880\n",
      "Epoch 1 Step 767 Loss 1.8432\n",
      "Epoch 1 Step 768 Loss 1.5840\n",
      "Epoch 1 Step 769 Loss 1.7528\n",
      "Epoch 1 Step 770 Loss 1.3032\n",
      "Epoch 1 Step 771 Loss 1.3649\n",
      "Epoch 1 Step 772 Loss 1.7543\n",
      "Epoch 1 Step 773 Loss 1.5763\n",
      "Epoch 1 Step 774 Loss 1.4560\n",
      "Epoch 1 Step 775 Loss 1.3663\n",
      "Epoch 1 Step 776 Loss 1.7237\n",
      "Epoch 1 Step 777 Loss 1.8932\n",
      "Epoch 1 Step 778 Loss 1.5276\n",
      "Epoch 1 Step 779 Loss 1.3543\n",
      "Epoch 1 Step 780 Loss 1.5527\n",
      "Epoch 1 Step 781 Loss 1.7897\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = params[\"epochs\"]\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)):\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    print ('Latest checkpoint restored!!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            print('Epoch {} Step {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             checkpoint_prefix))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, inputs):\n",
    "    attentions_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = \"\"\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_output, enc_hidden = model.encoder(inputs, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    dec_input = tf.expand_dims([vocab[\"<START>\"]], 0)\n",
    "    \n",
    "    context_vectors, _ = model.attention(dec_hidden, enc_output)\n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "\n",
    "        predictions. dec_hidden = model.decoder(dec_input, dec_hidden, enc_output, context_vector)\n",
    "        \n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        \n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += reverse_vocab[predicted_id] + \" \"\n",
    "        if reverse_vocab[predicted_id] == \"<STOP>\":\n",
    "            return result, sentence, attention_plot \n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    sentence = procepross_sentence(sentence, max_length_inp, vocab)\n",
    "    \n",
    "    result, sentence, attention_plot = evaluate(model, sentence)\n",
    "    \n",
    "    print(\"input: %s\" % (sentence))\n",
    "    print(\"predicted translation: {}\".format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(\" \")), :len(sentence.split(\" \"))]\n",
    "    plot_attention(attention_plot, sentence.split(\" \"), result.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
