{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"encoder_decoder.ipynb","provenance":[],"authorship_tag":"ABX9TyOJc3YxqJKT8Xq1DVLKxpNT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ShlIoSRXjTBz","colab_type":"code","outputId":"c78b16e2-31b6-47ef-a548-95ea92e0c46d","executionInfo":{"status":"ok","timestamp":1589638784771,"user_tz":-480,"elapsed":4118,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Sat May 16 14:19:44 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vepuzlgwjaRy","colab_type":"code","outputId":"0daaa540-109b-4dd0-ee9f-49d499f3e6a8","executionInfo":{"status":"ok","timestamp":1589638844581,"user_tz":-480,"elapsed":57144,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4Px1kGQSlL5j","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/Hashing\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dd0f61zOlXcZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","def load_dataset(train_x_path, train_y_path, test_x_path):\n","  train_X = np.loadtxt(train_x_path)\n","  train_Y = np.loadtxt(train_y_path)\n","  test_X = np.loadtxt(test_x_path)\n","\n","  train_X.dtype = \"float64\"\n","  train_Y.dtype = \"float64\"\n","  test_X.dtype = \"float64\"\n","\n","  return train_X, train_Y, test_X"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPhroY7VmpFp","colab_type":"code","colab":{}},"source":["train_x_path = \"./gen_data/train_X\"\n","train_y_path = \"./gen_data/train_Y\"\n","test_x_path = \"./gen_data/test_X\"\n","train_X, train_Y, test_X = load_dataset(train_x_path, train_y_path, test_x_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3z7KHUpnCyh","colab_type":"code","colab":{}},"source":["def load_vocab(vocab_path):\n","  vocab = {}\n","  reverse_vocab = {}\n","  for line in open(vocab_path, \"r\", encoding=\"utf-8\").readlines():\n","    word, index = line.strip().split(\"\\t\")\n","    index = int(index)\n","    vocab[word] = index\n","    reverse_vocab[index] = word\n","\n","  return vocab, reverse_vocab "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6huj9D9nxKl","colab_type":"code","colab":{}},"source":["vocab_path = \"./gen_data/vocab.txt\"\n","vocab, reverse_vocab=load_vocab(vocab_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Xa-U9O4n0SA","colab_type":"code","colab":{}},"source":["embedding_matrix = np.loadtxt(\"./gen_data/embedding_matrix\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdjGI5P0oVnR","colab_type":"code","outputId":"1c2aaeac-e41c-42a1-9e8b-b2bb8a911d78","executionInfo":{"status":"ok","timestamp":1589638909026,"user_tz":-480,"elapsed":31077,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["input_length = train_X.shape[1]\n","input_length"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["260"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"JbGu2VAGoeKO","colab_type":"code","outputId":"a56a162d-82b8-453e-ef2b-e55ad1a71466","executionInfo":{"status":"ok","timestamp":1589638909027,"user_tz":-480,"elapsed":30400,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["output_sequence_length = train_Y.shape[1]\n","output_sequence_length"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["33"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"Ql6UQbkLopGs","colab_type":"code","colab":{}},"source":["vocab_size = len(vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBRXlWdTo0wE","colab_type":"code","outputId":"d651c430-f9e1-490e-8f8e-421a68bae26b","executionInfo":{"status":"ok","timestamp":1589638909029,"user_tz":-480,"elapsed":29251,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["vocab_size"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32909"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"omi6QYDio9YU","colab_type":"code","colab":{}},"source":["sample_num = 640\n","train_X = train_X[:sample_num]\n","train_Y = train_Y[:sample_num]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pI0em13QpJPc","colab_type":"code","outputId":"6592c8d7-84fa-4139-8b9b-ee1033fbdcc0","executionInfo":{"status":"ok","timestamp":1589638909032,"user_tz":-480,"elapsed":27939,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["BUFFER_SIZE = len(train_X)\n","BUFFER_SIZE"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["640"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"kNxmT03WpOGD","colab_type":"code","outputId":"9cc02fc2-2cf8-4acb-eb7d-ab0372d9dd00","executionInfo":{"status":"ok","timestamp":1589638909033,"user_tz":-480,"elapsed":27291,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["max_length_inp = train_X.shape[1]\n","max_length_targ = train_Y.shape[1]\n","print(\"input length: {0}, output length: {1}\".format(max_length_inp, max_length_targ))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["input length: 260, output length: 33\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sC5-R6bppk1i","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 64\n","step_per_epoch = len(train_X) // BATCH_SIZE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2W1VSQXpzic","colab_type":"code","colab":{}},"source":["embedding_dim = 300\n","units = 1024\n","vocab_size = len(vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhX0pcamqRF5","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","dataset = tf.data.Dataset.from_tensor_slices((train_X, train_Y)).shuffle(BUFFER_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zxbjGQEIqclX","colab_type":"code","colab":{}},"source":["dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-cvigYQ7qod3","colab_type":"code","outputId":"64cf001e-b65b-439e-d10f-ce4fa4bdb9a2","executionInfo":{"status":"ok","timestamp":1589638916798,"user_tz":-480,"elapsed":31739,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["dataset"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((64, 260), (64, 33)), types: (tf.float64, tf.float64)>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"mA0fqvWBqsen","colab_type":"code","outputId":"bd93e489-1a2b-4d4f-d08d-7c258ff8aa4d","executionInfo":{"status":"ok","timestamp":1589638916799,"user_tz":-480,"elapsed":31075,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["train_X.shape"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(640, 260)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"KHgtry3tqwLp","colab_type":"code","colab":{}},"source":["## 构建Encoder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldiUJ8B9q4rX","colab_type":"code","colab":{}},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, embedding_matrix, enc_units, batch_size):\n","    super(Encoder, self).__init__()\n","    self.batch_size = batch_size\n","    self.enc_units = enc_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=True)\n","    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer=\"glorot_uniform\")\n","\n","  def __call__(self, x, hidden):\n","    x = self.embedding(x)\n","    print(x.shape)\n","    output, state = self.gru(x, initial_state=hidden)\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_size, self.enc_units))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N1sJGE1_smbf","colab_type":"code","colab":{}},"source":["encoder = Encoder(vocab_size, embedding_dim,embedding_matrix, units, BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0JkqcWo3sp4p","colab_type":"code","outputId":"cc31eed7-2ba7-402f-d980-e1ee13e5864d","executionInfo":{"status":"ok","timestamp":1589638916803,"user_tz":-480,"elapsed":28444,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["encoder"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.Encoder at 0x7f4169b227f0>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"xizPxukOswda","colab_type":"code","colab":{}},"source":["example_input_batch = tf.ones(shape=(BATCH_SIZE, max_length_inp), dtype=tf.int32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9q0oIqxMs3k6","colab_type":"code","outputId":"38a420a9-972d-44b8-808e-e4b6a6a72c3e","executionInfo":{"status":"ok","timestamp":1589638916807,"user_tz":-480,"elapsed":27252,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["example_input_batch.shape"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 260])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"UUFAx5n9s-yY","colab_type":"code","colab":{}},"source":["sample_hidden = encoder.initialize_hidden_state()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_wPbW4ztKKI","colab_type":"code","outputId":"c13f10dc-0837-42a9-bca2-8a401c1463f1","executionInfo":{"status":"ok","timestamp":1589638917056,"user_tz":-480,"elapsed":25815,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["sample_hidden.shape"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 1024])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"lOb58mxCtcCX","colab_type":"code","outputId":"8b59aa6e-cf73-4727-e144-b77e71ee3f2f","executionInfo":{"status":"ok","timestamp":1589638922026,"user_tz":-480,"elapsed":30140,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["(64, 260, 300)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"St7KU8tmyODB","colab_type":"code","outputId":"800dc3d6-0b5e-4c58-cef5-e64d99f1e63a","executionInfo":{"status":"ok","timestamp":1589638922030,"user_tz":-480,"elapsed":29445,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["sample_output.shape"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 260, 1024])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"DZHKl_QGyjww","colab_type":"code","outputId":"3f047283-ce13-4ef9-9eb1-93f8b67c0ae0","executionInfo":{"status":"ok","timestamp":1589638922033,"user_tz":-480,"elapsed":28805,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["sample_hidden.shape"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 1024])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"sRP6xv5yytYo","colab_type":"code","colab":{}},"source":["## 构建Attention"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NY3obob8MKPb","colab_type":"code","colab":{}},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n","\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gEKo4CQP5aa","colab_type":"code","colab":{}},"source":["## 测试Attention"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1epg_j7P9U8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"01a079d7-9e0c-4f20-b0fc-e01e7fa5cbe5","executionInfo":{"status":"ok","timestamp":1589638922037,"user_tz":-480,"elapsed":25917,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}}},"source":["attention_layer = BahdanauAttention(10)\n","attention_layer"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.BahdanauAttention at 0x7f4130683e48>"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"AxONgpucRa5P","colab_type":"code","colab":{}},"source":["attention_result, attention_weights = attention_layer(sample_hidden, sample_output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bB8R9wfTRqfG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"89ac9eb7-e4a6-4904-e22a-cdb8cde17b6f","executionInfo":{"status":"ok","timestamp":1589638923307,"user_tz":-480,"elapsed":25522,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}}},"source":["attention_result"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n","array([[-0.19075976, -0.14877167,  0.18199638, ...,  0.4263354 ,\n","         0.26286328, -0.51677346],\n","       [-0.19075976, -0.14877167,  0.18199638, ...,  0.4263354 ,\n","         0.26286328, -0.51677346],\n","       [-0.19075976, -0.14877167,  0.18199638, ...,  0.4263354 ,\n","         0.26286328, -0.51677346],\n","       ...,\n","       [-0.19075976, -0.14877167,  0.18199638, ...,  0.4263354 ,\n","         0.26286328, -0.51677346],\n","       [-0.19075976, -0.14877167,  0.18199638, ...,  0.4263354 ,\n","         0.26286328, -0.51677346],\n","       [-0.19075976, -0.14877167,  0.18199638, ...,  0.4263354 ,\n","         0.26286328, -0.51677346]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"JP5n0SxSSESv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"0e3c0fd9-4421-440f-a0ad-aafc9ff35006","executionInfo":{"status":"ok","timestamp":1589638923310,"user_tz":-480,"elapsed":24579,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}}},"source":["attention_weights.shape"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 260, 1])"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"6xoFhrJYSHCk","colab_type":"code","colab":{}},"source":["## 构建Decoder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qm0uRtQN--o8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"434ee245-c13f-4d6a-9a7f-3f16267e2a36","executionInfo":{"status":"ok","timestamp":1589640968523,"user_tz":-480,"elapsed":2040,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}}},"source":["help(tf.keras.layers.GRU)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Help on class GRU in module tensorflow.python.keras.layers.recurrent_v2:\n","\n","class GRU(tensorflow.python.keras.layers.recurrent.DropoutRNNCellMixin, tensorflow.python.keras.layers.recurrent.GRU)\n"," |  Gated Recurrent Unit - Cho et al. 2014.\n"," |  \n"," |  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n"," |  for details about the usage of RNN API.\n"," |  \n"," |  Based on available runtime hardware and constraints, this layer\n"," |  will choose different implementations (cuDNN-based or pure-TensorFlow)\n"," |  to maximize the performance. If a GPU is available and all\n"," |  the arguments to the layer meet the requirement of the CuDNN kernel\n"," |  (see below for details), the layer will use a fast cuDNN implementation.\n"," |  \n"," |  The requirements to use the cuDNN implementation are:\n"," |  \n"," |  1. `activation` == `tanh`\n"," |  2. `recurrent_activation` == `sigmoid`\n"," |  3. `recurrent_dropout` == 0\n"," |  4. `unroll` is `False`\n"," |  5. `use_bias` is `True`\n"," |  6. `reset_after` is `True`\n"," |  7. Inputs are not masked or strictly right padded.\n"," |  \n"," |  There are two variants of the GRU implementation. The default one is based on\n"," |  [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to hidden\n"," |  state before matrix multiplication. The other one is based on\n"," |  [original](https://arxiv.org/abs/1406.1078v1) and has the order reversed.\n"," |  \n"," |  The second variant is compatible with CuDNNGRU (GPU-only) and allows\n"," |  inference on CPU. Thus it has separate biases for `kernel` and\n"," |  `recurrent_kernel`. To use this variant, set `'reset_after'=True` and\n"," |  `recurrent_activation='sigmoid'`.\n"," |  \n"," |  For example:\n"," |  \n"," |  >>> inputs = tf.random.normal([32, 10, 8])\n"," |  >>> gru = tf.keras.layers.GRU(4)\n"," |  >>> output = gru(inputs)\n"," |  >>> print(output.shape)\n"," |  (32, 4)\n"," |  >>> gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)\n"," |  >>> whole_sequence_output, final_state = gru(inputs)\n"," |  >>> print(whole_sequence_output.shape)\n"," |  (32, 10, 4)\n"," |  >>> print(final_state.shape)\n"," |  (32, 4)\n"," |  \n"," |  Arguments:\n"," |    units: Positive integer, dimensionality of the output space.\n"," |    activation: Activation function to use.\n"," |      Default: hyperbolic tangent (`tanh`).\n"," |      If you pass `None`, no activation is applied\n"," |      (ie. \"linear\" activation: `a(x) = x`).\n"," |    recurrent_activation: Activation function to use\n"," |      for the recurrent step.\n"," |      Default: sigmoid (`sigmoid`).\n"," |      If you pass `None`, no activation is applied\n"," |      (ie. \"linear\" activation: `a(x) = x`).\n"," |    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n"," |    kernel_initializer: Initializer for the `kernel` weights matrix,\n"," |      used for the linear transformation of the inputs. Default:\n"," |      `glorot_uniform`.\n"," |    recurrent_initializer: Initializer for the `recurrent_kernel`\n"," |       weights matrix, used for the linear transformation of the recurrent\n"," |       state. Default: `orthogonal`.\n"," |    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n"," |    kernel_regularizer: Regularizer function applied to the `kernel` weights\n"," |      matrix. Default: `None`.\n"," |    recurrent_regularizer: Regularizer function applied to the\n"," |      `recurrent_kernel` weights matrix. Default: `None`.\n"," |    bias_regularizer: Regularizer function applied to the bias vector. Default:\n"," |      `None`.\n"," |    activity_regularizer: Regularizer function applied to the output of the\n"," |      layer (its \"activation\"). Default: `None`.\n"," |    kernel_constraint: Constraint function applied to the `kernel` weights\n"," |      matrix. Default: `None`.\n"," |    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n"," |      weights matrix. Default: `None`.\n"," |    bias_constraint: Constraint function applied to the bias vector. Default:\n"," |      `None`.\n"," |    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n"," |      transformation of the inputs. Default: 0.\n"," |    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n"," |      the linear transformation of the recurrent state. Default: 0.\n"," |    implementation: Implementation mode, either 1 or 2.\n"," |      Mode 1 will structure its operations as a larger number of\n"," |      smaller dot products and additions, whereas mode 2 will\n"," |      batch them into fewer, larger operations. These modes will\n"," |      have different performance profiles on different hardware and\n"," |      for different applications. Default: 2.\n"," |    return_sequences: Boolean. Whether to return the last output\n"," |      in the output sequence, or the full sequence. Default: `False`.\n"," |    return_state: Boolean. Whether to return the last state in addition to the\n"," |      output. Default: `False`.\n"," |    go_backwards: Boolean (default `False`).\n"," |      If True, process the input sequence backwards and return the\n"," |      reversed sequence.\n"," |    stateful: Boolean (default False). If True, the last state\n"," |      for each sample at index i in a batch will be used as initial\n"," |      state for the sample of index i in the following batch.\n"," |    unroll: Boolean (default False).\n"," |      If True, the network will be unrolled,\n"," |      else a symbolic loop will be used.\n"," |      Unrolling can speed-up a RNN,\n"," |      although it tends to be more memory-intensive.\n"," |      Unrolling is only suitable for short sequences.\n"," |    time_major: The shape format of the `inputs` and `outputs` tensors.\n"," |      If True, the inputs and outputs will be in shape\n"," |      `[timesteps, batch, feature]`, whereas in the False case, it will be\n"," |      `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n"," |      efficient because it avoids transposes at the beginning and end of the\n"," |      RNN calculation. However, most TensorFlow data is batch-major, so by\n"," |      default this function accepts input and emits output in batch-major\n"," |      form.\n"," |    reset_after: GRU convention (whether to apply reset gate after or\n"," |      before matrix multiplication). False = \"before\",\n"," |      True = \"after\" (default and CuDNN compatible).\n"," |  \n"," |  Call arguments:\n"," |    inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n"," |    mask: Binary tensor of shape `[samples, timesteps]` indicating whether\n"," |      a given timestep should be masked  (optional, defaults to `None`).\n"," |    training: Python boolean indicating whether the layer should behave in\n"," |      training mode or in inference mode. This argument is passed to the cell\n"," |      when calling it. This is only relevant if `dropout` or\n"," |      `recurrent_dropout` is used  (optional, defaults to `None`).\n"," |    initial_state: List of initial state tensors to be passed to the first\n"," |      call of the cell  (optional, defaults to `None` which causes creation\n"," |      of zero-filled initial state tensors).\n"," |  \n"," |  Method resolution order:\n"," |      GRU\n"," |      tensorflow.python.keras.layers.recurrent.DropoutRNNCellMixin\n"," |      tensorflow.python.keras.layers.recurrent.GRU\n"," |      tensorflow.python.keras.layers.recurrent.RNN\n"," |      tensorflow.python.keras.engine.base_layer.Layer\n"," |      tensorflow.python.module.module.Module\n"," |      tensorflow.python.training.tracking.tracking.AutoTrackable\n"," |      tensorflow.python.training.tracking.base.Trackable\n"," |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, time_major=False, reset_after=True, **kwargs)\n"," |  \n"," |  build(self, input_shape)\n"," |      Creates the variables of the layer (optional, for subclass implementers).\n"," |      \n"," |      This is a method that implementers of subclasses of `Layer` or `Model`\n"," |      can override if they need a state-creation step in-between\n"," |      layer instantiation and layer call.\n"," |      \n"," |      This is typically used to create the weights of `Layer` subclasses.\n"," |      \n"," |      Arguments:\n"," |        input_shape: Instance of `TensorShape`, or list of instances of\n"," |          `TensorShape` if the layer expects a list of inputs\n"," |          (one instance per input).\n"," |  \n"," |  call(self, inputs, mask=None, training=None, initial_state=None)\n"," |      This is where the layer's logic lives.\n"," |      \n"," |      Arguments:\n"," |          inputs: Input tensor, or list/tuple of input tensors.\n"," |          **kwargs: Additional keyword arguments.\n"," |      \n"," |      Returns:\n"," |          A tensor or list/tuple of tensors.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from tensorflow.python.keras.layers.recurrent.DropoutRNNCellMixin:\n"," |  \n"," |  get_dropout_mask_for_cell(self, inputs, training, count=1)\n"," |      Get the dropout mask for RNN cell's input.\n"," |      \n"," |      It will create mask based on context if there isn't any existing cached\n"," |      mask. If a new mask is generated, it will update the cache in the cell.\n"," |      \n"," |      Args:\n"," |        inputs: The input tensor whose shape will be used to generate dropout\n"," |          mask.\n"," |        training: Boolean tensor, whether its in training mode, dropout will be\n"," |          ignored in non-training mode.\n"," |        count: Int, how many dropout mask will be generated. It is useful for cell\n"," |          that has internal weights fused together.\n"," |      Returns:\n"," |        List of mask tensor, generated or cached mask based on context.\n"," |  \n"," |  get_recurrent_dropout_mask_for_cell(self, inputs, training, count=1)\n"," |      Get the recurrent dropout mask for RNN cell.\n"," |      \n"," |      It will create mask based on context if there isn't any existing cached\n"," |      mask. If a new mask is generated, it will update the cache in the cell.\n"," |      \n"," |      Args:\n"," |        inputs: The input tensor whose shape will be used to generate dropout\n"," |          mask.\n"," |        training: Boolean tensor, whether its in training mode, dropout will be\n"," |          ignored in non-training mode.\n"," |        count: Int, how many dropout mask will be generated. It is useful for cell\n"," |          that has internal weights fused together.\n"," |      Returns:\n"," |        List of mask tensor, generated or cached mask based on context.\n"," |  \n"," |  reset_dropout_mask(self)\n"," |      Reset the cached dropout masks if any.\n"," |      \n"," |      This is important for the RNN layer to invoke this in it call() method so\n"," |      that the cached mask is cleared before calling the cell.call(). The mask\n"," |      should be cached across the timestep within the same batch, but shouldn't\n"," |      be cached between batches. Otherwise it will introduce unreasonable bias\n"," |      against certain index of data within the batch.\n"," |  \n"," |  reset_recurrent_dropout_mask(self)\n"," |      Reset the cached recurrent dropout masks if any.\n"," |      \n"," |      This is important for the RNN layer to invoke this in it call() method so\n"," |      that the cached mask is cleared before calling the cell.call(). The mask\n"," |      should be cached across the timestep within the same batch, but shouldn't\n"," |      be cached between batches. Otherwise it will introduce unreasonable bias\n"," |      against certain index of data within the batch.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from tensorflow.python.keras.layers.recurrent.DropoutRNNCellMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from tensorflow.python.keras.layers.recurrent.GRU:\n"," |  \n"," |  get_config(self)\n"," |      Returns the config of the layer.\n"," |      \n"," |      A layer config is a Python dictionary (serializable)\n"," |      containing the configuration of a layer.\n"," |      The same layer can be reinstantiated later\n"," |      (without its trained weights) from this configuration.\n"," |      \n"," |      The config of a layer does not include connectivity\n"," |      information, nor the layer class name. These are handled\n"," |      by `Network` (one layer of abstraction above).\n"," |      \n"," |      Returns:\n"," |          Python dictionary.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Class methods inherited from tensorflow.python.keras.layers.recurrent.GRU:\n"," |  \n"," |  from_config(config) from builtins.type\n"," |      Creates a layer from its config.\n"," |      \n"," |      This method is the reverse of `get_config`,\n"," |      capable of instantiating the same layer from the config\n"," |      dictionary. It does not handle layer connectivity\n"," |      (handled by Network), nor weights (handled by `set_weights`).\n"," |      \n"," |      Arguments:\n"," |          config: A Python dictionary, typically the\n"," |              output of get_config.\n"," |      \n"," |      Returns:\n"," |          A layer instance.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from tensorflow.python.keras.layers.recurrent.GRU:\n"," |  \n"," |  activation\n"," |  \n"," |  bias_constraint\n"," |  \n"," |  bias_initializer\n"," |  \n"," |  bias_regularizer\n"," |  \n"," |  dropout\n"," |  \n"," |  implementation\n"," |  \n"," |  kernel_constraint\n"," |  \n"," |  kernel_initializer\n"," |  \n"," |  kernel_regularizer\n"," |  \n"," |  recurrent_activation\n"," |  \n"," |  recurrent_constraint\n"," |  \n"," |  recurrent_dropout\n"," |  \n"," |  recurrent_initializer\n"," |  \n"," |  recurrent_regularizer\n"," |  \n"," |  reset_after\n"," |  \n"," |  units\n"," |  \n"," |  use_bias\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from tensorflow.python.keras.layers.recurrent.RNN:\n"," |  \n"," |  __call__(self, inputs, initial_state=None, constants=None, **kwargs)\n"," |      Wraps `call`, applying pre- and post-processing steps.\n"," |      \n"," |      Arguments:\n"," |        *args: Positional arguments to be passed to `self.call`.\n"," |        **kwargs: Keyword arguments to be passed to `self.call`.\n"," |      \n"," |      Returns:\n"," |        Output tensor(s).\n"," |      \n"," |      Note:\n"," |        - The following optional keyword arguments are reserved for specific uses:\n"," |          * `training`: Boolean scalar tensor of Python boolean indicating\n"," |            whether the `call` is meant for training or inference.\n"," |          * `mask`: Boolean input mask.\n"," |        - If the layer's `call` method takes a `mask` argument (as some Keras\n"," |          layers do), its default value will be set to the mask generated\n"," |          for `inputs` by the previous layer (if `input` did come from\n"," |          a layer that generated a corresponding mask, i.e. if it came from\n"," |          a Keras layer with masking support.\n"," |      \n"," |      Raises:\n"," |        ValueError: if the layer's `call` method returns None (an invalid value).\n"," |        RuntimeError: if `super().__init__()` was not called in the constructor.\n"," |  \n"," |  compute_mask(self, inputs, mask)\n"," |      Computes an output mask tensor.\n"," |      \n"," |      Arguments:\n"," |          inputs: Tensor or list of tensors.\n"," |          mask: Tensor or list of tensors.\n"," |      \n"," |      Returns:\n"," |          None or a tensor (or list of tensors,\n"," |              one per output tensor of the layer).\n"," |  \n"," |  compute_output_shape(self, input_shape)\n"," |      Computes the output shape of the layer.\n"," |      \n"," |      If the layer has not been built, this method will call `build` on the\n"," |      layer. This assumes that the layer will later be used with inputs that\n"," |      match the input shape provided here.\n"," |      \n"," |      Arguments:\n"," |          input_shape: Shape tuple (tuple of integers)\n"," |              or list of shape tuples (one per output tensor of the layer).\n"," |              Shape tuples can include None for free dimensions,\n"," |              instead of an integer.\n"," |      \n"," |      Returns:\n"," |          An input shape tuple.\n"," |  \n"," |  get_initial_state(self, inputs)\n"," |  \n"," |  reset_states(self, states=None)\n"," |      Reset the recorded states for the stateful RNN layer.\n"," |      \n"," |      Can only be used when RNN layer is constructed with `stateful` = `True`.\n"," |      Args:\n"," |        states: Numpy arrays that contains the value for the initial state, which\n"," |          will be feed to cell at the first time step. When the value is None,\n"," |          zero filled numpy array will be created based on the cell state size.\n"," |      \n"," |      Raises:\n"," |        AttributeError: When the RNN layer is not stateful.\n"," |        ValueError: When the batch size of the RNN layer is unknown.\n"," |        ValueError: When the input numpy array is not compatible with the RNN\n"," |          layer state, either size wise or dtype wise.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from tensorflow.python.keras.layers.recurrent.RNN:\n"," |  \n"," |  states\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n"," |  \n"," |  __delattr__(self, name)\n"," |      Implement delattr(self, name).\n"," |  \n"," |  __getstate__(self)\n"," |  \n"," |  __setattr__(self, name, value)\n"," |      Support self.foo = trackable syntax.\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  add_loss(self, losses, inputs=None)\n"," |      Add loss tensor(s), potentially dependent on layer inputs.\n"," |      \n"," |      Some losses (for instance, activity regularization losses) may be dependent\n"," |      on the inputs passed when calling a layer. Hence, when reusing the same\n"," |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n"," |      be dependent on `a` and some on `b`. This method automatically keeps track\n"," |      of dependencies.\n"," |      \n"," |      This method can be used inside a subclassed layer or model's `call`\n"," |      function, in which case `losses` should be a Tensor or list of Tensors.\n"," |      \n"," |      Example:\n"," |      \n"," |      ```python\n"," |      class MyLayer(tf.keras.layers.Layer):\n"," |        def call(inputs, self):\n"," |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n"," |          return inputs\n"," |      ```\n"," |      \n"," |      This method can also be called directly on a Functional Model during\n"," |      construction. In this case, any loss Tensors passed to this Model must\n"," |      be symbolic and be able to be traced back to the model's `Input`s. These\n"," |      losses become part of the model's topology and are tracked in `get_config`.\n"," |      \n"," |      Example:\n"," |      \n"," |      ```python\n"," |      inputs = tf.keras.Input(shape=(10,))\n"," |      x = tf.keras.layers.Dense(10)(inputs)\n"," |      outputs = tf.keras.layers.Dense(1)(x)\n"," |      model = tf.keras.Model(inputs, outputs)\n"," |      # Activity regularization.\n"," |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n"," |      ```\n"," |      \n"," |      If this is not the case for your loss (if, for example, your loss references\n"," |      a `Variable` of one of the model's layers), you can wrap your loss in a\n"," |      zero-argument lambda. These losses are not tracked as part of the model's\n"," |      topology since they can't be serialized.\n"," |      \n"," |      Example:\n"," |      \n"," |      ```python\n"," |      inputs = tf.keras.Input(shape=(10,))\n"," |      x = tf.keras.layers.Dense(10)(inputs)\n"," |      outputs = tf.keras.layers.Dense(1)(x)\n"," |      model = tf.keras.Model(inputs, outputs)\n"," |      # Weight regularization.\n"," |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n"," |      ```\n"," |      \n"," |      The `get_losses_for` method allows to retrieve the losses relevant to a\n"," |      specific set of inputs.\n"," |      \n"," |      Arguments:\n"," |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n"," |          may also be zero-argument callables which create a loss tensor.\n"," |        inputs: Ignored when executing eagerly. If anything other than None is\n"," |          passed, it signals the losses are conditional on some of the layer's\n"," |          inputs, and thus they should only be run where these inputs are\n"," |          available. This is the case for activity regularization losses, for\n"," |          instance. If `None` is passed, the losses are assumed\n"," |          to be unconditional, and will apply across all dataflows of the layer\n"," |          (e.g. weight regularization losses).\n"," |  \n"," |  add_metric(self, value, aggregation=None, name=None)\n"," |      Adds metric tensor to the layer.\n"," |      \n"," |      Args:\n"," |        value: Metric tensor.\n"," |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n"," |          it indicates that the metric tensor provided has been aggregated\n"," |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n"," |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n"," |          given metric tensor will be sample-wise reduced using `mean` function.\n"," |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n"," |          aggregation='mean')`.\n"," |        name: String metric name.\n"," |      \n"," |      Raises:\n"," |        ValueError: If `aggregation` is anything other than None or `mean`.\n"," |  \n"," |  add_update(self, updates, inputs=None)\n"," |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n"," |      \n"," |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n"," |      Instructions for updating:\n"," |      `inputs` is now automatically inferred\n"," |      \n"," |      Weight updates (for instance, the updates of the moving mean and variance\n"," |      in a BatchNormalization layer) may be dependent on the inputs passed\n"," |      when calling a layer. Hence, when reusing the same layer on\n"," |      different inputs `a` and `b`, some entries in `layer.updates` may be\n"," |      dependent on `a` and some on `b`. This method automatically keeps track\n"," |      of dependencies.\n"," |      \n"," |      The `get_updates_for` method allows to retrieve the updates relevant to a\n"," |      specific set of inputs.\n"," |      \n"," |      This call is ignored when eager execution is enabled (in that case, variable\n"," |      updates are run on the fly and thus do not need to be tracked for later\n"," |      execution).\n"," |      \n"," |      Arguments:\n"," |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n"," |          that returns an update op. A zero-arg callable should be passed in\n"," |          order to disable running the updates by setting `trainable=False`\n"," |          on this Layer, when executing in Eager mode.\n"," |        inputs: Deprecated, will be automatically inferred.\n"," |  \n"," |  add_variable(self, *args, **kwargs)\n"," |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n"," |      \n"," |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n"," |      Instructions for updating:\n"," |      Please use `layer.add_weight` method instead.\n"," |  \n"," |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n"," |      Adds a new variable to the layer.\n"," |      \n"," |      Arguments:\n"," |        name: Variable name.\n"," |        shape: Variable shape. Defaults to scalar if unspecified.\n"," |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n"," |        initializer: Initializer instance (callable).\n"," |        regularizer: Regularizer instance (callable).\n"," |        trainable: Boolean, whether the variable should be part of the layer's\n"," |          \"trainable_variables\" (e.g. variables, biases)\n"," |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n"," |          Note that `trainable` cannot be `True` if `synchronization`\n"," |          is set to `ON_READ`.\n"," |        constraint: Constraint instance (callable).\n"," |        partitioner: Partitioner to be passed to the `Trackable` API.\n"," |        use_resource: Whether to use `ResourceVariable`.\n"," |        synchronization: Indicates when a distributed a variable will be\n"," |          aggregated. Accepted values are constants defined in the class\n"," |          `tf.VariableSynchronization`. By default the synchronization is set to\n"," |          `AUTO` and the current `DistributionStrategy` chooses\n"," |          when to synchronize. If `synchronization` is set to `ON_READ`,\n"," |          `trainable` must not be set to `True`.\n"," |        aggregation: Indicates how a distributed variable will be aggregated.\n"," |          Accepted values are constants defined in the class\n"," |          `tf.VariableAggregation`.\n"," |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n"," |          `collections`, `experimental_autocast` and `caching_device`.\n"," |      \n"," |      Returns:\n"," |        The created variable. Usually either a `Variable` or `ResourceVariable`\n"," |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n"," |        instance is returned.\n"," |      \n"," |      Raises:\n"," |        RuntimeError: If called with partitioned variable regularization and\n"," |          eager execution is enabled.\n"," |        ValueError: When giving unsupported dtype and no initializer or when\n"," |          trainable has been set to True with synchronization set as `ON_READ`.\n"," |  \n"," |  apply(self, inputs, *args, **kwargs)\n"," |      Deprecated, do NOT use! (deprecated)\n"," |      \n"," |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n"," |      Instructions for updating:\n"," |      Please use `layer.__call__` method instead.\n"," |      \n"," |      This is an alias of `self.__call__`.\n"," |      \n"," |      Arguments:\n"," |        inputs: Input tensor(s).\n"," |        *args: additional positional arguments to be passed to `self.call`.\n"," |        **kwargs: additional keyword arguments to be passed to `self.call`.\n"," |      \n"," |      Returns:\n"," |        Output tensor(s).\n"," |  \n"," |  compute_output_signature(self, input_signature)\n"," |      Compute the output tensor signature of the layer based on the inputs.\n"," |      \n"," |      Unlike a TensorShape object, a TensorSpec object contains both shape\n"," |      and dtype information for a tensor. This method allows layers to provide\n"," |      output dtype information if it is different from the input dtype.\n"," |      For any layer that doesn't implement this function,\n"," |      the framework will fall back to use `compute_output_shape`, and will\n"," |      assume that the output dtype matches the input dtype.\n"," |      \n"," |      Args:\n"," |        input_signature: Single TensorSpec or nested structure of TensorSpec\n"," |          objects, describing a candidate input for the layer.\n"," |      \n"," |      Returns:\n"," |        Single TensorSpec or nested structure of TensorSpec objects, describing\n"," |          how the layer would transform the provided input.\n"," |      \n"," |      Raises:\n"," |        TypeError: If input_signature contains a non-TensorSpec object.\n"," |  \n"," |  count_params(self)\n"," |      Count the total number of scalars composing the weights.\n"," |      \n"," |      Returns:\n"," |          An integer count.\n"," |      \n"," |      Raises:\n"," |          ValueError: if the layer isn't yet built\n"," |            (in which case its weights aren't yet defined).\n"," |  \n"," |  get_input_at(self, node_index)\n"," |      Retrieves the input tensor(s) of a layer at a given node.\n"," |      \n"," |      Arguments:\n"," |          node_index: Integer, index of the node\n"," |              from which to retrieve the attribute.\n"," |              E.g. `node_index=0` will correspond to the\n"," |              first time the layer was called.\n"," |      \n"," |      Returns:\n"," |          A tensor (or list of tensors if the layer has multiple inputs).\n"," |      \n"," |      Raises:\n"," |        RuntimeError: If called in Eager mode.\n"," |  \n"," |  get_input_mask_at(self, node_index)\n"," |      Retrieves the input mask tensor(s) of a layer at a given node.\n"," |      \n"," |      Arguments:\n"," |          node_index: Integer, index of the node\n"," |              from which to retrieve the attribute.\n"," |              E.g. `node_index=0` will correspond to the\n"," |              first time the layer was called.\n"," |      \n"," |      Returns:\n"," |          A mask tensor\n"," |          (or list of tensors if the layer has multiple inputs).\n"," |  \n"," |  get_input_shape_at(self, node_index)\n"," |      Retrieves the input shape(s) of a layer at a given node.\n"," |      \n"," |      Arguments:\n"," |          node_index: Integer, index of the node\n"," |              from which to retrieve the attribute.\n"," |              E.g. `node_index=0` will correspond to the\n"," |              first time the layer was called.\n"," |      \n"," |      Returns:\n"," |          A shape tuple\n"," |          (or list of shape tuples if the layer has multiple inputs).\n"," |      \n"," |      Raises:\n"," |        RuntimeError: If called in Eager mode.\n"," |  \n"," |  get_losses_for(self, inputs)\n"," |      Retrieves losses relevant to a specific set of inputs.\n"," |      \n"," |      Arguments:\n"," |        inputs: Input tensor or list/tuple of input tensors.\n"," |      \n"," |      Returns:\n"," |        List of loss tensors of the layer that depend on `inputs`.\n"," |  \n"," |  get_output_at(self, node_index)\n"," |      Retrieves the output tensor(s) of a layer at a given node.\n"," |      \n"," |      Arguments:\n"," |          node_index: Integer, index of the node\n"," |              from which to retrieve the attribute.\n"," |              E.g. `node_index=0` will correspond to the\n"," |              first time the layer was called.\n"," |      \n"," |      Returns:\n"," |          A tensor (or list of tensors if the layer has multiple outputs).\n"," |      \n"," |      Raises:\n"," |        RuntimeError: If called in Eager mode.\n"," |  \n"," |  get_output_mask_at(self, node_index)\n"," |      Retrieves the output mask tensor(s) of a layer at a given node.\n"," |      \n"," |      Arguments:\n"," |          node_index: Integer, index of the node\n"," |              from which to retrieve the attribute.\n"," |              E.g. `node_index=0` will correspond to the\n"," |              first time the layer was called.\n"," |      \n"," |      Returns:\n"," |          A mask tensor\n"," |          (or list of tensors if the layer has multiple outputs).\n"," |  \n"," |  get_output_shape_at(self, node_index)\n"," |      Retrieves the output shape(s) of a layer at a given node.\n"," |      \n"," |      Arguments:\n"," |          node_index: Integer, index of the node\n"," |              from which to retrieve the attribute.\n"," |              E.g. `node_index=0` will correspond to the\n"," |              first time the layer was called.\n"," |      \n"," |      Returns:\n"," |          A shape tuple\n"," |          (or list of shape tuples if the layer has multiple outputs).\n"," |      \n"," |      Raises:\n"," |        RuntimeError: If called in Eager mode.\n"," |  \n"," |  get_updates_for(self, inputs)\n"," |      Retrieves updates relevant to a specific set of inputs.\n"," |      \n"," |      Arguments:\n"," |        inputs: Input tensor or list/tuple of input tensors.\n"," |      \n"," |      Returns:\n"," |        List of update ops of the layer that depend on `inputs`.\n"," |  \n"," |  get_weights(self)\n"," |      Returns the current weights of the layer.\n"," |      \n"," |      The weights of a layer represent the state of the layer. This function\n"," |      returns both trainable and non-trainable weight values associated with this\n"," |      layer as a list of Numpy arrays, which can in turn be used to load state\n"," |      into similarly parameterized layers.\n"," |      \n"," |      For example, a Dense layer returns a list of two values-- per-output\n"," |      weights and the bias value. These can be used to set the weights of another\n"," |      Dense layer:\n"," |      \n"," |      >>> a = tf.keras.layers.Dense(1,\n"," |      ...   kernel_initializer=tf.constant_initializer(1.))\n"," |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n"," |      >>> a.get_weights()\n"," |      [array([[1.],\n"," |             [1.],\n"," |             [1.]], dtype=float32), array([0.], dtype=float32)]\n"," |      >>> b = tf.keras.layers.Dense(1,\n"," |      ...   kernel_initializer=tf.constant_initializer(2.))\n"," |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n"," |      >>> b.get_weights()\n"," |      [array([[2.],\n"," |             [2.],\n"," |             [2.]], dtype=float32), array([0.], dtype=float32)]\n"," |      >>> b.set_weights(a.get_weights())\n"," |      >>> b.get_weights()\n"," |      [array([[1.],\n"," |             [1.],\n"," |             [1.]], dtype=float32), array([0.], dtype=float32)]\n"," |      \n"," |      Returns:\n"," |          Weights values as a list of numpy arrays.\n"," |  \n"," |  set_weights(self, weights)\n"," |      Sets the weights of the layer, from Numpy arrays.\n"," |      \n"," |      The weights of a layer represent the state of the layer. This function\n"," |      sets the weight values from numpy arrays. The weight values should be\n"," |      passed in the order they are created by the layer. Note that the layer's\n"," |      weights must be instantiated before calling this function by calling\n"," |      the layer.\n"," |      \n"," |      For example, a Dense layer returns a list of two values-- per-output\n"," |      weights and the bias value. These can be used to set the weights of another\n"," |      Dense layer:\n"," |      \n"," |      >>> a = tf.keras.layers.Dense(1,\n"," |      ...   kernel_initializer=tf.constant_initializer(1.))\n"," |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n"," |      >>> a.get_weights()\n"," |      [array([[1.],\n"," |             [1.],\n"," |             [1.]], dtype=float32), array([0.], dtype=float32)]\n"," |      >>> b = tf.keras.layers.Dense(1,\n"," |      ...   kernel_initializer=tf.constant_initializer(2.))\n"," |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n"," |      >>> b.get_weights()\n"," |      [array([[2.],\n"," |             [2.],\n"," |             [2.]], dtype=float32), array([0.], dtype=float32)]\n"," |      >>> b.set_weights(a.get_weights())\n"," |      >>> b.get_weights()\n"," |      [array([[1.],\n"," |             [1.],\n"," |             [1.]], dtype=float32), array([0.], dtype=float32)]\n"," |      \n"," |      Arguments:\n"," |          weights: a list of Numpy arrays. The number\n"," |              of arrays and their shape must match\n"," |              number of the dimensions of the weights\n"," |              of the layer (i.e. it should match the\n"," |              output of `get_weights`).\n"," |      \n"," |      Raises:\n"," |          ValueError: If the provided weights list does not match the\n"," |              layer's specifications.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n"," |  \n"," |  activity_regularizer\n"," |      Optional regularizer function for the output of this layer.\n"," |  \n"," |  dtype\n"," |      Dtype used by the weights of the layer, set in the constructor.\n"," |  \n"," |  dynamic\n"," |      Whether the layer is dynamic (eager-only); set in the constructor.\n"," |  \n"," |  inbound_nodes\n"," |      Deprecated, do NOT use! Only for compatibility with external Keras.\n"," |  \n"," |  input\n"," |      Retrieves the input tensor(s) of a layer.\n"," |      \n"," |      Only applicable if the layer has exactly one input,\n"," |      i.e. if it is connected to one incoming layer.\n"," |      \n"," |      Returns:\n"," |          Input tensor or list of input tensors.\n"," |      \n"," |      Raises:\n"," |        RuntimeError: If called in Eager mode.\n"," |        AttributeError: If no inbound nodes are found.\n"," |  \n"," |  input_mask\n"," |      Retrieves the input mask tensor(s) of a layer.\n"," |      \n"," |      Only applicable if the layer has exactly one inbound node,\n"," |      i.e. if it is connected to one incoming layer.\n"," |      \n"," |      Returns:\n"," |          Input mask tensor (potentially None) or list of input\n"," |          mask tensors.\n"," |      \n"," |      Raises:\n"," |          AttributeError: if the layer is connected to\n"," |          more than one incoming layers.\n"," |  \n"," |  input_shape\n"," |      Retrieves the input shape(s) of a layer.\n"," |      \n"," |      Only applicable if the layer has exactly one input,\n"," |      i.e. if it is connected to one incoming layer, or if all inputs\n"," |      have the same shape.\n"," |      \n"," |      Returns:\n"," |          Input shape, as an integer shape tuple\n"," |          (or list of shape tuples, one tuple per input tensor).\n"," |      \n"," |      Raises:\n"," |          AttributeError: if the layer has no defined input_shape.\n"," |          RuntimeError: if called in Eager mode.\n"," |  \n"," |  input_spec\n"," |      `InputSpec` instance(s) describing the input format for this layer.\n"," |      \n"," |      When you create a layer subclass, you can set `self.input_spec` to enable\n"," |      the layer to run input compatibility checks when it is called.\n"," |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n"," |      of rank 4. As such, you can set, in `__init__()`:\n"," |      \n"," |      ```python\n"," |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n"," |      ```\n"," |      \n"," |      Now, if you try to call the layer on an input that isn't rank 4\n"," |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n"," |      error:\n"," |      \n"," |      ```\n"," |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n"," |      expected ndim=4, found ndim=1. Full shape received: [2]\n"," |      ```\n"," |      \n"," |      Input checks that can be specified via `input_spec` include:\n"," |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n"," |      - Shape\n"," |      - Rank (ndim)\n"," |      - Dtype\n"," |      \n"," |      For more information, see `tf.keras.layers.InputSpec`.\n"," |      \n"," |      Returns:\n"," |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n"," |  \n"," |  losses\n"," |      Losses which are associated with this `Layer`.\n"," |      \n"," |      Variable regularization tensors are created when this property is accessed,\n"," |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n"," |      propagate gradients back to the corresponding variables.\n"," |      \n"," |      Returns:\n"," |        A list of tensors.\n"," |  \n"," |  metrics\n"," |      List of `tf.keras.metrics.Metric` instances tracked by the layer.\n"," |  \n"," |  name\n"," |      Name of the layer (string), set in the constructor.\n"," |  \n"," |  non_trainable_variables\n"," |  \n"," |  non_trainable_weights\n"," |      List of all non-trainable weights tracked by this layer.\n"," |      \n"," |      Non-trainable weights are *not* updated during training. They are expected\n"," |      to be updated manually in `call()`.\n"," |      \n"," |      Returns:\n"," |        A list of non-trainable variables.\n"," |  \n"," |  outbound_nodes\n"," |      Deprecated, do NOT use! Only for compatibility with external Keras.\n"," |  \n"," |  output\n"," |      Retrieves the output tensor(s) of a layer.\n"," |      \n"," |      Only applicable if the layer has exactly one output,\n"," |      i.e. if it is connected to one incoming layer.\n"," |      \n"," |      Returns:\n"," |        Output tensor or list of output tensors.\n"," |      \n"," |      Raises:\n"," |        AttributeError: if the layer is connected to more than one incoming\n"," |          layers.\n"," |        RuntimeError: if called in Eager mode.\n"," |  \n"," |  output_mask\n"," |      Retrieves the output mask tensor(s) of a layer.\n"," |      \n"," |      Only applicable if the layer has exactly one inbound node,\n"," |      i.e. if it is connected to one incoming layer.\n"," |      \n"," |      Returns:\n"," |          Output mask tensor (potentially None) or list of output\n"," |          mask tensors.\n"," |      \n"," |      Raises:\n"," |          AttributeError: if the layer is connected to\n"," |          more than one incoming layers.\n"," |  \n"," |  output_shape\n"," |      Retrieves the output shape(s) of a layer.\n"," |      \n"," |      Only applicable if the layer has one output,\n"," |      or if all outputs have the same shape.\n"," |      \n"," |      Returns:\n"," |          Output shape, as an integer shape tuple\n"," |          (or list of shape tuples, one tuple per output tensor).\n"," |      \n"," |      Raises:\n"," |          AttributeError: if the layer has no defined output shape.\n"," |          RuntimeError: if called in Eager mode.\n"," |  \n"," |  stateful\n"," |  \n"," |  trainable\n"," |  \n"," |  trainable_variables\n"," |      Sequence of trainable variables owned by this module and its submodules.\n"," |      \n"," |      Note: this method uses reflection to find variables on the current instance\n"," |      and submodules. For performance reasons you may wish to cache the result\n"," |      of calling this method if you don't expect the return value to change.\n"," |      \n"," |      Returns:\n"," |        A sequence of variables for the current module (sorted by attribute\n"," |        name) followed by variables from all submodules recursively (breadth\n"," |        first).\n"," |  \n"," |  trainable_weights\n"," |      List of all trainable weights tracked by this layer.\n"," |      \n"," |      Trainable weights are updated via gradient descent during training.\n"," |      \n"," |      Returns:\n"," |        A list of trainable variables.\n"," |  \n"," |  updates\n"," |  \n"," |  variables\n"," |      Returns the list of all layer variables/weights.\n"," |      \n"," |      Alias of `self.weights`.\n"," |      \n"," |      Returns:\n"," |        A list of variables.\n"," |  \n"," |  weights\n"," |      Returns the list of all layer variables/weights.\n"," |      \n"," |      Returns:\n"," |        A list of variables.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Class methods inherited from tensorflow.python.module.module.Module:\n"," |  \n"," |  with_name_scope(method) from builtins.type\n"," |      Decorator to automatically enter the module name scope.\n"," |      \n"," |      >>> class MyModule(tf.Module):\n"," |      ...   @tf.Module.with_name_scope\n"," |      ...   def __call__(self, x):\n"," |      ...     if not hasattr(self, 'w'):\n"," |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n"," |      ...     return tf.matmul(x, self.w)\n"," |      \n"," |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n"," |      names included the module name:\n"," |      \n"," |      >>> mod = MyModule()\n"," |      >>> mod(tf.ones([1, 2]))\n"," |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n"," |      >>> mod.w\n"," |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n"," |      numpy=..., dtype=float32)>\n"," |      \n"," |      Args:\n"," |        method: The method to wrap.\n"," |      \n"," |      Returns:\n"," |        The original method wrapped such that it enters the module's name scope.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from tensorflow.python.module.module.Module:\n"," |  \n"," |  name_scope\n"," |      Returns a `tf.name_scope` instance for this class.\n"," |  \n"," |  submodules\n"," |      Sequence of all sub-modules.\n"," |      \n"," |      Submodules are modules which are properties of this module, or found as\n"," |      properties of modules which are properties of this module (and so on).\n"," |      \n"," |      >>> a = tf.Module()\n"," |      >>> b = tf.Module()\n"," |      >>> c = tf.Module()\n"," |      >>> a.b = b\n"," |      >>> b.c = c\n"," |      >>> list(a.submodules) == [b, c]\n"," |      True\n"," |      >>> list(b.submodules) == [c]\n"," |      True\n"," |      >>> list(c.submodules) == []\n"," |      True\n"," |      \n"," |      Returns:\n"," |        A sequence of all submodules.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n"," |  \n"," |  __new__(cls, *args, **kwargs)\n"," |      Create and return a new object.  See help(type) for accurate signature.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0pg-QQEW51HH","colab_type":"code","colab":{}},"source":["class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, embedding_matrix, dec_units, batch_size):\n","    super(Decoder, self).__init__()\n","    self.batch_size = batch_size\n","    self.dec_units = dec_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=True)\n","    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer=\"glorot_uniform\")\n","\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # used for attention\n","    self.attention = BahdanauAttention(self.dec_units)\n","  \n","  def call(self, x, hidden, enc_output):\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","    x = self.embedding(x)\n","    print(\"after embedding shape:\", x.shape)\n","    print(\"context_vector shape:\", context_vector.shape)\n","    print(\"attention_weights shape:\", attention_weights.shape)\n","\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","    print(\"expand context_vector shape:\", tf.expand_dims(context_vector, 1).shape)\n","    print(\"after concat x's shape:\", x.shape)\n","\n","    output, state = self.gru(x)\n","    print(\"output shape:\", output.shape)\n","    print(\"state shape:\", state.shape)\n","    \n","    output = tf.reshape(output, (-1, output.shape[2]))\n","    print(\"after reshape output's shape:\", output.shape)\n","\n","    x = self.fc(output)\n","    print(\"x shape:\", x.shape)\n","    \n","\n","    return x, state, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qHk9TRo9lB9","colab_type":"code","colab":{}},"source":["## 测试 Decoder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTSeLjZd9250","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":179},"outputId":"67705064-c489-4081-ce96-410a379cc0ce","executionInfo":{"status":"ok","timestamp":1589641522943,"user_tz":-480,"elapsed":2406,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}}},"source":["decoder = Decoder(vocab_size, embedding_dim, embedding_matrix, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)), sample_hidden, sample_output)"],"execution_count":69,"outputs":[{"output_type":"stream","text":["after embedding shape: (64, 1, 300)\n","context_vector shape: (64, 1024)\n","attention_weights shape: (64, 260, 1)\n","expand context_vector shape: (64, 1, 1024)\n","after concat x's shape: (64, 1, 1324)\n","output shape: (64, 1, 1024)\n","state shape: (64, 1024)\n","after reshape output's shape: (64, 1024)\n","x shape: (64, 32909)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FTIsibuq-dM0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"c0a0bdcf-e948-4225-dae0-6b49812d0cd1","executionInfo":{"status":"ok","timestamp":1589641110579,"user_tz":-480,"elapsed":1150,"user":{"displayName":"Crise Zhaopp","photoUrl":"","userId":"18306759907168652538"}}},"source":["tf.random.uniform((64, 1)).shape"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 1])"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"2deeqJis_l4d","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}